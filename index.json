[{"authors":["Michael Pilosov"],"categories":["Artwork"],"content":"The purpose of this document is to record the changes made to this website and various other notes that I may want to come back to. I do not expect it to be of particular use to anyone other than myself.\n","date":1545268567,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1545268567,"objectID":"193656f33ba78663fc2eff3ee64d6f9b","permalink":"https://www.michaelpilosov.com/devlog/","publishdate":"2018-12-19T18:16:07-07:00","relpermalink":"/devlog/","section":"devlog","summary":"Cataloging changes made to the Academic theme.","tags":["dev"],"title":"devlog","type":"docs"},{"authors":null,"categories":null,"content":"The developers over at the Jupyter Project have been busy developing tools to help scientists and developers leverage the latest developments in web frameworks.\n Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.\n All of this development is in the hopes of advancing open-science practices, hence the name of this tutorial. The code and computational resources required to recreate results should be readily available to the audience of scientific papers.\nThere are a number of projects that attempt to address the difficulties of accomplishing this. We summarize them briefly in the next section.\n","date":1545116400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1545375600,"objectID":"895b70c0555628e48b6471a6a5bae4ed","permalink":"https://www.michaelpilosov.com/openscience/","publishdate":"2018-12-18T00:00:00-07:00","relpermalink":"/openscience/","section":"openscience","summary":"The developers over at the Jupyter Project have been busy developing tools to help scientists and developers leverage the latest developments in web frameworks.\n Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.\n All of this development is in the hopes of advancing open-science practices, hence the name of this tutorial. The code and computational resources required to recreate results should be readily available to the audience of scientific papers.","tags":null,"title":"Open Science","type":"docs"},{"authors":null,"categories":null,"content":" Thank you for taking the time to check out my resume! Have a look around the sections listed in the menu on the left.1\n[ TO DO ] Some information about my beliefs regarding the presentation of information.\nAbout My Research I work in the field of Uncertainty Quantification, specifically on inverse problems (usually with models involving physics).\nI am concerned about how experimental data impacts predictions.\nSometimes that means getting the most out of what limited data is available.\nOther times it involves designing the experiment so that each measurement is as informative as possible.\nNot all data are equally useful. I help decide what is worth measuring.\nWhy it is Useful The applications of my academic research are numerous.\nExamples include groundwater contamination and remediation, storm surge prediction, oil recovery or exploration, and imaging technology.\nI have my advisor—Dr. Troy Butler—to thank for opening the door to such broadly applicable and practical research.\n Mobile users: to expand this resume, click the menu ↖️ ^   ","date":1536472800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1545202800,"objectID":"fe0a7b66b73da9f01310bb11d50ee415","permalink":"https://www.michaelpilosov.com/resume/","publishdate":"2018-09-09T00:00:00-06:00","relpermalink":"/resume/","section":"resume","summary":"Thank you for taking the time to check out my resume! Have a look around the sections listed in the menu on the left.1\n[ TO DO ] Some information about my beliefs regarding the presentation of information.\nAbout My Research I work in the field of Uncertainty Quantification, specifically on inverse problems (usually with models involving physics).\nI am concerned about how experimental data impacts predictions.\nSometimes that means getting the most out of what limited data is available.","tags":null,"title":"Resume","type":"docs"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"25cb8f346fb6c519eb714b5055711680","permalink":"https://www.michaelpilosov.com/wid/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/wid/experience/","section":"wid","summary":"","tags":null,"title":"Overview","type":"wid"},{"authors":null,"categories":null,"content":"","date":1524204000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524204000,"objectID":"1285d6091a844e198e380c1500c72560","permalink":"https://www.michaelpilosov.com/wid/resources/","publishdate":"2018-04-20T00:00:00-06:00","relpermalink":"/wid/resources/","section":"wid","summary":"","tags":null,"title":"Resources","type":"wid"},{"authors":null,"categories":null,"content":"","date":1461132000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461132000,"objectID":"677923068b09760ec0be5173f4cbdb23","permalink":"https://www.michaelpilosov.com/wid/posts/","publishdate":"2016-04-20T00:00:00-06:00","relpermalink":"/wid/posts/","section":"wid","summary":"","tags":null,"title":"Recent Galleries","type":"wid"},{"authors":null,"categories":null,"content":"Here is a summary of my activity on Github from my legacy username.\n Loading the data just for you.   new GitHubCalendar(\".calendar\", \"mpilosov\");  ","date":1547535600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547535600,"objectID":"446e65f2afac0358c772da84d822b629","permalink":"https://www.michaelpilosov.com/activity/","publishdate":"2019-01-15T00:00:00-07:00","relpermalink":"/activity/","section":"","summary":"What have I been up to?","tags":null,"title":"GitHub Activity","type":"page"},{"authors":["Michael Pilosov"],"categories":["documentation"],"content":" Jan 1, 2019 Status Report Okay so where are we at?\n Spawner needs to be sussed out. Right now I really like DockerSpawner, and it seems to persist storage.  Each student gets a user account but it\u0026rsquo;s only used for authenetication right now. Can\u0026rsquo;t figure out volume-mapping.  With dockerspawner.DockerSpawner:\n Each time someone is logged in, a container is either loaded up or built from jupyterlab_img container. These are very flexible, many stacks available. In this set-up, each student gets a container, which is a full-fledged linux machine. Since Docker is managing these alongside Dockerhub in the network, communication between containers is not possible right now. should be One hour of inactivity results in shutdown of container thanks to a python script from jupyter.  What we want: dockerspawner.SystemUserSpawner\n with maps to home directories that exist on the jupyterhub container. this way a teacher opens the docker container with the \u0026ldquo;hub\u0026rdquo; and all the students are there.  What we have now:\n each student has a container with their name on it. each is its own linux machine to get into their linux machines, run docker exec -ti jupyter-{surname} /bin/bash, do your thing, Ctrl-D to exit. The script that gets installed in the hub container stops idle single-user servers (I think this means it shuts down the containers that are inactive). The containers are spun up based on a jupyterlab image.  What happens when we update this? Perhaps to include files for every student?  Alternatively, you have all of them learn to manage push/pull from a class repository It appears that to do this, we sub-class the Spawner.   from dockerspawner import DockerSpawner class MyDockerSpawner(DockerSpawner): team_map = { 'user1': 'team1', 'user2': 'team1', 'user3': 'team2', } def start(self): team = self.team_map[self.user.name] # add team volume to volumes self.volumes['jupyterhub-team-{}'.format(team)] = { 'bind': '/home/shared/{}'.format(team), 'mode': 'rw', # or ro for read-only } c.JupyterHub.spawner_class = MyDockerSpawner  New Server As root:\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; sudo apt-get update apt-cache policy docker-ce sudo apt-get install -y docker-ce sudo groupadd docker sudo usermod -aG docker $USER # also add any users you want to be running docker. # I added `michael` on my machine. will have to log out/in to refresh group membership. sudo apt-get install -y docker-compose  As user michael:\ncd repos/ git clone git clone https://github.com/mathematicalmichael/hubsetup.git cd hubsetup/ # make sure you clean up images/volumes/containers. I didn't have much there from before, did have hello-world. docker-compose build  This came up, may be a problem?\nWARNING: The COMPOSE_PROJECT_NAME variable is not set. Defaulting to a blank string.  But it kept going\u0026hellip;\nbut then.\nERROR: Service 'jupyterlab' failed to build: failed to register layer: Error processing tar file(exit status 1): write /opt/conda/lib/python3.6/site-packages/pandas/_libs/tslibs/timestamps.cpython-36m-x86_64-linux-gnu.so: no space left on device  So I went ahead and deleted a couple GB of space by removing unused conda environments and Lucas\u0026rsquo; user account.\ndocker-compose up  fairly sure this will fail because of a mis-specified IP address. Should also enable security since now I have them on this server.\nthe SSL is messing with me since I already have it set up on the server.\ntrying to launch jupyterhub with dockerspawner with jupyterhub local install.\nexport DOCKER_JUPYTER_IMAGE=jupyter/datascience-notebook:7254cdcfa22b\nOkay well the hub worked and spawner did not.\nI dove into making a custom spawner (may be necessary?), but it was a rabbit hole.\nNote from 1/8/19: The image needs to be available on the machine. It is separate from the hub, so just make sure the names line up correctly by checking docker images against the jupyterhub_config.py file.\nJan 2-3, 2019 See proxy page.\nUseful Reading Here is something interesting for version-controlling notebooks. https://github.com/mwouts/jupytext\nHere is an introduction to the notebook format. https://nbformat.readthedocs.io/en/latest/format_description.html TODO: You should turn this into a write-up.\nThe basic examples in here are actually a great demo of publishing LaTeX documents right from Jupyter. https://github.com/jupyter/nbconvert-examples\nNice article https://blog.dominodatalab.com/data-science-vs-engineering-tension-points/\nThis might be how to set up binderhub on your own \u0026ndash; minikube https://github.com/jupyterhub/binderhub/blob/master/CONTRIBUTING.md\nOther Projects for sharing results: https://github.com/minrk/thebelab https://github.com/QuantStack/voila Write about them in a new section summarizing sharing results.\nhttps://github.com/jupyter/dashboards\nAdding extra libraries to the jupyter-stacks image https://github.com/binder-examples/jupyter-stacks\nAllowing students to get latest files without knowing git https://github.com/jupyterhub/nbgitpuller\n When a link is clicked, we try to make opinionated intelligent guesses on how to do a merge automatically, without making the user do a conflict resolution. nbgitpuller is designed to be used by folks who do not know that git is being used underneath, and are only pulling content one way from a source and modifying it - not pushing it back. So we have made the following opinionated decisions.\n If content has changed in both places, prefer local changes over remote changes. If a file was deleted locally but present in the remote, remote file is restored to local repository. This allows users to get a \u0026lsquo;fresh copy\u0026rsquo; of a file by just deleting the file locally \u0026amp; clicking the link again. If a file exists locally but is untracked by git (maybe someone uploaded it manually), then rename the file, and pull in remote copy.   Hippylib-Hub, example to follow. https://github.com/g2s3-2018/hippylib-hub\nDockerspawner Want: - Dockerspawner is nice (can restart hub without issues). - Although, restarting if all-in-one isn\u0026rsquo;t that bad, either. Temporary inconvenience.\nAdd this to installations!!! It\u0026rsquo;s amazing. https://github.com/yuvipanda/nbresuse\nThis might be a good thing to test on our server. https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\nUSEFUL: in Vim: r:! openssl rand -hex 32 will paste a token into a file like config.yaml\nproxy: secretToken: xxxx  File Storage Wow I can\u0026rsquo;t believe this exists. https://www.katacoda.com/\nThis is a good file-storage solution https://www.youtube.com/watch?v=hqE5c5pyfrk https://storageos.com/developers/ another alternative, which appears to be a bit more complicated to set-up (though helm-chart should be available by now), but is open-source and from redhat: https://www.youtube.com/watch?v=Fgpr2lMnBVY [16:30]\nKubernetes Kubernetes 101 introduction https://medium.com/google-cloud/kubernetes-101-pods-nodes-containers-and-clusters-c1509e409e16\nThe monitoring of my memory usage led me to discover that repeated execution of plotting cells led to memory usage going through the roof. The solution was to add this cell-magic to the top of any plotting-cell: %reset -f out. What this does is purge the output of the cell\nJan 4-5, 2019 Tried and failed to get Traefik working. See proxy notes. Took some time to relax.\nNote\nIf you base a Dockerfile on this image:\nFROM juptyerhub/jupyterhub-onbuild:0.6 \u0026hellip; then your jupyterhub_config.py adjacent to your Dockerfile will be loaded into the image and used by JupyterHub.\nFile permissions Correct permissions for exposed shared directories require chmod -R 777 for the folder at the top\nWhat this means:\nPermissions: 1 – can execute 2 – can write 4 – can read\nThe octal number is the sum of those free permissions, i.e. 3 (1+2) – can execute and write 6 (2+4) – can write and read\nPosition of the digit in value: 1 – what owner can 2 – what users in the file group(class) can 3 – what users not in the file group(class) can\nSo the third is what we care about since no we don\u0026rsquo;t want to create users on the machine running docker.\nDatabase https://jupyterhub.readthedocs.io/en/stable/reference/database.html It comes pre-packaged with one, but it is recommended to use something else for production, which we will do!\nHash authentication\u0026hellip; Very nice https://github.com/thedataincubator/jupyterhub-hashauthenticator\nYou can generate a good secret key with openssl rand -hex 32.\nc.JupyterHub.authenticator_class = 'hashauthenticator.HashAuthenticator' c.HashAuthenticator.secret_key = 'my secret key' # Defaults to '' c.HashAuthenticator.password_length = 10 # Defaults to 6 c.HashAuthenticator.show_logins = True # Optional, defaults to False  If the show_logins option is set to True, a CSV file containing login names and passwords will be served (to admins only) at /hub/login_list. Do we want this? Maybe for analytics? If possible.\nTo figure out my password, I used hashauthpw --length 10 mathematicalmichael [secret key] on any computer that has run pip install jupyterhub-hashauthenticator\nMultiple Spawners multiple spawners! definitely my favorite way to go. Says this will allow them to choose upon login. If so\u0026hellip; amazing.\n[multiple spawners issue on github][https://github.com/jupyterhub/dockerspawner/issues/236]:\nfrom dockerspawner import SystemUserSpawner class MultiDockerImageSpawner(SystemUserSpawner): images = { 'SciPy': 'jupyter/scipy-notebook:0f73f7488fa0', 'Tensorflow': 'jupyter/tensorflow-notebook:59904dd7776a', 'R': 'jupyter/r-notebook:59904dd7776a', } def _options_form_default(self): outval = \u0026quot;\u0026quot;\u0026quot; \u0026lt;label for=\u0026quot;image\u0026quot;\u0026gt;Docker Image\u0026lt;/label\u0026gt; \u0026lt;select name=\u0026quot;image\u0026quot;\u0026gt; \u0026quot;\u0026quot;\u0026quot; for name, image in self.images.items(): outval += \u0026quot;\u0026lt;option value=\\\u0026quot;%s\\\u0026quot;\u0026gt;%s (%s)\u0026lt;/option\u0026gt;\u0026quot; % (name, name, image) outval += \u0026quot;\u0026quot;\u0026quot; \u0026lt;/select\u0026gt; \u0026quot;\u0026quot;\u0026quot; return outval def options_from_form(self, formdata): options = {} options['image'] = formdata.get('image', ['SciPy'])[0] self.image = self.images[options['image']] return options  Everything is now up and running! New images can be added with total ease, the hub restart only has minimal disruption.\nA note on startup files: \u0026gt; IPython startup files, placed in ~/.ipython/profile_default/startup will be executed. These can be Python scripts (.py) or IPython scripts (.ipy with %magic commands). Notebooks aren\u0026rsquo;t supported as startup files, but if it really needs to be a notebook, you can use %run /path/to/notebook.ipynb in a .ipy startup file.\nThis means that for testing, we can create an image with a file that gets run at startup, set it as the temporary default, and launch servers.\nI GOT RSTUDIO WORKING. Okay, so just, build any image you want, and reference it in the spawner. If you can execute the following and see the same output, you\u0026rsquo;ll have something working properly.\nmpilosov@math-ws-204:~/Packages/deploy/singleuser$ docker run --rm -ti rstudio_test Executing the command: jupyter notebook [I 22:39:24.130 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret [I 22:39:24.745 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [I 22:39:24.745 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 22:39:24.748 NotebookApp] Serving notebooks from local directory: /home/jovyan [I 22:39:24.748 NotebookApp] The Jupyter Notebook is running at: [I 22:39:24.748 NotebookApp] http://(620acce394ce or 127.0.0.1):8888/?token=cbf208a110db20a3fce4814d5cf1bf2e41aca5e4a165c69d [I 22:39:24.749 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:39:24.749 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://(620acce394ce or 127.0.0.1):8888/?token=cbf208a110db20a3fce4814d5cf1bf2e41aca5e4a165c69d ^C[I 22:39:36.445 NotebookApp] interrupted Serving notebooks from local directory: /home/jovyan 0 active kernels The Jupyter Notebook is running at: http://(620acce394ce or 127.0.0.1):8888/?token=cbf208a110db20a3fce4814d5cf1bf2e41aca5e4a165c69d Shutdown this notebook server (y/[n])? ^C[C 22:39:37.364 NotebookApp] received signal 2, stopping [I 22:39:37.366 NotebookApp] Shutting down 0 kernels  Jan 6-7, 2019 Math-hub everything is up and running on math-hub, 100 notebooks idle take up 8gb of ram.\nI managed to mount volumes easily, but if you create a new volume with docker, use docker inspect to find out where it is and change the permissions.\npilosovm@math-hub:~/repos/deploy$ docker inspect rw_shared_volume [ { \u0026quot;CreatedAt\u0026quot;: \u0026quot;2019-01-04T17:57:04-07:00\u0026quot;, \u0026quot;Driver\u0026quot;: \u0026quot;local\u0026quot;, \u0026quot;Labels\u0026quot;: {}, \u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/rw_shared_volume/_data\u0026quot;, \u0026quot;Name\u0026quot;: \u0026quot;rw_shared_volume\u0026quot;, \u0026quot;Options\u0026quot;: {}, \u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot; } ] pilosovm@math-hub:~/repos/deploy$ sudo chmod 777 /var/lib/docker/volumes/rw_shared_volume/_data [sudo] password for pilosovm: pilosovm@math-hub:~/repos/deploy$  startup scripts: bootstrap scripts https://github.com/jupyterhub/jupyterhub/tree/master/examples/bootstrap-script\nJan 8, 2019 https://www.paraview.org/web/ Paraview in-browser is now a thing\u0026hellip; Can we somehow create a container that includes the ability to launch this application?\nhttps://jupyter-docker-stacks.readthedocs.io/en/latest/using/recipes.html Jupyter Docker-Stacks\nPython 2 Adding Python 2: dd a Python 2.x environment Python 2.x was removed from all images on August 10th, 2017, starting in tag cc9feab481f7. You can add a Python 2.x environment by defining your own Dockerfile inheriting from one of the images like so:\n# Choose your desired base image FROM jupyter/scipy-notebook:latest # Create a Python 2.x environment using conda including at least the ipython kernel # and the kernda utility. Add any additional packages you want available for use # in a Python 2 notebook to the first line here (e.g., pandas, matplotlib, etc.) RUN conda create --quiet --yes -p $CONDA_DIR/envs/python2 python=2.7 ipython ipykernel kernda numpy pandas matplotlib ipywidgets yaml \u0026amp;\u0026amp; \\ conda clean -tipsy USER root # Create a global kernelspec in the image and modify it so that it properly activates # the python2 conda environment. RUN $CONDA_DIR/envs/python2/bin/python -m ipykernel install \u0026amp;\u0026amp; \\ $CONDA_DIR/envs/python2/bin/kernda -o -y /usr/local/share/jupyter/kernels/python2/kernel.json USER $NB_USER  Mixed Authentication The main authentication page on the Jupyterhub wiki is pretty useful but also kind of incomplete.\nRight now I am creating users based on the folders in /home/math, authenticating with HashAuthenticator, but this example demonstrates how to mix Authentication methods.\nTO DO Try google authentication on your own website.\nJan 8-10, 2019 Final Challenges Proxy stress, user testing, configuration, security.\nI spoke with Audrey about setting up a hub. She wants very clear set of instructions.\nJan 11, 2019 Finishing Touches Need to write up an entire summary.\nHere\u0026rsquo;s what needs to happen:\n Project Name (minimal changes) Document Process on Hub using newest version of repo. Nginx / Letsencrypt Instructions Re-do with math.computer on a new Droplet, screen cap? Wrap procedure into bash script?  Changes for a user to make to set up a new hub: - touch secrets/postgres.env and touch userlist - rename project folder before running anything else. - change .env to reflect project name, include the port in there (no changes should be necessary to any other files) - (optional) tweak limits in jupyterhub_config.py - (goal: no secrets, auth, etc). try to edit makefile so that it takes care of all of that stuff. (or wrap into a first_run.sh script). - get password the first time? hashauth\u0026hellip; (or automate it so that it\u0026rsquo;s printed to a file)?\nCustomizing Spawning Options Based on users\u0026hellip;\nfrom dockerspawner import DockerSpawner class MyDockerSpawner(DockerSpawner): team_map = { 'username1': 'team-a', 'username2': 'team-b', 'username3': 'team-a', } def start(self): if self.user.name in self.team_map: team = self.team_map[self.user.name] # add team volume to volumes self.volumes['/directory/jupyterhub-team-{}'.format(team)] = { 'bind': '/home/jovyan/teamfolder', 'mode': 'rw', # or ro for read-only } return super().start() c.JupyterHub.spawner_class = MyDockerSpawner  So, with that, refer to jupyterhub_config.py for instances of DockerSpawner and the children of that subclass. You can modify these properties (such as image)\nmathematicalmichael@math-hub:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE math-user latest 64698da59274 3 hours ago 11.5GB math latest 7368bc798b20 4 hours ago 1.05GB postgres 9.5 fc003c9dded6 29 hours ago 227MB jupyter/datascience-notebook latest 18c805bb3afb 3 days ago 6.32GB jupyterhub/jupyterhub-onbuild 0.9.4 9ca16c1a77c3 3 months ago 812MB  However, my Disk-Usage on Digital Ocean reads 15GB. This tells me that math-user includes the size jupyter/datascience-notebook, since the sum of these two alone would exceed 15GB. \u0026lsquo;\nJan 15\nNote: Copying over from walkthrough.\nWe will use this guy\u0026rsquo;s walkthrough.\ncurl -L https://raw.githubusercontent.com/wmnnd/nginx-certbot/master/init-letsencrypt.sh \u0026gt; init-letsencrypt.sh sed 's/example.com/mathfight.club/g' init-letsencrypt.sh \u0026gt; letsencrypt.sh mv letsencrypt.sh init-letsencrypt.sh chmod +x init-letsencrypt.sh sudo ./init-letsencrypt.sh  Had to edit some lines, ran out of requests for letsencrypt.\n","date":1545721200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546378980,"objectID":"2bc1d02af202a1f22f2151824c35d245","permalink":"https://www.michaelpilosov.com/devlog/jupyterhub/","publishdate":"2018-12-25T00:00:00-07:00","relpermalink":"/devlog/jupyterhub/","section":"devlog","summary":"Notes from configuring Jupyterhub","tags":["dev","jupyter","openscience","docker"],"title":"Jupyterhub Configuration","type":"docs"},{"authors":["Michael Pilosov"],"categories":["documentation"],"content":" Dec 22, 2018 Today I began to structure and fill in the basic information for the Open Science/Docker Documentation.\n12/24/2019 - 01/01/2019\nToday I started by watching a video to be aware of the problems I might face going forward with JupyterHub. I watched someone from Berkeley describe how they\u0026rsquo;ve scaled to thousands of users across many professors and managed to not grow their IT team while doing so.\nI will start by using this space here to take notes on the following video(s):\nDeploying Jupyter Notebooks for Students and Researchers (PyData 2016)   Managing a 1000+ Student JupyterHub without Losing your Sanity (JupyterCon 2018)   Here are the stated goals in the video:\n [7:30] Infrastructure Shouldn\u0026rsquo;t Bottleneck  Instructors should be able to install packages as needed  [8:00] Anyone can Deploy  Treat Admins as Equal (grad students, teachers, etc)  [9:00] Automate Workflows  Avoid manual processes. No one-off scripts to bootstrap solutions  [9:30] Reduce Human Maintenance  Build on top of tools that already exist Let academic edit Dockerfile directly on Github, issue pull-request. Travis Process builds image, and if successful, requests to merge Rather than CLI, hit a couple buttons to set off other Travis Processes to update Helm  [13:55]: Reproducibility  Tag versions, everything. The release cycles of the core infrastructure components do not align with each other. In pip file, in Dockerfile, all of it. Get git hashes for latest commits. Tag Docker images with the hashes of the repositories that generated them This makes it much easier to re-deploy!  [15:30] Horror Story of Hub failing during Finals week because a Cloud bill was unpaid (grant ran out)  All the versioning helped saved them Despite coming up on different nodes, the same hub came out of it.  [17:00] Observability  Monitoring (container provider may have analytics) This may be less of a problem for us if we deploy on our own servers. Figure out needs of students to figure out how many nodes. After observing, they noticed most students used under 1GB, allowing them to double capacity per node. Students that went above were writing runaway processes anyway.  [19:30] Incident Reporting  any time something goes down, write a report. [Incident Reports at Berkeley][github.com/data-8/infrastructure] Improves deployment. Event summary, timeline, conclusion, and action items. Reports are \u0026ldquo;blameless\u0026rdquo; to encourage transparency with relevant details  [21:30] Generalization  Be a good open-source citizen This means digging into problems and reporting bugs Not every problem is your fault. Sometimes an upstream thing breaks it, and if you report it, you may save other people time/effort. Don\u0026rsquo;t rely on forks and custom patches. Contribute. Think about reproducible deployment scenarios.  [24:30] Contact Information (encouraged)  Ryan Lovett Yuvi Panda    Scaling JupyterHub to many users (PyData Tel-Aviv 2018)   \u0026ldquo;JupyterHub and JupyterLab: Perfect Together (PyData 2018)\u0026rdquo;   JupyterHub with Kubernetes:   Log A lot of solutions exist for using docker to handle the file storage and user authentication.\nHere is the solution that I think will work best for workshops:\nOpen DreamKit\n[11:37 AM] Pilosov, Michael\nOkay, can\u0026rsquo;t confirm until I try it, but I am reasonably certain that while this set-up is useful in many contexts. It doesn\u0026rsquo;t address the teacher/student/group collab management that we quite wanted (and that I focused on getting working first and have done already). ​\n[11:39 AM] Pilosov, Michael\nThe way the build works here with data management is actually much \u0026ldquo;slimmer\u0026rdquo; in fact, and I think there\u0026rsquo;s a use for it for us for sure\u0026hellip; but I suspect it doesn\u0026rsquo;t allow two users to share data, or a super-user to poke around in files. ​\n[11:41 AM] Pilosov, Michael\nBut it would allow for secure authentication for like, a workshop, just using people\u0026rsquo;s github accounts, and data-persistence and management would be more ideal in that sense since you can turn on permanence for each user, destroy it when the workshop is over as superuser (but without ever being able to poke around in their files, which is a nice security feature for the user I suppose\u0026hellip;).\n[11:52 AM] Pilosov, Michael\nIn short, what we want instead is for docker to launch up a virtual LINUX machine, where all memory persists within the container. It\u0026rsquo;s as if you set up a \u0026ldquo;new computer\u0026rdquo; for each class/workshop. Each class gets its own port. Importantly, this makes \u0026ldquo;management\u0026rdquo; very familiar, identical to what you already know how to do, rather than having to \u0026ldquo;learn docker\u0026rdquo; to manage student\u0026rsquo;s work. also, updating things can be done this way without re-building images through docker. which is really nice for on-the-fly changes.\nWhat\u0026rsquo;s the downside you ask? It\u0026rsquo;s definitely less memory efficient, but it\u0026rsquo;s not a problem at all. we\u0026rsquo;re talking like\u0026hellip; cheap laptop levels of storage, nothing major.\n[12:20 AM] Finished watching the first video and going throught he tutorial above for the most part. I still think the Linux within each container is the simplest option for someone to manage on their own. it is definitely the most familiar environment.\nTo start, let me ssh into my workstation and see if I can revive the docker container from the summer workshop. I will log my commands here!\n# start the server docker run -td -p 80:8000 --name=labhub mathematicalmichael/labhub-test # check that it is up docker ps # look for ip under process associated with your machine. look for # inet addr: \u0026lt;XXX.XXX.XXX.XXX\u0026gt;  Then we connect to auraria-anywhere via Cisco AnyConnect and attempt to connect to :8000 via our browser.\nThis site can\u0026rsquo;t be reached\n Debug mode:\ndocker logs labhub  showed me that\nCreating 3 new users... Created user0 with password Breckenridge0_g2s3 Created user1 with password Breckenridge1_g2s3 Created user2 with password Breckenridge2_g2s3 *** Running /etc/rc.local... *** Booting runit daemon... *** Runit started as PID 66 *** Running jupyterhub --Spawner.cmd='jupyter-labhub' --no-ssl... *** jupyterhub --Spawner.cmd='jupyter-labhub' --no-ssl exited with status 127. *** Shutting down runit daemon (PID 66)... *** Killing all processes...  So that tells me I messed up the .Dockerfile. It might be instructive to just learn from what they did. I don\u0026rsquo;t really need their libraries (which is what I believe was causing the build to fail before).\n** It will definitely be better to start with a fresh Linux image and go from there, since we want to build on top of the newest release anyway, and make sure we understand how its done, keep images/layers light. **\nFrom MUQ-Hippylib (see muq for more info)\nFROM quay.io/fenicsproject/stable:2017.2.0.r3 MAINTAINER U. Villa USER root RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -yy pwgen npm nodejs-legacy python3-pip libgeos-dev\u0026amp;\u0026amp; \\ npm install -g configurable-http-proxy \u0026amp;\u0026amp; \\ pip3 install jupyterhub==0.8.1 \u0026amp;\u0026amp; \\ pip3 install ipython[notebook]==6.2.1 h5py pandas \u0026amp;\u0026amp; \\ pip install --user https://github.com/matplotlib/basemap/archive/master.zip RUN apt-get clean \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* #RUN mkdir /etc/certs #RUN touch /etc/certs/ssl.key #RUN touch /etc/certs/ssl.crt #RUN openssl req -x509 -nodes -days 730 -newkey rsa:2048 \\ # -subj \u0026quot;/C=XY/ST=XYZ/L=XYZ/O=XYZ/CN=example.com\u0026quot; \\ # -keyout /etc/certs/ssl.key -out /etc/certs/ssl.crt USER fenics # Install MUQ RUN cd /home/fenics \u0026amp;\u0026amp; \\ mkdir Installations; mkdir Installations/MUQ_INSTALL \u0026amp;\u0026amp; \\ git clone --depth 1 https://bitbucket.org/mituq/muq2.git \u0026amp;\u0026amp; \\ cd muq2/; mkdir build; cd build; \\ cmake -DCMAKE_INSTALL_PREFIX=/home/fenics/Installations/MUQ_INSTALL -DMUQ_USE_PYTHON=ON ../ \u0026amp;\u0026amp; \\ make install # Install hIPPYlib RUN cd /home/fenics/Installations \u0026amp;\u0026amp; \\ git clone https://github.com/hippylib/hippylib.git \u0026amp;\u0026amp; \\ chmod -R o+rx hippylib # Copy the notebooks RUN cd /home/fenics/Installations \u0026amp;\u0026amp; \\ git clone https://github.com/g2s3-2018/labs.git COPY python3_config.json /usr/local/share/jupyter/kernels/python3/kernel.json ENV LD_LIBRARY_PATH /home/fenics/Installations/MUQ_INSTALL/lib:/home/fenics/Installations/MUQ_INSTALL/muq_external/lib ENV PYTHONPATH /home/fenics/Installations/MUQ_INSTALL/lib:/home/fenics/Installations/hippylib USER root COPY jupyterhub_config.py /home/fenics/jupyterhub_config.py COPY make-users-std-password.sh /etc/my_init.d/make-users-std-password.sh RUN chmod +x /etc/my_init.d/make-users-std-password.sh RUN rm /etc/my_init.d/set-home-permissions.sh COPY update_lab.sh /home/fenics/update_lab.sh RUN chmod +x /home/fenics/update_lab.sh RUN mkdir -p /home/fenics/.jupyter COPY jupyter_notebook_config.py /home/fenics/.jupyter/jupyter_notebook_config.py ENV NUMBER_OF_USERS 60 WORKDIR /home/fenics/ ENTRYPOINT [\u0026quot;/sbin/my_init\u0026quot;,\u0026quot;--\u0026quot;] CMD [\u0026quot;jupyterhub\u0026quot;]  So it appears that we\u0026rsquo;re starting with a root image from the Fenics Project. Quay.io has a summary of security vulnerabilities for each repository tag.\nThe one used by Dr. Umberto Villa had 3 High-Risk Vulnerabilities and 151 fixable. More modern ones such as 2018.1.0.r3 have only 57 Medium-Risk ones and 25 fixable.\nThis is a considerable advantage and thus I believe the correct place to start from. It is also evident that the developers have been working on reducing image sizes, with a clear downward-trend each time a new one comes up. The latest does also seem to be built on top of the Ubuntu 18.04, which is exactly what we wanted!!!\nWonderful.\nOkay, but before I go on building my customized version[^builderrors], I want to remember how to get the hippylib-muq one working. I have the Dockerhub image from mparno/hippylib-muq and can run it (create an instance) with\ndocker run -td -p 80:8000 --name=labhub mparno/muq-hippylib  So this creates an instance that will map to port 80 (default address when you land on an IP, something we should map with sub-domains later on to have classhub.math.ucdenver.edu rather than ports to memorize). This \u0026ldquo;instance\u0026rdquo; (container) is now created and the ports mapped, so now we have to start it.\ndocker start labhub # now can go to base IP address and log in! # later on... stop it. docker stop labhub  Alright, that\u0026rsquo;s fine and well. We have a baseline set of instructions to go off now, and can learn from. However, seeing as quite a lot is happening in those instructions, and no SSL security has been established, I would like to perhaps instead start with Dockerfiles from the Jupyter Project. Moreover, these will be newer versions and should come with Lab enabled already since it is finally in a stable release.\n[^builderrors] I remember that some updates to MUQ\u0026hellip; likely because of a lack of pinning to specific releases, caused build errors when I tried to start up my labhub. Though it might have been the initial commands, based on the readout above.\nSO we\u0026rsquo;ll have to do two things:\n Get Hub/Lab up and running with a single user based on the instructions from Project Jupyter. Use this on top of the Fenics build to get import dolfin working inside of the Hub.   NOTE The ease of sharing memory for class-uses is actually a BRAND NEW effort from Jupyter (a week old?). See here, which should remove the need for the solution we\u0026rsquo;re building, allow for much better and more lightweight scalability.\nI just came across dockerspawner which can be enabled with the following in jupyterhub_config.py:\nc.JupyterHub.spawner_class = 'dockerspawner.DockerSpawner'  More appropriate for the use case we have is SystemUserSpawner\n If you want to spawn notebook servers for users that correspond to system users, you can use the SystemUserSpawner instead. Add the following to your jupyterhub_config.py:\nc.JupyterHub.spawner_class = 'dockerspawner.SystemUserSpawner' The SystemUserSpawner will also need to know where the user home directories are on the host. By default, it expects them to be in /home/\u0026lt;username\u0026gt;, but if you want to change this, you\u0026rsquo;ll need to further modify the jupyterhub_config.py. For example, the following will look for a user\u0026rsquo;s home directory on the host system at /volumes/user/\u0026lt;username\u0026gt;:\nc.SystemUserSpawner.host_homedir_format_string = '/volumes/user/{username}'  For a full example of how SystemUserSpawner is used, see the compmodels-jupyterhub (warning: old) repository (this additionally runs the JupyterHub server within a docker container, and authenticates users using GitHub OAuth).\n I believe the correct entry-point for us will actually be here, at Project Jupyter\u0026rsquo;s Deploy-Docker repository.\n jupyterhub-deploy-docker provides a reference deployment of JupyterHub, a multi-user Jupyter Notebook environment, on a single host using Docker. This deployment is NOT intended for a production environment. It is a reference implementation that does not meet traditional requirements in terms of availability nor scalability.\n Okay, so this is actually a great solution, but it\u0026rsquo;s the same thing more/less as the \u0026ldquo;french\u0026rdquo; solution, with the encryption being handled directly by Jupyterhub.\nSince we want to be building Linux images and then running a Hub on each one, we basically just need to paste installation instructions into the Dockerfile.\nconda install -c conda-forge jupyterhub conda install jupyterlab  Docker Jupyter Stacks However, I then came across Jupyter Docker Stacks, which appear to be a number of recipes I can start with.\nSo let\u0026rsquo;s go ahead and try this on the server.\n Example 2: This command pulls the jupyter/datascience-notebook image tagged 3772fffc4aa4 from Docker Hub if it is not already present on the local host. It then starts an ephemeral container running a Jupyter Notebook server and exposes the server on host port 10000. The command mounts the current working directory on the host as /home/jovyan/work in the container. The server logs appear in the terminal. Visiting http://:10000/?token= in a browser loads JupyterLab, where hostname is the name of the computer running docker and token is the secret token printed in the console. Docker destroys the container after notebook server exit, but any files written to ~/work in the container remain intact on the host.\ndocker run --rm -p 10000:8888 -e JUPYTER_ENABLE_LAB=yes -v \u0026quot;$PWD\u0026quot;:/home/jovyan/work jupyter/datascience-notebook:3772fffc4aa4   My workstation was unable to find the image locally so it began to pull it from docker-cloud. This took a moment. Appears to be several gb large.\nOkay, well I managed to connect to it by visiting \u0026lt;IP\u0026gt;:10000/?token=XXXX...XXXX, where I grabbed the token from the output of the terminal window.\nThis is great functionality, but it\u0026rsquo;s not the hub we\u0026rsquo;re looking for. That said, the included dockerfiles are very instructive.\nHere is the dependency list. Each one is a root image of the next.\nbase-notebook \u0026gt; minimal-notebook \u0026gt; scipy-notebook \u0026gt; datascience-notebook\nThe last install, the [datascience-notebook][https://github.com/jupyter/docker-stacks/blob/master/datascience-notebook/Dockerfile] includes R and Julia on top of Python. The heavy HDF5 dependency for Julia is not included if the build is in test-mode (see line 10 and 73). It will also link the kernels together. As tempting as it is to start here, I\u0026rsquo;d like to start earlier.\nThe scipy-notebook features installs of widgets and more, and is the minimum requirement for most of what we need.\nSomething I noticed was that it includes an old version of pip. If you launch the container, it mounts whatever directory you were in when you executed the docker run command to a \u0026ldquo;fake one\u0026rdquo; inside docker (so pwd returns /home/jovyan/work). This is actually kind of nice. If volumes of students can be mounted by the professor to have a look around, that would be great. That would, of course, require learning some docker. Students can even install additional libraries as needed, but they will disappear next time they connect (though the files stick around!).\nWhat we need to do (I think), is merge this Dockerfile with the JupyterHub deployments. I think this just comes down to configuring the authenticator correctly (and referencing Umberto\u0026rsquo;s file above).\nLet\u0026rsquo;s look at the base-notebook, since it seems that scipy-notebook just adds bells and whistles (widgets already is calling jupyterlab extension manager). And yes, it appears hub is installed!\n# Copyright (c) Jupyter Development Team. # Distributed under the terms of the Modified BSD License. # Ubuntu 18.04 (bionic) from 2018-05-26 # https://github.com/docker-library/official-images/commit/aac6a45b9eb2bffb8102353c350d341a410fb169 ARG BASE_CONTAINER=ubuntu:bionic-20180526@sha256:c8c275751219dadad8fa56b3ac41ca6cb22219ff117ca98fe82b42f24e1ba64e FROM $BASE_CONTAINER LABEL maintainer=\u0026quot;Jupyter Project \u0026lt;jupyter@googlegroups.com\u0026gt;\u0026quot; ARG NB_USER=\u0026quot;jovyan\u0026quot; ARG NB_UID=\u0026quot;1000\u0026quot; ARG NB_GID=\u0026quot;100\u0026quot; USER root # Install all OS dependencies for notebook server that starts but lacks all # features (e.g., download as all possible file formats) ENV DEBIAN_FRONTEND noninteractive RUN apt-get update \u0026amp;\u0026amp; apt-get -yq dist-upgrade \\ \u0026amp;\u0026amp; apt-get install -yq --no-install-recommends \\ wget \\ bzip2 \\ ca-certificates \\ sudo \\ locales \\ fonts-liberation \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN echo \u0026quot;en_US.UTF-8 UTF-8\u0026quot; \u0026gt; /etc/locale.gen \u0026amp;\u0026amp; \\ locale-gen # Configure environment ENV CONDA_DIR=/opt/conda \\ SHELL=/bin/bash \\ NB_USER=$NB_USER \\ NB_UID=$NB_UID \\ NB_GID=$NB_GID \\ LC_ALL=en_US.UTF-8 \\ LANG=en_US.UTF-8 \\ LANGUAGE=en_US.UTF-8 ENV PATH=$CONDA_DIR/bin:$PATH \\ HOME=/home/$NB_USER ADD fix-permissions /usr/local/bin/fix-permissions # Create jovyan user with UID=1000 and in the 'users' group # and make sure these dirs are writable by the `users` group. RUN groupadd wheel -g 11 \u0026amp;\u0026amp; \\ echo \u0026quot;auth required pam_wheel.so use_uid\u0026quot; \u0026gt;\u0026gt; /etc/pam.d/su \u0026amp;\u0026amp; \\ useradd -m -s /bin/bash -N -u $NB_UID $NB_USER \u0026amp;\u0026amp; \\ mkdir -p $CONDA_DIR \u0026amp;\u0026amp; \\ chown $NB_USER:$NB_GID $CONDA_DIR \u0026amp;\u0026amp; \\ chmod g+w /etc/passwd \u0026amp;\u0026amp; \\ fix-permissions $HOME \u0026amp;\u0026amp; \\ fix-permissions $CONDA_DIR USER $NB_UID # Setup work directory for backward-compatibility RUN mkdir /home/$NB_USER/work \u0026amp;\u0026amp; \\ fix-permissions /home/$NB_USER # Install conda as jovyan and check the md5 sum provided on the download site ENV MINICONDA_VERSION 4.5.11 RUN cd /tmp \u0026amp;\u0026amp; \\ wget --quiet https://repo.continuum.io/miniconda/Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh \u0026amp;\u0026amp; \\ echo \u0026quot;e1045ee415162f944b6aebfe560b8fee *Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh\u0026quot; | md5sum -c - \u0026amp;\u0026amp; \\ /bin/bash Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh -f -b -p $CONDA_DIR \u0026amp;\u0026amp; \\ rm Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh \u0026amp;\u0026amp; \\ $CONDA_DIR/bin/conda config --system --prepend channels conda-forge \u0026amp;\u0026amp; \\ $CONDA_DIR/bin/conda config --system --set auto_update_conda false \u0026amp;\u0026amp; \\ $CONDA_DIR/bin/conda config --system --set show_channel_urls true \u0026amp;\u0026amp; \\ $CONDA_DIR/bin/conda install --quiet --yes conda=\u0026quot;${MINICONDA_VERSION%.*}.*\u0026quot; \u0026amp;\u0026amp; \\ $CONDA_DIR/bin/conda update --all --quiet --yes \u0026amp;\u0026amp; \\ conda clean -tipsy \u0026amp;\u0026amp; \\ rm -rf /home/$NB_USER/.cache/yarn \u0026amp;\u0026amp; \\ fix-permissions $CONDA_DIR \u0026amp;\u0026amp; \\ fix-permissions /home/$NB_USER # Install Tini RUN conda install --quiet --yes 'tini=0.18.0' \u0026amp;\u0026amp; \\ conda list tini | grep tini | tr -s ' ' | cut -d ' ' -f 1,2 \u0026gt;\u0026gt; $CONDA_DIR/conda-meta/pinned \u0026amp;\u0026amp; \\ conda clean -tipsy \u0026amp;\u0026amp; \\ fix-permissions $CONDA_DIR \u0026amp;\u0026amp; \\ fix-permissions /home/$NB_USER # Install Jupyter Notebook, Lab, and Hub # Generate a notebook server config # Cleanup temporary files # Correct permissions # Do all this in a single RUN command to avoid duplicating all of the # files across image layers when the permissions change RUN conda install --quiet --yes \\ 'notebook=5.7.2' \\ 'jupyterhub=0.9.4' \\ 'jupyterlab=0.35.4' \u0026amp;\u0026amp; \\ conda clean -tipsy \u0026amp;\u0026amp; \\ jupyter labextension install @jupyterlab/hub-extension@^0.12.0 \u0026amp;\u0026amp; \\ npm cache clean --force \u0026amp;\u0026amp; \\ jupyter notebook --generate-config \u0026amp;\u0026amp; \\ rm -rf $CONDA_DIR/share/jupyter/lab/staging \u0026amp;\u0026amp; \\ rm -rf /home/$NB_USER/.cache/yarn \u0026amp;\u0026amp; \\ fix-permissions $CONDA_DIR \u0026amp;\u0026amp; \\ fix-permissions /home/$NB_USER USER root EXPOSE 8888 WORKDIR $HOME # Configure container startup ENTRYPOINT [\u0026quot;tini\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;--\u0026quot;] CMD [\u0026quot;start-notebook.sh\u0026quot;] # Add local files as late as possible to avoid cache busting COPY start.sh /usr/local/bin/ COPY start-notebook.sh /usr/local/bin/ COPY start-singleuser.sh /usr/local/bin/ COPY jupyter_notebook_config.py /etc/jupyter/ RUN fix-permissions /etc/jupyter/ # Switch back to jovyan to avoid accidental container runs as root USER $NB_UID  It seems that the implementation at the Deploy-Docker repository actually handles multi-users, and you can CHOOSE ANY OF the aforementioned builds from Jupyter\u0026rsquo;s Page on Docker Cloud\nI think I will try this approach. The key difference here is that users are isolated through where docker mounts their directories, not through permissions.\nSo the idea is that you have a Linux machine running docker. All the students files are in some directory there. The Hub spawns up a Docker container on-demand for each student, mounting them appropriately. A teacher can access the files since they are simply the user that is using Docker. (anyone with sudo permissions on the UNIX machine? or just part of the correct usergroup? Joe can help there.) When the teachers log in, their accounts get mounted in a place they can see the whole class, but not other instructor\u0026rsquo;s classes!\nVirtualization can be used to segment classes, or they can run together. It doesn\u0026rsquo;t matter. Port-forwarding with sub-addresses can be used to direct to the correct hub for login. One hub runs per class. One docker instance manages all the hubs.\nI\u0026rsquo;m going to try it this way first, rather than the isolated UNIX-machine way.\nThe \u0026ldquo;Correct Way\u0026rdquo; We will be using dockerspawner, and the directions in their README.\nActually\u0026hellip; It seems this is what the point of Deploy Docker is. Slight modifications are made so that the dockerspawner launches up our chosen base-image!\nChoose a pinned image on docker cloud based on the builds from the stacks on github.\nSince we really need all the stuff included in the scipy stack, and these images are no smaller than 2GB, we might as well spring for the datascience stack since they are the same size and include R and Julia, which our colleagues may appreciate.\nAs of this writing, here is a recent tag: 7254cdcfa22b\nwhen running, be aware that no sudo priviledges will be granted. -e GRANT_SUDO=yes needs to be included upon docker run.\nSo, following the Deploy-Docker Dockerfile, I can see that it is running relatively recent versions of jupyterhub and dockerspawner, despite many files being unchanged for years. That\u0026rsquo;s actually kind of promising for maintenance purposes.\nStarting from Zero\u0026hellip; SSH into my main machine. Move into a directory to perform the cloning. Needed to first run\nsudo apt install docker-compose\ngit clone https://github.com/jupyterhub/jupyterhub-deploy-docker.git cd jupyterhub-deploy-docker mkdir -p secrets cd secrets openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem # bad idea? mv mycert.pem jupyterhub.crt mv mykey.key jupyterhub.key cd .. mv .env # change line 19 to DOCKER_NOTEBOOK_IMAGE=jupyter/datascience-notebook:7254cdcfa22b  In jupyterhub_config.py add the lines\nc.DockerSpawner.environment = { 'JUPYTER_ENABLE_LAB': 'yes' }  Now we face the authentication problem. We did generate a self-signed cert (hopefully), but the IP address of my workstation is not public. For extra security, best to keep it this way.\nWe want to change authentication to users\u0026hellip; The problem is that I don\u0026rsquo;t want to be setting up github users accounts. Lines 56-57 in juptyerhub_config.py are the problem:\nc.JupyterHub.authenticator_class = 'oauthenticator.GitHubOAuthenticator' c.GitHubOAuthenticator.oauth_callback_url = os.environ['OAUTH_CALLBACK_URL']  I think by commenting it out, PAMAuthenticator is default. Should be good enough? Well now if we change this we will have to create user accounts to log in with.\nLet\u0026rsquo;s create a userlist file in the root directory vim userlist\nmichael admin troy admin varis  Now to actually create these users. Let\u0026rsquo;s see how this was done in hippylib-hub:\n#!/bin/bash echo \u0026quot;Update Lab materials from GitHub\u0026quot; cd /home/fenics/Installations/labs \u0026amp;\u0026amp; git pull echo \u0026quot;Creating ${NUMBER_OF_USERS} new users...\u0026quot; for ((i = 0; i \u0026lt; ${NUMBER_OF_USERS}; i++)); do password=\u0026quot;Breckenridge${i}_g2s3\u0026quot; useradd \u0026quot;user${i}\u0026quot; -m -s /bin/bash echo \u0026quot;user${i}:${password}\u0026quot; | chpasswd user${i} cp -rf /home/fenics/.jupyter /home/user${i}/.jupyter chown -R user${i} /home/user${i}/.jupyter chmod -R u+rX /home/user${i}/.jupyter cp -r /home/fenics/Installations/labs/Labs /home/user${i}/ chown -R user${i} /home/user${i}/Labs chmod -R u+rX /home/user${i}/Labs echo \u0026quot;Created user${i} with password ${password}\u0026quot; done  So we have to translate this into our Dockerfile.jupyterhub file.\n# Copyright (c) Jupyter Development Team. # Distributed under the terms of the Modified BSD License. ARG JUPYTERHUB_VERSION FROM jupyterhub/jupyterhub-onbuild:$JUPYTERHUB_VERSION # Install dockerspawner, oauth, postgres RUN /opt/conda/bin/conda install -yq psycopg2=2.7 \u0026amp;\u0026amp; \\ /opt/conda/bin/conda clean -tipsy \u0026amp;\u0026amp; \\ /opt/conda/bin/pip install --no-cache-dir \\ oauthenticator==0.8.* \\ dockerspawner==0.9.* # We make additions here. RUN useradd michael -m -s /bin/bash RUN echo \u0026quot;michael:test_password\u0026quot; | chpasswd michael RUN echo \u0026quot;Created user Michael\u0026quot; # Copy TLS certificate and key ENV SSL_CERT /srv/jupyterhub/secrets/jupyterhub.crt ENV SSL_KEY /srv/jupyterhub/secrets/jupyterhub.key COPY ./secrets/*.crt $SSL_CERT COPY ./secrets/*.key $SSL_KEY RUN chmod 700 /srv/jupyterhub/secrets \u0026amp;\u0026amp; \\ chmod 600 /srv/jupyterhub/secrets/* COPY ./userlist /srv/jupyterhub/userlist  Let\u0026rsquo;s try it?? The instructions say all I need now is to run make build in the root directory.\nFailed. Commenting out lines referencing GitHub in Makefile (lines 23-24) and secrets/oauth.env (line 48).\nI dont have any clue what is going on in the secrets files. Doing my best but unable to make it work.\nOkay let\u0026rsquo;s try this.\ndocker run -p 8000:8000 -d --name jupyterhub jupyterhub/jupyterhub jupyterhub  My errors seem to be caused by outdated docker. Since I was running on 16.04, the apt repositories didn\u0026rsquo;t have newer versions, so I needed to manually add the updates:\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; sudo apt-get update apt-cache policy docker-ce sudo apt-get install -y docker-ce  after setting ARG JUPYTERHUB_VERSION=0.90 in the header, I was able to finally run make build (but who knows how it will work).\nThere\u0026rsquo;s no particular reason Hub has to run in a docker container. It can still spawn up new processes with docker.\nOnce make build finished, I had to run docker-compose up -d\ndocker ps revealed two containers. One is a database of some sort (what was causing me trouble with installation earlier before I faked the files).\nLogs are showing me that my problem is indeed with POSTGRES_PASSWORD \u0026hellip; and this part is certainly a bit above my paygrade.\n","date":1545634800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546378980,"objectID":"d29ef9ce610e6a9a28601b51f6185671","permalink":"https://www.michaelpilosov.com/devlog/docker/","publishdate":"2018-12-24T00:00:00-07:00","relpermalink":"/devlog/docker/","section":"devlog","summary":"JupyterHub Day!!","tags":["dev","jupyter","openscience","docker"],"title":"Early Exploration with Docker \u0026 Kubernetes","type":"docs"},{"authors":null,"categories":null,"content":" Table of Contents  Introduction  Images Containers  Comparison to Virtual Machines  Documentation  Install  Linux macOS Windows  Configure  Permissions Start on boot Re-route IP Troubleshooting  IP Forwarding Limiting   Security  Security Tips Deployment  Making Docker Safe for Production  Port-Exposal More  User Namespaces The Security Roadmap Security Videos   Cheat Sheet  Images  Lifecycle Info Cleaning up Load/Save image  Layers Containers  Lifecycle Starting and Stopping Info Import / Export Executing Commands Container Import/Export  Management  CPU Constraints Memory Constraints Capabilities  Dockerfile  Instructions Tutorial Examples  Networks  Lifecycle Info Connection  Registries  Registries v. Repositories Run local registry  Links Volumes  Lifecycle Info   Useful Commands/Tips  Versions Basics  Get IP Address Get Port Mapping  General  Find containers using regular expression: Get environment settings Kill running containers Delete all containers (force!! running or stopped containers) Delete old containers Delete stopped containers Delete containers after stopping Delete dangling images Delete all images Delete dangling volumes Show image dependencies df Heredoc Docker Container Prune Last Ids Commit  Monitoring  Monitor system resource utilization for running containers Volumes can be files  Efficiency Best Practices    Introduction From the Docker Documentation:\n Docker is a platform for developers and sysadmins to develop, deploy, and run applications with containers. \u0026gt; The use of Linux containers to deploy applications is called containerization. Containers are not new, but their use for easily deploying applications is.\nContainerization is increasingly popular because containers are:\n Flexible: Even the most complex applications can be containerized. Lightweight: Containers leverage and share the host kernel. Interchangeable: You can deploy updates and upgrades on-the-fly. Portable: You can build locally, deploy to the cloud, and run anywhere. Scalable: You can increase and automatically distribute container replicas. Stackable: You can stack services vertically and on-the-fly.   Images An image is a collection of files (a package) that is executable. It has all the files necessary to run an application, from dependencies to configuration files.\nIt is analogous to a class from object-oriented programming.\nContainers \u0026ldquo;A container is a runtime instance of an image,\u0026rdquo; or in other words, it is an instance of a class. One image can be used to \u0026ldquo;spin up\u0026rdquo; many containers. A container is what an image becomes (in the computer\u0026rsquo;s memory) when it is launched. It is a user process with a state and need for access to resources.\nDocker being a \u0026ldquo;machine\u0026rdquo; of sorts, it has its own processes, which happen to be instances of images\u0026ndash;containers.\nJust as you would in Linux, you can see a list of your running containers by issuing (sudo) docker ps (more on that soon).\nComparison to Virtual Machines  A container runs natively on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight.\nBy contrast, a virtual machine (VM) runs a full-blown “guest” operating system with virtual access to host resources through a hypervisor. In general, VMs provide an environment with more resources than most applications need.\n Here is a Virtual Machine:\nAnd here is a Container:\nNotice how Docker sits atop the Host Operating System 1 and is performing the management role of resources among applications in place of a \u0026ldquo;hypervisor\u0026rdquo; from a Virtual Machine.\nDocumentation Here is the Cheatsheet from which I will be pulling much of what is in this post.\nback to top\nInstall Linux Quick and easy install script provided by Docker:\ncurl -sSL https://get.docker.com/ | sh  From the author of the cheatsheet:\n If you\u0026rsquo;re not willing to run a random shell script, please see the installation instructions for your distribution. If you are a complete Docker newbie, you should follow the series of tutorials now.\n macOS Download and install Docker Community Edition. if you have Homebrew-Cask, just type brew cask install docker.\nOnce you\u0026rsquo;ve installed Docker Community Edition, click the docker icon in Launchpad. Then start up a container:\ndocker run hello-world  Note: You may have to restart your shell session (either by closing and re-launching a new one, or logging out of your remote server connection with Ctrl-D and logging back in with ssh). This is what I had to do.\nIf successful, you should see a \u0026ldquo;Hello from Docker!\u0026rdquo; printout in your console.\nAnd that\u0026rsquo;s it, you have a running Docker container (this one comes with the install).\nHowever, we are not quite done yet. Let\u0026rsquo;s get to some configuration\u0026hellip;\nWindows Go deal with it yourself. It\u0026rsquo;s similar to the Desktop-version available for Mac, but comes with all sorts of caveats you should read through first. It should be fairly straightforward for Windows 10 Users. My suggestion is to simply go with Linux, since our focus is on using Docker for deploying to servers, which are unlikely to be running Windows for the use-cases we have in mind. That said, should be doable, if you insist on it.\nback to top\nConfigure One thing you may notice is that docker commands require the use of sudo, which we would like to avoid. To avoid permission errors (and the use of sudo), add your user to the docker group.\nPost-Installation Steps contains optional procedures for configuring Linux hosts to work better with Docker. The following is taken from that source, and much more Troubleshooting Information can be found there.\nPermissions To create the docker group and add your user:\n Create the docker group. Add your user to the docker group. Log out and log back in so that your group membership is re-evaluated. Some caveats may apply. 2  sudo groupadd docker sudo usermod -aG docker $USER  Verify that you can run docker commands without sudo.\ndocker run hello-world  This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits.\n If you initially ran Docker CLI commands using sudo before adding your user to the docker group, you may see the following error, which indicates that your ~/.docker/ directory was created with incorrect permissions due to the sudo commands.\nWARNING: Error loading config file: /home/user/.docker/config.json - stat /home/user/.docker/config.json: permission denied  To fix this problem, either remove the ~/.docker/ directory (it is recreated automatically, but any custom settings are lost), or change its ownership and permissions using the following commands:\nsudo chown \u0026quot;$USER\u0026quot;:\u0026quot;$USER\u0026quot; /home/\u0026quot;$USER\u0026quot;/.docker -R sudo chmod g+rwx \u0026quot;$HOME/.docker\u0026quot; -R   Start on boot Sometimes you want Docker to be the main thing running on a server and thus started up on boot (for the occasional restart). This feature may be desired for servers that host critical processes using Docker. Most current Linux distributions (RHEL, CentOS, Fedora, Ubuntu 16.04 and higher) use systemd to manage which services start when the system boots. 3\nsudo systemctl enable docker  To disable this behavior, use disable instead.\nsudo systemctl disable docker  Re-route IP By default, the Docker daemon listens for connections on a UNIX socket to accept requests from local clients.\nIt is possible to allow Docker to accept requests from remote hosts by configuring it to listen on an IP address and port as well as the UNIX socket.\n For more detailed information on this configuration option take a look at “Bind Docker to another host/port or a unix socket” section of the Docker CLI Reference article.\nSecurity Notice: Before configuring Docker to accept connections from remote hosts it is critically important that you understand the security implications of opening docker to the network. If steps are not taken to secure the connection, it is possible for remote non-root users to gain root access on the host. For more information on how to use TLS certificates to secure this connection, check this article on how to protect the Docker daemon socket.\n Configuring Docker to accept remote connections can be done with the docker.service systemd unit file for Linux distributions using systemd.\n Use the command sudo systemctl edit docker.service to open an override file for docker.service in a text editor.\nAdd or modify the following lines, substituting your own values.\n[Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// -H tcp://127.0.0.1:2375 Save the file.  Reload the systemctl configuration and restart Docker.\nsudo systemctl daemon-reload sudo systemctl restart docker.service   Check to see whether the change was honored by reviewing the output of netstat to confirm dockerd is listening on the configured port, which should look similar to:\n$ sudo netstat -lntp | grep dockerd tcp 0 0 127.0.0.1:2375 0.0.0.0:* LISTEN 3758/dockerd   To enable IPv6 on the Docker daemon, see Enable IPv6 support.\nTroubleshooting More troubleshooting information can be found in the Troubleshooting section of the Post-Install documentation page.\nHere we attempt to address just a couple of the most common things that we may have to do.\nIP Forwarding If you manually configure your network using systemd-network with systemd version 219 or higher, Docker containers may not be able to access your network.\n Beginning with systemd version 220, the forwarding setting for a given network (net.ipv4.conf.\u0026lt;interface\u0026gt;.forwarding) defaults to off. This setting prevents IP forwarding. It also conflicts with Docker’s behavior of enabling the net.ipv4.conf.all.forwarding setting within containers.\n  To work around this on RHEL, CentOS, or Fedora, edit the \u0026lt;interface\u0026gt;.network file in /usr/lib/systemd/network/ on your Docker host (ex: /usr/lib/systemd/network/80-container-host0.network) and add the following block within the [Network] section.\n[Network] ... IPForward=kernel # OR IPForward=true ...   This configuration allows IP forwarding from the container as expected.\nLimiting You may see\nWARNING: Your kernel does not support swap limit capabilities. Limitation discarded.   This warning does not occur on RPM-based systems, which enable these capabilities by default. If you don’t need these capabilities, you can ignore the warning.\n You can enable these capabilities on Ubuntu or Debian by following these instructions. Memory and swap accounting incur an overhead of about 1% of the total available memory and a 10% overall performance degradation, even if Docker is not running.\n  Log into the Ubuntu or Debian host as a user with sudo privileges.\n Edit the /etc/default/grub file. Add or edit the GRUB_CMDLINE_LINUX line to add the following two key-value pairs:\n  GRUB_CMDLINE_LINUX=\u0026quot;cgroup_enable=memory swapaccount=1\u0026quot;\nSave and close the file.\n Update GRUB. sudo update-grub  If your GRUB configuration file has incorrect syntax, an error occurs. In this case, repeat steps 3 and 4. The changes take effect when the system is rebooted.\n back to top\nSecurity  This is where security tips about Docker go. The Docker security page goes into more detail.\n First things first: Docker runs as root.\nIf you are in the docker group, you effectively have root access.\nIf you expose the docker UNIX socket to a container, you are giving the container root access to the host.\n Docker should not be your only defense. You should secure and harden it.\n The security tips following are useful if you\u0026rsquo;ve already hardened containers in the past, but are not a substitute for understanding. 4\nSecurity Tips For greatest security, you want to run Docker inside a virtual machine. (Source: Docker Security Team Lead slides / notes.\n Then, run with AppArmor / seccomp / SELinux / grsec etc to limit the container permissions. See the Docker 1.10 security features for more details.\nDocker image ids are sensitive information and should not be exposed to the outside world. Treat them like passwords.\n See the Docker Security Cheat Sheet by Thomas Sjögren: some good stuff about container hardening in there.\nCheck out the docker bench security script for a security benchmark.\n  Download the white papers and subscribe to the mailing lists (unfortunately Docker does not have a unique mailing list, only dev / user). To begin with, see this (foot)note from the cheatsheet 5.\n Deployment Making Docker Safe for Production Since Docker 1.11, you can easily limit the number of active processes running inside a container to prevent fork bombs.\n This requires a Linux kernel \u0026gt;= 4.3 with CGROUP_PIDS=y to be in the kernel configuration.\ndocker run --pids-limit=64  Also available since docker 1.11 is the ability to prevent processes from gaining new privileges.\n This feature have been in the Linux kernel since version 3.5. You can read more about it in this blog post.\ndocker run --security-opt=no-new-privileges   From the Docker Security Cheat Sheet (it\u0026rsquo;s in PDF which makes it hard to use, so copying below) by Container Solutions :\n Be aware that the following may affect the performance of your applications in unexpected ways if you are not sure what kinds of communications requirements you need.\nProceed with caution. Reference the [presentation above][docker-production].\n  Turn off interprocess communication with:\ndocker -d --icc=false --iptables  Set the container to be read-only:\ndocker run --read-only  Verify images with a hashsum:\ndocker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be  Set volumes to be read only:\ndocker run -v $(pwd)/secrets:/secrets:ro debian  Define and run a user in your Dockerfile so you don\u0026rsquo;t run as root inside the container:\nRUN groupadd -r user \u0026amp;\u0026amp; useradd -r -g user user USER user   Port-Exposal Exposing incoming ports through the host container is fiddly but doable.\n This is done by mapping the container port to the host port (only using localhost interface) using -p:\ndocker run -p 127.0.0.1:$HOSTPORT:$CONTAINERPORT --name CONTAINER -t someimage  You can tell Docker that the container listens on the specified network ports at runtime by using EXPOSE:\nEXPOSE \u0026lt;CONTAINERPORT\u0026gt;  Note that EXPOSE does not expose the port itself \u0026ndash; only -p will do that. To expose the container\u0026rsquo;s port on your localhost\u0026rsquo;s port:\niptables -t nat -A DOCKER -p tcp --dport \u0026lt;LOCALHOSTPORT\u0026gt; -j DNAT --to-destination \u0026lt;CONTAINERIP\u0026gt;:\u0026lt;PORT\u0026gt;  If you\u0026rsquo;re running Docker in Virtualbox, you then need to forward the port there as well, using forwarded_port. Define a range of ports in your Vagrantfile like this so you can dynamically map them:\nVagrant.configure(VAGRANTFILE_API_VERSION) do |config| ... (49000..49900).each do |port| config.vm.network :forwarded_port, :host =\u0026gt; port, :guest =\u0026gt; port end ... end  If you forget what you mapped the port to on the host container, use docker port to show it:\ndocker port CONTAINER $CONTAINERPORT  More User Namespaces There\u0026rsquo;s also work on user namespaces \u0026ndash; it is in 1.10 but is not enabled by default.\nTo enable user namespaces (\u0026ldquo;remap the userns\u0026rdquo;) in Ubuntu 15.10, follow the blog example.\nThe Security Roadmap The Docker roadmap talks about seccomp support. There is an AppArmor policy generator called bane, and they\u0026rsquo;re working on security profiles.\nSecurity Videos  Using Docker Safely Securing your applications using Docker Container security: Do containers actually contain? Linux Containers: Future or Fantasy?  back to top\nCheat Sheet The following is full Cheat Sheet mentioned earlier, and presented here for convenience. Feel free to use the table of contents on the right sidebar (or duplicated below for mobile users) to more easily navigate this page than on the Github Gist 6.\nTable of Contents\nImages Images are just templates for docker containers.\nLifecycle  docker images shows all images. docker import creates an image from a tarball. docker build creates image from Dockerfile. docker commit creates image from a container, pausing it temporarily if it is running. docker rmi removes an image. docker load loads an image from a tar archive as STDIN, including images and tags (as of 0.7). docker save saves an image to a tar archive stream to STDOUT with all parent layers, tags \u0026amp; versions (as of 0.7).  Info  docker history shows history of image. docker tag tags an image to a name (local or registry).  Cleaning up While you can use the docker rmi command to remove specific images, there\u0026rsquo;s a tool called docker-gc that will safely clean up images that are no longer used by any containers.\nLoad/Save image Load an image from file:\ndocker load \u0026lt; my_image.tar.gz  Save an existing image:\ndocker save my_image:my_tag | gzip \u0026gt; my_image.tar.gz  Layers The versioned filesystem in Docker is based on layers. They\u0026rsquo;re like git commits or changesets for filesystems.\nEventually you\u0026rsquo;ll want to make changes to an existing image, and will find yourself manipulating the Dockerfile that defines the build and configuration of the image. You can think of this as mimicking the keystrokes a user would have to enter in order to set up each application on a fresh computer.\nPretty much every image we\u0026rsquo;ll be interested in will have it\u0026rsquo;s origin in some version of a Linux distribution on top of which a number of commands are run to define the files necessary for the use-case.\nSince each \u0026ldquo;version\u0026rdquo; of an image is an entire filesystem, building one version of an image based on a previous one can lead to lots of unnecessary files being tracked.\nAs stated in the Dockerfile section, the command RUN executes any commands in a new layer on top of the current image, and commits the results.\nThis \u0026ldquo;on top\u0026rdquo; part is especially important to understand, and several things can be done to keep subsequent changes to an image relatively \u0026ldquo;lightweight.\u0026rdquo;\nFor example, make sure to clean up the APT repositories.\nContainers Your basic isolated Docker process. Containers are to Virtual Machines as threads are to processes. Or you can think of them as chroots on steroids.\nLifecycle  docker create creates a container but does not start it. docker rename allows the container to be renamed. docker run creates and starts a container in one operation. docker rm deletes a container. docker update updates a container\u0026rsquo;s resource limits.  Normally if you run a container without options it will start and stop immediately, if you want keep it running you can use the command, docker run -td container_id this will use the option -t that will allocate a pseudo-TTY session and -d that will detach automatically the container (run container in background and print container ID).\nIf you want a transient container, docker run --rm will remove the container after it stops.\nIf you want to map a directory on the host to a docker container, docker run -v $HOSTDIR:$DOCKERDIR. Also see Volumes.\nIf you want to remove also the volumes associated with the container, the deletion of the container must include the -v switch like in docker rm -v.\nThere\u0026rsquo;s also a logging driver available for individual containers in docker 1.10. To run docker with a custom log driver (i.e., to syslog), use docker run --log-driver=syslog.\nAnother useful option is docker run --name yourname docker_image because when you specify the --name inside the run command this will allow you to start and stop a container by calling it with the name the you specified when you created it.\nStarting and Stopping  docker start starts a container so it is running. docker stop stops a running container. docker restart stops and starts a container. docker pause pauses a running container, \u0026ldquo;freezing\u0026rdquo; it in place. docker unpause will unpause a running container. docker wait blocks until running container stops. docker kill sends a SIGKILL to a running container. docker attach will connect to a running container.  If you want to integrate a container with a host process manager, start the daemon with -r=false then use docker start -a.\nIf you want to expose container ports through the host, see the exposing ports section.\nRestart policies on crashed docker instances are covered here.\nInfo  docker ps shows running containers. docker logs gets logs from container. (You can use a custom log driver, but logs is only available for json-file and journald in 1.10). docker inspect looks at all the info on a container (including IP address). docker events gets events from container. docker port shows public facing port of container. docker top shows running processes in container. docker stats shows containers\u0026rsquo; resource usage statistics. docker diff shows changed files in the container\u0026rsquo;s FS.  docker ps -a shows running and stopped containers.\ndocker stats --all shows a running list of containers.\nImport / Export  docker cp copies files or folders between a container and the local filesystem. docker export turns container filesystem into tarball archive stream to STDOUT.  Executing Commands  docker exec to execute a command in container.  To enter a running container, attach a new shell process to a running container called foo, use: docker exec -it foo /bin/bash.\nContainer Import/Export Import a container as an image from file:\ncat my_container.tar.gz | docker import - my_image:my_tag  Export an existing container:\ndocker export my_container | gzip \u0026gt; my_container.tar.gz  The difference between loading a saved image and importing an exported container as an image\n Loading an image using the load command creates a new image including its history.\nImporting a container as an image using the import command creates a new image, excluding the history, which results in a smaller image size compared to loading an image.\n Management CPU Constraints You can limit CPU, either using a percentage of all CPUs, or by using specific cores.\nFor example, you can tell the cpu-shares setting. The setting is a bit strange \u0026ndash; 1024 means 100% of the CPU, so if you want the container to take 50% of all CPU cores, you should specify 512. See https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_cpu for more:\ndocker run -ti -c 512 agileek/cpuset-test  You can also only use some CPU cores using cpuset-cpus. See https://agileek.github.io/docker/2014/08/06/docker-cpuset/ for details and some nice videos:\ndocker run -ti --cpuset-cpus=0,4,6 agileek/cpuset-test  Note that Docker can still see all of the CPUs inside the container \u0026ndash; it just isn\u0026rsquo;t using all of them. See https://github.com/docker/docker/issues/20770 for more details.\nMemory Constraints You can also set memory constraints on Docker:\ndocker run -it -m 300M ubuntu:14.04 /bin/bash  Capabilities Linux capabilities can be set by using cap-add and cap-drop. See https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities for details.\nThis should be used for greater security.\n To mount a FUSE based filesystem, you need to combine both \u0026ndash;cap-add and \u0026ndash;device:\ndocker run --rm -it --cap-add SYS_ADMIN --device /dev/fuse sshfs  Give access to a single device:\ndocker run -it --device=/dev/ttyUSB0 debian bash  Give access to all devices:\ndocker run -it --privileged -v /dev/bus/usb:/dev/bus/usb debian bash  more info about privileged containers here\nDockerfile The configuration file.\nSets up a Docker container when you run docker build on it. Vastly preferable to docker commit.\nHere are some common text editors and their syntax highlighting modules you could use to create Dockerfiles:\n Sublime Text 2 Atom Vim Emacs Also see Docker meets the IDE  Instructions  .dockerignore FROM Sets the Base Image for subsequent instructions. MAINTAINER (deprecated - use LABEL instead) Set the Author field of the generated images. RUN execute any commands in a new layer on top of the current image and commit the results. CMD provide defaults for an executing container. EXPOSE informs Docker that the container listens on the specified network ports at runtime. NOTE: does not actually make ports accessible. ENV sets environment variable. ADD copies new files, directories or remote file to container. Invalidates caches. Avoid ADD and use COPY instead. COPY copies new files or directories to container. Note that this only copies as root, so you have to chown manually regardless of your USER / WORKDIR setting. See https://github.com/moby/moby/issues/30110 ENTRYPOINT configures a container that will run as an executable. VOLUME creates a mount point for externally mounted volumes or other containers. USER sets the user name for following RUN / CMD / ENTRYPOINT commands. WORKDIR sets the working directory. ARG defines a build-time variable. ONBUILD adds a trigger instruction when the image is used as the base for another build. STOPSIGNAL sets the system call signal that will be sent to the container to exit. LABEL apply key/value metadata to your images, containers, or daemons.  Tutorial  Flux7\u0026rsquo;s Dockerfile Tutorial  Examples  Examples Best practices for writing Dockerfiles Michael Crosby has some more Dockerfiles best practices / take 2. Building Good Docker Images / Building Better Docker Images Managing Container Configuration with Metadata How to write excellent Dockerfiles  Networks  Docker has a networks feature. Not much is known about it, so this is a good place to expand the cheat sheet. There is a note saying that it\u0026rsquo;s a good way to configure docker containers to talk to each other without using ports. See working with networks for more details.\n Lifecycle  docker network create docker network rm  Info  docker network ls docker network inspect  Connection  docker network connect docker network disconnect  You can specify a specific IP address for a container:\n# create a new bridge network with your subnet and gateway for your ip block $ docker network create --subnet 203.0.113.0/24 --gateway 203.0.113.254 iptastic # run a nginx container with a specific ip in that block $ docker run --rm -it --net iptastic --ip 203.0.113.2 nginx # curl the ip from any other place (assuming this is a public ip block duh) $ curl 203.0.113.2  Registries Registries v. Repositories A repository is a hosted collection of tagged images that together create the file system for a container.\n A registry is a host \u0026ndash; a server that stores repositories and provides an HTTP API for managing the uploading and downloading of repositories.\n  Docker.com hosts its own index to a central registry which contains a large number of repositories.\nHaving said that, the central docker registry does not do a good job of verifying images and should be avoided if you\u0026rsquo;re worried about security.\n  docker login to login to a registry. docker logout to logout from a registry. docker search searches registry for image. docker pull pulls an image from registry to local machine. docker push pushes an image to the registry from local machine.  Run local registry You can run a local registry by using the docker distribution project and looking at the local deploy instructions.\nAlso see the mailing list.\nLinks Links are how Docker containers talk to each other through TCP/IP ports. Linking into Redis and Atlassian show worked examples. You can also resolve links by hostname.\nThis has been deprected to some extent by user-defined networks.\nNOTE: If you want containers to ONLY communicate with each other through links, start the docker daemon with -icc=false to disable inter process communication.\nIf you have a container with the name CONTAINER (specified by docker run --name CONTAINER) and in the Dockerfile, it has an exposed port:\nEXPOSE 1337  Then if we create another container called LINKED like so:\ndocker run -d --link CONTAINER:ALIAS --name LINKED user/wordpress  Then the exposed ports and aliases of CONTAINER will show up in LINKED with the following environment variables:\n$ALIAS_PORT_1337_TCP_PORT $ALIAS_PORT_1337_TCP_ADDR  And you can connect to it that way.\nTo delete links, use docker rm --link.\nGenerally, linking between docker services is a subset of \u0026ldquo;service discovery\u0026rdquo;, a big problem if you\u0026rsquo;re planning to use Docker at scale in production. Please read The Docker Ecosystem: Service Discovery and Distributed Configuration Stores for more info.\nVolumes Docker volumes are free-floating filesystems. They don\u0026rsquo;t have to be connected to a particular container. You should use volumes mounted from data-only containers for portability.\nLifecycle  docker volume create docker volume rm  Info  docker volume ls docker volume inspect  Volumes are useful in situations where you can\u0026rsquo;t use links (which are TCP/IP only). For instance, if you need to have two docker instances communicate by leaving stuff on the filesystem.\nYou can mount them in several docker containers at once, using docker run --volumes-from.\nBecause volumes are isolated filesystems, they are often used to store state from computations between transient containers. That is, you can have a stateless and transient container run from a recipe, blow it away, and then have a second instance of the transient container pick up from where the last one left off.\nSee advanced volumes for more details. Container42 is also helpful.\nYou can map MacOS host directories as docker volumes:\ndocker run -v /Users/wsargent/myapp/src:/src  You can use remote NFS volumes if you\u0026rsquo;re feeling brave.\nYou may also consider running data-only containers as described here to provide some data portability.\nBe aware that you can mount files as volumes.\nback to top\nUseful Commands/Tips Sources:\n 15 Docker Tips in 5 minutes CodeFresh Everyday Hacks Docker  Versions  It is very important that you always know the current version of Docker you are currently running on at any point in time. This is very helpful because you get to know what features are compatible with what you have running. This is also important because you know what containers to run from the docker store when you are trying to get template containers. That said let see how to know what version of docker we have running currently\n  docker version checks what version of docker you have running Usage: docker version [OPTIONS]  Get the server version\n$ docker version --format '{{.Server.Version}}' 1.8.0  Dump raw JSON data\n$ docker version --format '{{json .}}' {\u0026quot;Client\u0026quot;:{\u0026quot;Version\u0026quot;:\u0026quot;1.8.0\u0026quot;,\u0026quot;ApiVersion\u0026quot;:\u0026quot;1.20\u0026quot;,\u0026quot;GitCommit\u0026quot;:\u0026quot;f5bae0a\u0026quot;,\u0026quot;GoVersion\u0026quot;:\u0026quot;go1.4.2\u0026quot;,\u0026quot;Os\u0026quot;:\u0026quot;linux\u0026quot;,\u0026quot;Arch\u0026quot;:\u0026quot;am\u0026quot;}  Basics Get IP Address docker inspect $(dl) | grep -wm1 IPAddress | cut -d '\u0026quot;' -f 4  or install jq:\ndocker inspect $(dl) | jq -r '.[0].NetworkSettings.IPAddress'  or using a go template:\ndocker inspect -f '{{ .NetworkSettings.IPAddress }}' \u0026lt;container_name\u0026gt;  or when building an image from Dockerfile, when you want to pass in a build argument:\nDOCKER_HOST_IP=`ifconfig | grep -E \u0026quot;([0-9]{1,3}\\.){3}[0-9]{1,3}\u0026quot; | grep -v 127.0.0.1 | awk '{ print $2 }' | cut -f2 -d: | head -n1` echo DOCKER_HOST_IP = $DOCKER_HOST_IP docker build \\ --build-arg ARTIFACTORY_ADDRESS=$DOCKER_HOST_IP -t sometag \\ some-directory/  Get Port Mapping docker inspect -f '{{range $p, $conf := .NetworkSettings.Ports}} {{$p}} -\u0026gt; {{(index $conf 0).HostPort}} {{end}}' \u0026lt;containername\u0026gt;  General Find containers using regular expression: for i in $(docker ps -a | grep \u0026quot;REGEXP_PATTERN\u0026quot; | cut -f1 -d\u0026quot; \u0026quot;); do echo $i; done  Get environment settings docker run --rm ubuntu env  Kill running containers docker kill $(docker ps -q)  Delete all containers (force!! running or stopped containers) docker rm -f $(docker ps -qa)  Delete old containers docker ps -a | grep 'weeks ago' | awk '{print $1}' | xargs docker rm  Delete stopped containers docker rm -v $(docker ps -a -q -f status=exited)  Delete containers after stopping docker stop $(docker ps -aq) \u0026amp;\u0026amp; docker rm -v $(docker ps -aq)  Delete dangling images docker rmi $(docker images -q -f dangling=true)  Delete all images docker rmi $(docker images -q)  Delete dangling volumes As of Docker 1.9:\ndocker volume rm $(docker volume ls -q -f dangling=true)  In 1.9.0, the filter dangling=false does not work - it is ignored and will list all volumes.\nShow image dependencies docker images -viz | dot -Tpng -o docker.png  df docker system df presents a summary of the space currently used by different docker objects.\nHeredoc Docker Container docker build -t htop - \u0026lt;\u0026lt; EOF FROM alpine RUN apk --no-cache add htop EOF  Prune The new Data Management Commands have landed as of Docker 1.13:\n docker system prune docker volume prune docker network prune docker container prune docker image prune  Last Ids alias dl='docker ps -l -q' docker run ubuntu echo hello world docker commit $(dl) helloworld  Commit with command (needs Dockerfile)\ndocker commit -run='{\u0026quot;Cmd\u0026quot;:[\u0026quot;postgres\u0026quot;, \u0026quot;-too -many -opts\u0026quot;]}' $(dl) postgres  Monitoring Monitor system resource utilization for running containers To check the CPU, memory, and network I/O usage of a single container, you can use:\ndocker stats \u0026lt;container\u0026gt;  For all containers listed by id:\ndocker stats $(docker ps -q)  For all containers listed by name:\ndocker stats $(docker ps --format '{{.Names}}')  For all containers listed by image:\ndocker ps -a -f ancestor=ubuntu  Remove all untagged images\ndocker rmi $(docker images | grep “^” | awk '{split($0,a,\u0026quot; \u0026quot;); print a[3]}')  Remove container by a regular expression\ndocker ps -a | grep wildfly | awk '{print $1}' | xargs docker rm -f  Remove all exited containers\ndocker rm -f $(docker ps -a | grep Exit | awk '{ print $1 }')  Volumes can be files Be aware that you can mount files as volumes. For example you can inject a configuration file like this:\n# copy file from container docker run --rm httpd cat /usr/local/apache2/conf/httpd.conf \u0026gt; httpd.conf # edit file vim httpd.conf # start container with modified configuration docker run --rm -ti -v \u0026quot;$PWD/httpd.conf:/usr/local/apache2/conf/httpd.conf:ro\u0026quot; -p \u0026quot;80:80\u0026quot; httpd  Efficiency  Cleaning APT in a RUN layer. Note 7\nRUN {apt commands} \\ \u0026amp;\u0026amp; apt-get clean \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*  Flatten an image\nID=$(docker run -d image-name /bin/bash) docker export $ID | docker import – flat-image-name  For backup\nID=$(docker run -d image-name /bin/bash) (docker export $ID | gzip -c \u0026gt; image.tgz) gzip -dc image.tgz | docker import - flat-image-name   Best Practices This is where general Docker best practices and war stories go:\n The Rabbit Hole of Using Docker in Automated Tests Bridget Kromhout has a useful blog post on running Docker in production (2014) at Dramafever. There\u0026rsquo;s also a best practices blog post (2014) from Lyst. Building a Development Environment With Docker (2013) Discourse in a Docker Container (2013)   In this tutorial, we will be using Linux since that is what almost every server runs, but as the principle of Docker is that it makes applications independent of platforms, everything herein should be applicable no matter what machine you are running. ^ If testing on a virtual machine, it may be necessary to restart the virtual machine for changes to take effect. On a desktop Linux environment such as X Windows, log out of your session completely and then log back in. ^ Ubuntu 14.10 and below use upstart. See the post-installation instructions for support. ^ For an understanding of what containers leave exposed, you should read Understanding and Hardening Linux Containers by Aaron Grattafiori. This is a complete and comprehensive guide to the issues involved with containers, with a plethora of links and footnotes leading on to yet more useful content. ^ You should start off by using a kernel with unstable patches for grsecurity / pax compiled in, such as Alpine Linux. If you are using grsecurity in production, you should spring for commercial support for the stable patches, same as you would do for RedHat. It\u0026rsquo;s $200 a month, which is nothing to your devops budget. ^ The gist was scraped and mildly edited on 12/22/18, so it may behoove you to check the original source for any updates. If you find typos/corrections/updates that should be included below, please get in touch. ^ This should be done in the same layer as other apt commands. Otherwise, the previous layers still persist the original information and your images will still be fat. ^   ","date":1545462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545462000,"objectID":"1d40bfc572be293df926bd136a1c1834","permalink":"https://www.michaelpilosov.com/openscience/docker/","publishdate":"2018-12-22T00:00:00-07:00","relpermalink":"/openscience/docker/","section":"openscience","summary":"Table of Contents  Introduction  Images Containers  Comparison to Virtual Machines  Documentation  Install  Linux macOS Windows  Configure  Permissions Start on boot Re-route IP Troubleshooting  IP Forwarding Limiting   Security  Security Tips Deployment  Making Docker Safe for Production  Port-Exposal More  User Namespaces The Security Roadmap Security Videos   Cheat Sheet  Images  Lifecycle Info Cleaning up Load/Save image  Layers Containers  Lifecycle Starting and Stopping Info Import / Export Executing Commands Container Import/Export  Management  CPU Constraints Memory Constraints Capabilities  Dockerfile  Instructions Tutorial Examples  Networks  Lifecycle Info Connection  Registries  Registries v.","tags":null,"title":"Docker","type":"docs"},{"authors":null,"categories":null,"content":" Adding a Hub Introduction This document assumes that you have followed the walkthrough at least once before. You already have:\n Logged in with an account that has sudo permissions Docker installed Proxy configured, secured with HTTPS  This walk-through will guide you through the bare-minimal steps to set up a new hub.\nPreliminaries First, we need to decide on the following:\n Name of new hub An available port for it to be hosted on  We will run the following commands to find out what is already in use:\ndocker ps --format \u0026quot;table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\u0026quot;  You will see the names and ports formatted printed out. Here is an example of what that might look like:\nNAMES STATUS PORTS math8660-user-troy Up 32 minutes 8888/tcp math-user-michael Up 3 days 8888/tcp math8660 Up 3 days 0.0.0.0:8002-\u0026gt;8000/tcp math8660-db Up 3 days 5432/tcp math Up 3 days 0.0.0.0:8000-\u0026gt;8000/tcp math-db Up 3 days 5432/tcp  What we are seeing is a list of processes being run by Docker and their respective ports. The ones that read 8888/tcp are on the local docker-network (these are the single-user notebook servers). It is fine for these numbers to conflict since they are not ports open on the main server. The databases on 5432 are similarly of no concern to us.\nIf the output of this command is blank (perhaps this is your first time), then nothing is running, and so you can stick with the default name and port number in our deployment.\n We can also see that three hubs are live on this server, listening for connections on local ports 8000 and 8001 in addition to the two user containers.\nWe simply need to choose any port other than 8000 or 8002 (e.g. 8001, 8003, 8451, 8762, etc.).\nWe also see that the two names in use are math (default in our deployment) and math8660, so we would have to choose a different name. In our deployment, this name is ALSO where your hub will live (example.com/math8660 and example.com/math are live in the above output).\nThe proxy configurations must direct traffic to these \u0026ldquo;locations\u0026rdquo; (/math and /math8660) for it to be publicly accessible. See the Proxy section of the Walkthrough.\n Before proceeding, let us get the necessary files.\nClone the repository from GitHub or copy an existing hub\u0026rsquo;s directory and rename the folder, or upload the files in any way you want.\nRename it to something memorable (perhaps the name of your hub). Here we choose \u0026ldquo;deploy\u0026rdquo; as our folder name.\ngit clone https://github.com/mathematicalmichael/jupyterhub-deploy-docker.git mv jupyterhub-deploy-docker/ deploy/ cd deploy  The last line brings us into the deploy directory, where the instructions that follow pick up. They assume that you have cloned the repo.\nQuick Install I have added a convenience script that allows for automatic creation of a hub. It creates one default hub-admin user and prints the password as output when it completes the build process and the hub is live. It will also print out the entry that needs to be added to /etc/nginx/sites-enabled/hub.conf in order to add your hub.\nInstructions If you have no existing hub named math and port 8000 is available, then nothing needs to be edited. (If you need to reassign HUB_NAME and PORT_NUM, edit the first lines of .env). Once you are ready, run:\nThis may take 10-15 minutes in total but will handle everything for you.\n ./setup.sh  This script presumes this is your first run. It creates several files that are only required initially, and re-running it will delete any changes you made to userlist unless you comment out the first line (which creates this file).\n A dialogue will come across the screen tell you the password to get into the hub and what to add to the proxy configuration. Once you log in, you can add users through the Admin panel.\nHowever, to manage shared volumes for groups/teams, you must change userlist accordingly.\nGroup Memberships The setup.sh script additionally creates one group (team) named shared and associated volume, mounting it to hub-admin\u0026rsquo;s working directory. All shared (group) directories are prepended with shared-, which is what jupyterhub_config.py is expecting when it reads userlist to determine group memberships.\nIf you open up userlist and add more users who belong to the shared group, they will also see this folder and be able to read/write to it. You can add any phrase (groupname) to a user\u0026rsquo;s line, but be sure to create the volume and set its permissions before refreshing the hub (if you do not do this, Docker will create the volume automatically, but users will only be given read permissions by default).\nTo create volumes for new groups and set the permissions, run the following command (which parses information to find where Docker has linked the new volume, and passes the directory to chmod:\nexport SHARED_VOLUME_NAME=newGroupNameHere docker volume create shared-$SHARED_VOLUME_NAME sudo chmod 777 $(docker inspect shared-$SHARED_VOLUME_NAME | grep \u0026quot;Mountpoint\u0026quot; | awk '{print $2}' | sed 's/\u0026quot;//g' | sed 's/,//g')  Be sure to change the above to reflect any group names you (or your students) choose, and run it for every new group name.\n An example userlist might look like (group orders do not matter):\nhub-admin shared admin mpilosov admin shared team-1 halljord team-1 tbutler admin team-1  Finding out Passwords After adding users/groups to userlist, you will have to restart hub for changes to take effect. This can be accomplished through the Control Panel by clicking \u0026ldquo;Shut Down Hub\u0026rdquo; and refreshing the page).\nLog in as hub-admin and visit /hub/login_list to see all users and passwords from userlist.\nAdditionally, if you edit the last line of .env and uncomment the last lines of show_login.sh, you can see individual user passwords from the command-line by running ./show_login.sh from the root project directory (here, /deploy).\nSharing Passwords To share passwords across hubs, simply be sure that secrets/oauth.env is the same. Copy it over from any existing hubs you desire.\nRestarting the Hub You will need to run the following from the hub\u0026rsquo;s root directory for these changes (as well as those made to userlist), to take effect:\ndocker-compose down docker-compose build docker-compose up -d  These commands stop and destroy the containers running the hub and database, remove the existing image, re-build the Hub\u0026rsquo;s image and re-launch it (with no data loss, since volumes and networks are preserved).\n Manual Install If you want to perform the steps carried out in setup.sh yourself, you can follow these directions (or simply run the commands in that file one-by-one). This script is designed to save you time and effort.\nConfigure Settings Now that we are in the deploy directory (or whatever you named your hub):\nmkdir secrets make secrets/postgres.env make secrets/oauth.env make userlist  If you copied an existing hub, be aware that your secret files will be shared between the two (same login credentials).\n At the very least, remove postgres.env and re-make the secret key there. If you want passwords to be shared for users across hubs, keep oauth.env the same among the hubs. Passwords are set during the build process.\n You will see some dialogue regarding how to add users to the hub initially. We only need to be concerned with adding one administrative user at this time since we can add users later through the Hub\u0026rsquo;s interface. Run the following command and edit the userlist according to the prompt you just saw, which will show you how to format this file.\nvim userlist  Now, run vim .env to change the name of your hub and the port to avoid conflicts with running Docker processes.\nThe noteboook image that gets built by default is quite sizable.\nYou may consider changing this in the .env file ($DOCKER_NOTEBOOK_IMAGE) if you\u0026rsquo;re just testing it out and want to make sure it works (perhaps to jupyter/minimal-notebook or another pre-built stack ). But, the one we ship is feature-rich. Consider it a \u0026ldquo;show off what this can do\u0026rdquo; example.\n The notebook image that gets build starts off with $DOCKER_NOTEBOOK_IMAGE and adds in some features. To change the image, edit singleuser/Dockerfile and delete as much as you would like.\nAt any point you can re-build this image with make notebook_image (which we will run in a moment in the next step).\nBuild We are now ready to build our hub! The last line also runs it as a background process.\nmake build make notebook_image docker-compose up -d  And that is all. Sit back and wait. It will take a while.\nYou might see a bunch of red messages like these fly by during the build process.\nchgrp: changing group of '/opt/conda/var/cache/fontconfig/0c78243b-3123-48a4-91b4-49cb45a27aaf-le64.cache-7': Operation not permitted chgrp: changing group of '/opt/conda/var/cache/fontconfig/2fd305a6-4303-4a09-8894-d1594f7ec636-le64.cache-7': Operation not permitted . . .  As far as I can tell, this is not a problem and occurs somewhere outside of the instructions I have added on top of the pre-built images supplied by Jupyter.\nIf successful, you should see green done output at the very end.\n When you want to destroy the images that get built (make build runs docker-compose build with some other options to configure it properly), and the associated containers created from them, you can run:\ndocker-compose down  to clean up. Since data is external to the containers that host our application, you won\u0026rsquo;t lose any configurations at all.\nDirect Traffic From the walkthrough, you should have already configured the nginx proxy. Now all that is left is to tell our proxy the \u0026ldquo;location\u0026rdquo; of our hub so that it can become publicly accessible.\nsudo vim /etc/nginx/sites-enabled/hub.conf  You need to look for the server object (if secured, on port 443) that matches the domain name that this hub will exist on, and add an entry inside of this server configuration. It will look like this (for the default hub that ships with the deployment, specified in .env):\nlocation /math { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; }  Now, when example.com/math is hit, our proxy (nginx) will direct traffic to localhost (127.0.0.1). Specifically, to port 8000. Make sure that the entry you add matches the .env file. Here, we show the default.\nAcquire Credentials To get your password, vim Makefile and uncomment the following line (around 5): include secrets/oauth.env (Alternatively, you can just run source secrets/oauth.env and skip editing the file).\nOnce that is done, you can run make login to find out the password for whatever user is specified in .env. Hopefully the user in .env matches at least one of the admin users you put in userlist, otherwise you would be querying passwords for non-existent users. If not, edit .env and userlist and run docker-compose down; docker-compose up -d to re-build the images.\n Test You can now log in by visiting example.com/math and ensure that everything is working as expected out of the box.\nYou can add users through Control Panel \u0026gt; Admin! ![snapshot-of-admin][hub-admin]\nNow that you have your password, log in to the hub. If you navigate to /hub/login_list you will see passwords for everyone in userlist. Since the connection is not yet secure, do not do this yet.\nYou can bring up or take down your hub with docker-compose up -d and docker-compose down, but must run these in the same directory as the docker-compose.yml file.\nManagement Adding Packages Assuming you are ssh\u0026rsquo;ed into the server and inside the directory that has your hub, simply edit singleuser/Dockerfile and use the Docker Reference on the next page to run commands.\nManaging Users/Work When logged in to the hub, you can access Control Panel \u0026gt; Admin to add users and access their servers.\nTo disable the feature of Admins being able to log in as the students, set c.JupyterHub.admin_access = False in jupyterhub_config.py.\nGroup Sharing Sharing files between groups is a feature I personally coded into the jupyterhub_config.py file. Towards the top of the script, the file userlist is opened (in the container\u0026hellip; so if you update it, you can run docker restart hub-name since this configuration file only runs at hub startup). Better yet, do this from the Admin menu directly.\nEach line in userlist has the name of the student, followed by their group names, separated by spaces.\n Instead of restarting through the docker command, you can \u0026ldquo;Power Down\u0026rdquo; the Hub from the Control Panel in JupyterHub. This has the effect of propagating changes to lists of users. Docker takes care of re-starting the hub automatically.\nIf userlist is given the right permissions with chown 777 userlist, and mounted inside the professor\u0026rsquo;s directory, it may be possible to manage group memberships without ever logging into the math-hub server (except to update the single-user notebook image), editing the userlist right from the web-interface, provided the group volume permissions have previously been set correctly.\n jupyterhub_config.py will check the user\u0026rsquo;s groups and mount all appropriate volumes. A volume is created for the group if one does not exist.\nBy default, when volumes are created, the permissions are not set in a way where users can write files.\n To fix this, you would find the shared group volume in question, use docker inspect to find out where it is, and change the permissions with chmod. Here is an example:\npilosovm@math-hub:~$ docker volume ls | grep \u0026quot;shared-*\u0026quot; | awk '{print $2}' ro_shared_volume rw_shared_volume shared-broncos shared-test-group pilosovm@math-hub:~$ pilosovm@math-hub:~$ docker volume inspect shared-broncos | grep \u0026quot;Mountpoint\u0026quot; \u0026quot;Mountpoint\u0026quot;: \u0026quot;/var/lib/docker/volumes/shared-broncos/_data\u0026quot;, pilosovm@math-hub:~$ chmod 777 /var/lib/docker/volumes/shared-broncos/_data  In this manner, you can create new volumes (e.g. docker volume create shared-groupname) and set the permissions as desired with chmod, modifying userlist to add groupname to the appropriate users.\nRinse, repeat.\nThe architecture implemented here mounts volumes based on the words that follow the username in userlist and prepends them with shared. If you want to avoid naming conflicts, use group names such as math8660-group1, math8660-group2, etc., or something unique chosen by the group members). It is up to you to avoid naming conflicts across multiple hubs when sharing group volumes.\n More customization is possible. For example, you can have multiple hubs set up on the same server (e.g. at example.com/hub1/ and example.com/hub2/, etc.) but create one shared-admin volume that every administrator has access to. To accomplish this, (or to add any new group volume):\ndocker volume create shared-admin docker volume inspect shared-admin | grep \u0026quot;Mountpoint\u0026quot;  then use chmod 777 PATH where PATH matches the path in the output of the last command. Now this volume is visible to anyone for whom it is mounted.\nHowever, in jupyterhub_config.py, we have this:\n33 for i in range(1,len(parts)): 34 group_id = parts.pop() 35 if group_id != 'admin': # no need for an admin group. 36 group_map[user_name].append(group_id)  We purposefully disable this from happening by default. To enable it, simply change the logic here. Erase line 35, un-indent line 36, and the shared-admin volume will be mounted for all admins of all hubs. Don\u0026rsquo;t forget to restart your hub for the changes to take effect.\n","date":1545462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545462000,"objectID":"0466bf99816ed349fa235696de37c572","permalink":"https://www.michaelpilosov.com/openscience/hubsetup/","publishdate":"2018-12-22T00:00:00-07:00","relpermalink":"/openscience/hubsetup/","section":"openscience","summary":"Adding a Hub Introduction This document assumes that you have followed the walkthrough at least once before. You already have:\n Logged in with an account that has sudo permissions Docker installed Proxy configured, secured with HTTPS  This walk-through will guide you through the bare-minimal steps to set up a new hub.\nPreliminaries First, we need to decide on the following:\n Name of new hub An available port for it to be hosted on  We will run the following commands to find out what is already in use:","tags":null,"title":"Deploying a New Hub","type":"docs"},{"authors":null,"categories":null,"content":" The End-User Experience You go to a web-address and are confronted with the Jupyterhub login portal. Once you log in, you see a drop-down menu that allows you to choose what \u0026ldquo;stack\u0026rdquo; of software dependencies you would like to use. After you make your selection, you are redirected to an in-browser interface where all your files are available. Jupyter Notebooks, R Scripts, anything you want, can now be run directly through this interface.\nThe work is performed \u0026ldquo;in the cloud.\u0026rdquo; What this means is that a connection is established between your browser window (the client) and a server (the host) elsewhere that is capable of performing the calculations you need.\nBenefits You do not have to worry about installing software dependencies, since the administrator has already loaded them into the environment that you selected. The machine you are working on will not be doing anything except communicating commands to the host and displaying the result, which means your computer will run on less energy.\nMoreover, it means that since all you need is a functional internet browser, you can do all of your scientific work with a budget laptop (or in a pinch, a phone/tablet). For academic institutions, this means that entire laboratories of computers can be re-purposed for better uses, since students will not be limited by the technology they can afford.\nIf you are an administrative user, you can add other users through a control-panel available through this interface, and even be able to see when these users are logged in. If configured by the system administrator who set all this up, you may also be able to start an environment as any of these users, and look around their folders/files, perhaps as a teacher helping de-bug some code during remotely-held office hours.\nSounds neat, doesn\u0026rsquo;t it? Especially for academics, this lowering of friction to programming is especially appealing. The hardest parts of setting up a class with the resources they need to start learning how to code have all been abstracted away for both the students and teachers who serve as administrators. All of this is already possible, and there are a lot of ways to set this exact thing up, which makes navigating how to do so a little difficult. We will discuss this later after building up the proper motivation.\nApplication Architecture This separation of where work is performed and where its results are shown is actually essential to understanding how most modern \u0026ldquo;applications\u0026rdquo; are built. By not burdening end-users with installation dependencies that vary with the technology they own, application developers avoid worrying about cross-platform support. Most applications are now simply re-skinned versions of a browser called Chromium1, which direct you to the company\u0026rsquo;s servers where all the work is performed.\nIn this spirit, the Jupyter Project has done the same thing for scientists and programmers. The open-source community that powers most of the work done by Jupyter is largely concentrated at UC Berkeley, and the adoption of their work is spreading fast across the world. However, due to the rapid pace of development, a cohesive narrative that explains how all the technologies developed by the Jupyter Project are connected is hard to come by. We attempt to address this problem with this document, stepping through the layers of abstraction and building up a story that helps develop an understanding of how these underlying technologies interface and the many ways in which they can be configured2.\nJupyterhub The management of users and the environments they are able to use is done through the interface known as Jupyterhub (the \u0026ldquo;Hub\u0026rdquo; for short). The Hub controls who is allowed to access a notebook server through an Authenticator. This can be a username/password combination, or a one-click login through a third-party such as Github, Gitlab, Google, Azure, or your university\u0026rsquo;s login page.\nJupyterhub passes information to an Authenticator, which returns a \u0026ldquo;successful\u0026rdquo; (or not) message to the Hub, which (if successful) then performs some action. In the example above, the action is performed was to first display a drop-down menu that allows a user to select an environment. (This can be bypassed if you want every end-user to have the same environment). Jupyterhub then \u0026ldquo;spawns\u0026rdquo; a \u0026ldquo;single-user (notebook) server\u0026rdquo; to establish the connection between computational resources and the user-facing interface through the browser3. The Hub launches such a server for each user that successfully logs in.\nJupyterhub is thus made up of:\n An Authenticator (that lets people in or keeps them out), a Spawner (which defines individual environments), a Database (to store users), and a Server (to connect all of it).  Notebook Server At the heart of the system is the Jupyter Notebook (server). The notebook server communicates with the relevant processes on the host machine to carry out the computations that you (the client) asked it to perform through your browser interface. The machine running the notebook server has all the necessary dependencies to perform the work, and your end-user experienced is managed through the Jupyter notebook server.\nThe native file-format for the interface the notebook server is the \u0026ldquo;interactive notebook\u0026rdquo; (which has the .ipynb ending). All that these notebooks are is a collection of key-value pairs (dictionaries, linked lists, many words exist), each of which corresponds to one \u0026ldquo;cell\u0026rdquo; in the notebook and carries input/output information along with formatting specifications. Since it is best to see by example, here is an abstraction of what this looks like:\n{\u0026quot;cell_1\u0026quot;: {info} } {\u0026quot;cell_2\u0026quot;: {info} } {\u0026quot;cell_3\u0026quot;: {info} }  where each one of these {info} elements corresponds to a dictionary itself, which might look something like:\n{\u0026quot;input\u0026quot;: \u0026lt;\u0026gt;, \u0026quot;output\u0026quot;: \u0026lt;\u0026gt;, \u0026quot;execution_count\u0026quot;: \u0026lt;\u0026gt;, \u0026quot;kernel\u0026quot;: \u0026lt;\u0026gt;, ... }  All of this information is used to format an HTML page that is then rendered by a browser of your choice, since at this point it is simply a web page.\nNow, it is important to note at this time that if you are using a computer that has all the required software dependencies you need, then you can become the \u0026ldquo;host\u0026rdquo; yourself, and run the notebook server locally. In the event that you do this, the website you visit to see the notebook server file-browsing interface and work with notebooks will look something like http://localhost:8000, where localhost is telling your browser that \u0026hellip; well, the host is local, not somewhere remote\u0026hellip;4. The :8000 (the number can differ) is the port that is being opened on the machine to allow for input/output connections, which is what is handled by the server. (Remember this\u0026hellip; it\u0026rsquo;ll come up later).\n An Analogy\nIf the website is considered the \u0026ldquo;address\u0026rdquo; of the machine(s), the port is the \u0026ldquo;apartment number.\u0026rdquo; Many \u0026ldquo;tenants\u0026rdquo; (applications) can \u0026ldquo;live\u0026rdquo; (be hosted) at the same \u0026ldquo;address\u0026rdquo; (website). Some buildings choose to have a doorman, so you can ask for directions to the right place by name instead of apartment number. So if you do not see a port number (:XXXX) anywhere in the browser address, you can assume that the configuration implemented has a \u0026ldquo;doorman\u0026rdquo; (proxy) directing traffic to the right places.\n This about sums up what you need to know about the notebook server. Somewhere there exists a machine that can do what you want. By running a notebook server on this machine, you can connect to it through a web-browser.\nSpawning There are many ways to start the (single-user) notebook servers. They can be started as individual processes on the same machine that the Hub is running on, which is the default behavior, and the implementation used in The Littlest Jupyterhub.\nHowever, in the spirit of modularity, they can also be started on machines elsewhere in the world. These machines may be set up with all the dependencies, waiting for authenticated connections from Hub, or they can be created (and destroyed) on-demand. They can be physical machines, or \u0026ldquo;virtual machines,\u0026rdquo; which (as the name suggests) are emulations of computers that run as processes on top of some existing architecture.\nContainerization Virtual machines allow for something called \u0026ldquo;containerization,\u0026rdquo; which isolated applications in (usually Linux) virtual environments that contain little else but the bare necessities for running a given application.\nTo scale to millions of users, services exist that allow for the creation and destruction of virtual machines on-demand, with low latency, and pricing computed by the second. Such services sit atop a software platform called Kubernetes, and require a decent amount of technical experience to configure. Jupyterhub can be configured to \u0026ldquo;spawn\u0026rdquo; single-user notebook servers through such a service using an extension called \u0026ldquo;Kubespawner.\u0026rdquo; However, we do not provide any more information about this herein because the wiki for Jupyterhub is written specifically for this implementation case.\n We focus on a solution that is capable of scaling to hundreds of users and can be built with existing server resources that a department may have that are underutilized, but is a little more technically challenging to implement than The Littlest Jupyterhub, which basically functions as a one-click install and provides its own set of configuration scripts.\n Docker A simpler solution though, can be achieved through Docker, which (although it has a learning curve), is capable of scaling to very large workloads and can be configured to connect multiple servers and balance traffic amongst them.\n We choose to run Jupyterhub within a Docker container, the Database in another container, and then spawn single-user notebook servers each in isolated containers as well. The latter containers are ephemeral, which means that once a user shuts down their single-user server (or it is shut down due to inactivity after a predetermined amount of time), the container is destroyed. Containerizing every aspect of this project allows for a really simple deployment scenario, which we walk through on the Deployment page.\n We can \u0026ldquo;containerize\u0026rdquo; the environments we want our users to experience in their single-user notebook servers through the use of a \u0026ldquo;Dockerfile,\u0026rdquo; which simply lists a set of instructions to configure a brand-new Ubuntu (Linux distribution) machine to handle all the dependencies.\nThe single-user notebook servers can be launched from a choice of \u0026ldquo;images,\u0026rdquo; which define the containers that are created/destroyed on-demand.\nIn this way, our applications are \u0026ldquo;stateless,\u0026rdquo; which means that all the information (files, data, configuration) is connected to the application virtually (Docker handles these connections through something called \u0026ldquo;volumes\u0026rdquo;. So, if properly configured, a student can launch any environment they want and still see the same set of files.\nDatabase A basic SQL database comes prepackaged and configured with Jupyterhub, but for \u0026ldquo;production\u0026rdquo; purposes, the developers in the Jupyter Project advise using a more resilient database such as PostgresDB, which is the solution we implemented, following the Deployment Example provided by Jupyter.\nThis postgres database will exist inside its own container and be connected to an external volume (managed by Docker), so that we can create/destroy the Hub and not lose any of our user data.\nAuthenticator The Wiki has a list of authenticator options. The default behavior is to check the system users on the machine running Jupyterhub. In the jupyterhub_config.py file which is used to set up the Hub, a whitelist and admins attribute are defined to control who is allowed in. If inside a container, this means that users must be created inside of there! The nice thing about using an authentication service such as OAuth is that it simplifies the login entirely, since accounts exist on other platforms to verify users. The downside is that you must own a \u0026ldquo;fully qualified domain name\u0026rdquo; in order to do this, which means you must own the website (can be as low as $2-10/year, available via Namecheap).\nProxy As mentioned in the analogy in earlier sections, the \u0026ldquo;doorman\u0026rdquo; that directs traffic around is the component of this application known as the \u0026ldquo;proxy\u0026rdquo;, and is the last part left in securing and configuring your server.\nVolumes Files and folders, as mentioned, persist on the machine that is hosting the application, and can even be configured (e.g., with symbolic links), to connect to file-servers elsewhere. My university, for example, has a system for all of us to be able to store files on a remote server, and we can \u0026ldquo;mount\u0026rdquo; these connections appropriately by configuring Jupyterhub so that students can more readily access such resources.\n which serves as the backbone for Google Chrome. ^ We note that the reason that there are so many possible ways to set up such a system is because the developers ensured the modularity of all the application components. There is a lot of segmentation that allows for this technology to scale to thousands of users. The lessons learned by mobile developers about scaling in popularity have all been implemented here. ^ The distinction is made for (single-user) servers since Jupyterhub itself is a server communicating with other applications/servers. ^ I suppose this would be a good time to mention that any website you visit is simply an alias for an \u0026ldquo;address\u0026rdquo; to some other computer elsewhere in the world. The address system used on the internet is known as the \u0026ldquo;IP address\u0026rdquo;, so the localhost is telling the browser to look for a connection on the same machine. ^   ","date":1545462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547054400,"objectID":"de16281685c8608512a32abebbee1099","permalink":"https://www.michaelpilosov.com/openscience/jupyterhub/","publishdate":"2018-12-22T00:00:00-07:00","relpermalink":"/openscience/jupyterhub/","section":"openscience","summary":"The End-User Experience You go to a web-address and are confronted with the Jupyterhub login portal. Once you log in, you see a drop-down menu that allows you to choose what \u0026ldquo;stack\u0026rdquo; of software dependencies you would like to use. After you make your selection, you are redirected to an in-browser interface where all your files are available. Jupyter Notebooks, R Scripts, anything you want, can now be run directly through this interface.","tags":null,"title":"JupyterHub","type":"docs"},{"authors":["Michael Pilosov"],"categories":["documentation"],"content":" Jan 1, 2019 Today I split up the documentation here on devlog and began the write-ups for Jupyterhub. I think I should segment out a section that goes through the entire process of deployment\nNotes: - bash script to give you nice shortcuts - SSH key generated ahead of time, ready to log in without passwords - get Traefik proxy working - It seems extremely straightforward except that I do not know how to direct traffic to a folder. I suppose that since it is running port-mappings, I could have an nginx server\nThis is helpful, but it\u0026rsquo;s not using docker. https://github.com/jupyterhub/the-littlest-jupyterhub/tree/master/tljh\nokay the only thing I\u0026rsquo;ve managed to set up correctly is nginx outside of docker. This is annoying.\n0 3 * * 1 certbot renew --pre-hook \u0026quot;service nginx stop\u0026quot; --post-hook \u0026quot;service nginx start\u0026quot;  crontab for renewals.\nhub.conf inside of /etc/nginx/sites-enabled (to be mounted)\n# top-level http config for websocket headers # If Upgrade is defined, Connection = upgrade # If Upgrade is empty, Connection = close map $http_upgrade $connection_upgrade { default upgrade; '' close; } # HTTP server to redirect all 80 traffic to SSL/HTTPS server { listen 80; server_name hub.consistentbayes.com; # Tell all requests to port 80 to be 302 redirected to HTTPS return 302 https://$host$request_uri; } # HTTPS server to handle JupyterHub server { listen 443; ssl on; server_name hub.consistentbayes.com; ssl_certificate /etc/letsencrypt/live/consistentbayes.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/consistentbayes.com/privkey.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_dhparam /etc/ssl/certs/dhparam.pem; ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA'; ssl_session_timeout 1d; ssl_session_cache shared:SSL:50m; ssl_stapling on; ssl_stapling_verify on; add_header Strict-Transport-Security max-age=15768000; # Managing literal requests to the JupyterHub front end location / { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } # Managing requests to verify letsencrypt host location ~ /.well-known { allow all; } }  whereas the website looks like /etc/nginxsites-enabled/consistentbayes.conf\nserver { listen 80; server_name consistentbayes.com; # Tell all requests to port 80 to be 302 redirected to HTTPS return 302 https://$host$request_uri; } server { listen 443; ssl on; # INSERT OTHER SSL PARAMETERS HERE AS ABOVE ssl_certificate /etc/letsencrypt/live/consistentbayes.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/consistentbayes.com/privkey.pem; # Set the appropriate root directory root /var/www/consistentbayes.com/public_html; # Set URI handling location / { try_files $uri $uri/ =404; } # Managing requests to verify letsencrypt host location ~ /.well-known { allow all; } }  Traefik Problems netstat -ltnp | grep -w ':80'\nSo far this is the easiest I\u0026rsquo;ve had it configuring the set-up.. with nginx. Talk to Joe about getting Traefik to work. https://blog.raveland.org/post/traefik_le/ maybe that will help??? https://www.bennadel.com/blog/3420-obtaining-a-wildcard-ssl-certificate-from-letsencrypt-using-the-dns-challenge.htm or that\u0026hellip; (it is important to note that you need to follow instructions after feb 18 due to letsencrypt changing something major).\nJan 2, 2019 Spent day configuring proxy. Succeeded eventually on my domain, but not with a reverse proxy. Just a regular one\u0026hellip;\nMessed around with apache but couldn\u0026rsquo;t make it work. Documentation on Jupyter\u0026rsquo;s website is god awful, there are numerous omissions and a typo in the configuration files (including ones from the repository I got my original setup files), no context or setup instructions. They just assume you know what you are doing.\nI do not, though. So let\u0026rsquo;s go through it.\nHere\u0026rsquo;s what a reverse-proxy is doing. It catches requests to a website and handles them.\nWhen you hit consistentbayes.com, it directs you to a folder with a bunch of files that make up my website. I had that working with Apache but couldn\u0026rsquo;t do what I wanted, which was direct traffic to \u0026ldquo;hub.consistentbayes.com\u0026rdquo; instead.\nI got the \u0026ldquo;hub\u0026rdquo; part set up by adding an A Record into my DNS Control Panel (talk more about that later). Now the proxy just had to be able to handle and direct that request to a jupyterhub instance running in a docker container that was exposing itself on http://127.0.0.1:8000 (I\u0026rsquo;ll show config files later). The hub would be in a container that is just a Linux machine, with users and everything in there.\nIf the container name is \u0026ldquo;nostalgic_colden\u0026rdquo; (TO DO: figure out how to name these)\u0026hellip; then running\ndocker exec -ti -u mpilosov nostalgic_colden /bin/bash  will log you into the linux computer as \u0026ldquo;mpilosov\u0026rdquo;\nThe user accounts are handled at build-time, but can be managed within the computer just as you would on a linux machine (to log in as root, omit -u mpilosov above).\nSo\u0026hellip; those accounts. Github usernames. By far the best authentication method I found, but it relied on having a Fully Qualified Domain Name (website name, which the server at school would not let me do). But\u0026hellip; we can loop the authenticator to whatever we want.\nHere are my nginx configurations:\nIn /etc/nginx/sites-enabled/consistentbayes.conf\nserver { listen 80; server_name NO_HUB.DOMAIN.TLD; # Tell all requests to port 80 to be 302 redirected to HTTPS return 302 https://$host$request_uri; } server { listen 443; ssl on; # INSERT OTHER SSL PARAMETERS HERE AS ABOVE # SSL cert may differ # Set the appropriate root directory root /var/www/html; # Set URI handling location / { try_files $uri $uri/ =404; } # Managing requests to verify letsencrypt host location ~ /.well-known { allow all; } }  In /etc/nginx/sites-enabled/jupyterhub.conf (titles dont matter it seems).\n# top-level http config for websocket headers # If Upgrade is defined, Connection = upgrade # If Upgrade is empty, Connection = close map $http_upgrade $connection_upgrade { default upgrade; '' close; } # HTTP server to redirect all 80 traffic to SSL/HTTPS server { listen 80; server_name HUB.DOMAIN.TLD; # Tell all requests to port 80 to be 302 redirected to HTTPS return 302 https://$host$request_uri; } # HTTPS server to handle JupyterHub server { listen 443; ssl on; server_name HUB.DOMAIN.TLD; ssl_certificate /etc/letsencrypt/live/HUB.DOMAIN.TLD/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/HUB.DOMAIN.TLD/privkey.pem; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_dhparam /etc/ssl/certs/dhparam.pem; ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA'; ssl_session_timeout 1d; ssl_session_cache shared:SSL:50m; ssl_stapling on; ssl_stapling_verify on; add_header Strict-Transport-Security max-age=15768000; # Managing literal requests to the JupyterHub front end location / { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } # Managing requests to verify letsencrypt host location ~ /.well-known { allow all; } }  Talk about how you had to generate letsencrypt scripts for nginx with certbot.\nsudo apt install python-certbot sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install python-certbot-nginx sudo certbot certonly  I actually ran into trouble being unable to start nginx with the sites enabled. Kind of a catch-22. Removed letsencrypt files\n$ sudo certbot certonly --webroot -w /var/www/example -d example.com -d www.example.com -w /var/www/thing -d thing.is -d m.thing.is  This command will obtain a single cert for example.com, www.example.com, thing.is, and m.thing.is; it will place files below /var/www/example to prove control of the first two domains, and under /var/www/thing for the second pair.\nI dont quite want that. Let\u0026rsquo;s try the interactive.; sudo certbot certonly Hit 1, enter website names (consistentbayes.com, www.consistentbayes.com),\nThen do the same again for the hub.consistentbayes.com\nAnd we\u0026rsquo;re golden. The files have been placed where nginx is expecting them to be. (e.g. /etc/letsencrypt/live/hub.consistentbayes.com/fullchain.pem)\nAnd also you will need to issue the following command once.\nopenssl dhparam -out /etc/ssl/certs/dhparam.pem 4096  And then go ahead and make the Dockerfile:\n# Designed to be run as # # docker run -it -p 8000:8000 jupyterhub/oauthenticator FROM jupyterhub/jupyterhub MAINTAINER Project Jupyter \u0026lt;ipython-dev@scipy.org\u0026gt; # Install oauthenticator from git RUN python3 -m pip install oauthenticator RUN python3 -m pip install notebook\u0026gt;=4.0 # Create oauthenticator directory and put necessary files in it RUN mkdir /srv/oauthenticator WORKDIR /srv/oauthenticator ENV OAUTHENTICATOR_DIR /srv/oauthenticator ADD jupyterhub_config.py jupyterhub_config.py ADD addusers.sh /srv/oauthenticator/addusers.sh ADD userlist /srv/oauthenticator/userlist ADD ssl /srv/oauthenticator/ssl RUN chmod 700 /srv/oauthenticator RUN [\u0026quot;sh\u0026quot;, \u0026quot;/srv/oauthenticator/addusers.sh\u0026quot;]  Then this bash script addusers.sh:\n#!/bin/sh IFS=\u0026quot; \u0026quot; for line in `cat userlist`; do test -z \u0026quot;$line\u0026quot; \u0026amp;\u0026amp; continue user=`echo $line | cut -f 1 -d' '` echo \u0026quot;adding user $user\u0026quot; useradd -m -s /bin/bash $user # cp -r /srv/ipython/examples /home/$user/examples mkdir /home/$user/examples # may only be necessary since we are copying files from root above. chown -R $user /home/$user/examples done  And, for now, the simplest version of our jupyterhub_config.py:\n# Configuration file for Jupyter Hub c = get_config() c.JupyterHub.log_level = 10 from oauthenticator.github import LocalGitHubOAuthenticator c.JupyterHub.authenticator_class = LocalGitHubOAuthenticator c.GenericOAuthenticator.login_service = 'my service' c.LocalGitHubOAuthenticator.create_system_users = True c.Authenticator.whitelist = whitelist = set() c.JupyterHub.admin_users = admin = set() import os import sys join = os.path.join here = os.path.dirname(__file__) root = os.environ.get('OAUTHENTICATOR_DIR', here) sys.path.insert(0, root) with open(join(root, 'userlist')) as f: for line in f: if not line: continue parts = line.split() name = parts[0] whitelist.add(name) if len(parts) \u0026gt; 1 and parts[1] == 'admin': admin.add(name) c.GitHubOAuthenticator.oauth_callback_url = os.environ['OAUTH_CALLBACK_URL']  A userlist file with github usernames1\nmpilosov admin mathematicalmichael eescu  A ssl folder with encryption keys in them (which we won\u0026rsquo;t use!) (We don\u0026rsquo;t use them because we expose Jupyterhub in http, and use the reverse-proxy to handle the security).\nAnd an env file\n# add your github oauth config to this file, # and run the container with `docker run -it -p 9000:8000 --env-file=env jupyterhub-oauth` OAUTH_CLIENT_ID= OAUTH_CLIENT_SECRET= OAUTH_CALLBACK_URL=https://hub.consistentbayes.com/hub/oauth_callback  Which you get when you register the application on Github. (I filled in the third to show you, the first two are secrets!)\nSo from here on, we can start tweaking the spawner, volume persistence, do checks on memory limits, etc. Write up detailed instructions. Jupyterhub from docker like this is nice.\nI think we can test directory-mounting right to the linux machine to be honest.\nOMG I RAN THIS AND IT WORKED. I mounted volumes to folders on a per-user basis (since users live inside the container that jupyterhub is in). No write permissions, but they can see files just fine. Cant duplicate.\ndocker run -it -p 8000:8000 -v /home/michael/repos/:/home/mathematicalmichael/examples -v /home/michael/repos:/home/mpilosov/examples --env-file=env --name hubtest jupyterhub-oauth  Problem with relying on user data in the hub is potential updates could cause loss of files. mounting volumes would fix this for sure. But they can\u0026rsquo;t write files in this new directory, so\u0026hellip;\nI think the best storage solution is each student has their own container. hubname-studentname that a teacher can get into. For workshops, the solution presented as-is works great.\n Make sure to upgrade pip, no one seems to. volume permissions are probably things joe knows about.  VOLUMES WORKING! --mount creates directories if they dont exist, whereas --volume does not. Good to create up at root one /shared directory. Additionally we can link them to volumes to host their data instead, managed via docker. Ideally we use a mix of the two. Adding a volume (even if it doesn\u0026rsquo;t exist) to be managed by docker would be accomplished with -v volume-name:/directory/to/mount\nSo we should think about what in our docker-compose we can automatically mount. Additionally (or in replacement), volumes can be handled via the spawner.\nIn either set up, we can shut down and restart the container all we want and data persists. We can even re-build the image. Volumes seem to just be symbolic links to directories on your unix machine, anyway, visible via docker inspect volume-name.\ndocker run -d -it -p 8000:8000 --mount source=/home/michael/repos/packages/lyricalart,type=bind,target=/home/mpilosov/examples/shared-folder-mp/,bind-propagation=rshared --mount source=/home/michael/repos/packages/lyricalart,type=bind,target=/home/mathematicalmichael/shared-folder-mm/,bind-propagation=rshared --env-file=env --name hub jupyterhub-oauth  Now that we\u0026rsquo;ve got this figured out\u0026hellip; It would be nice to figure out how to get that docker-compose up and running, including with the nginx server, all based on a configuration file that takes in the website name, ip of host, etc. so you can get in, git clone, run bash script, edit the ip, and close out knowing it will work.\nTo build everything, we did docker build -t jupyterhub-oauth . in the directory we set up.\nDefinitely want to containerize each student. Currently it\u0026rsquo;s possible to get into the directories mounted above as shared by going through Terminal. Can move files in/out\u0026hellip; not good.\nTo destroy containers after they close down, simply add .remove = True to the c.DockerSpawner attributes.\n[8:59 PM] Pilosov, Michael oficially did it on hub.consistentbayes.com! holy shit that took forever but I linked up the pieces. Now it spawns up containers per user, with BOTH volumes that persist even as containers change and shared directories accessible by all users, automatically mounted to every user. One environment file controls everything. the version of jupyterhub, the docker image you want to spawn, etc. ​ [9:04 PM] Pilosov, Michael that is our ideal set up.. the shared folders can be determined by rules such as group membership (students within class or even, and the environment to spawn can be similarly chosen. This means we can use one hub for every student across every class with total ease. No containers stick around. At all. They just get appropriately mounted to volumes when they are created. the decision to make one hub per class is purely an aesthetic one. the hub can spin up dozens of different configurations depending on what the user needs. I\u0026rsquo;m going to package this all nice so that I can deploy to a powerful server that I can rent for like \u0026hellip;. a couple hours for testing. Throw a whole bunch of simultaneous use-cases at it. ​ [9:08 PM] Pilosov, Michael we can even use the admin panel to start/stop servers instead of logging in simultaneously. if each single-user server has a heavy python script to run on startup, we can simulate heavy loads. I played around a little already and saw the containers being created (not restarted, created!) when I \u0026ldquo;started\u0026rdquo; the single-user server and then thrown away when I clicked \u0026ldquo;stop\u0026rdquo; which keeps our physical memory literally AT THE LOWEST possible limit at any given time since stopped containers dont have to sit around.\nSo, how?\nI cloned the jupyterhub-deploy-docker repo. (again), and made my fixes\u0026hellip;\nI don\u0026rsquo;t know why the Makefile doesn\u0026rsquo;t handle this, but echo \u0026quot;POSTGRES_PASSWORD=$( openssl rand -hex 32)\u0026quot; \u0026gt;\u0026gt; secrets/postgres.env will create a necessary file for make build to work. And after that works, run make notebook_image to create the necessary image to be spawned based on the .env file in the root directory.\nNote from 1/8/2019: Just run make secrets/postgres.env or whatever other file it needs, and it will create and set permissions. If it doesn\u0026rsquo;t create the file, then simply add it with touch.\nI commented out lines 40-49 in the Makefile since I want to handle certification on my own through the reverse proxy.\nWould be nice to get this working with nginx as well. But all that stuff can be handled from the bash script.\nIn the jupyterhub_config.py file, I removed references to SSL and set up shared volumes. It would be great to learn how to sub-class the spawner now and create rules for mounting volumes. A simple restart lets students gain new files.\nI also removed SSL references from Dockerfile.jupyerhub\nDocker-compose guidelines\nJust tested the containerized solution and love it (even though i built small containers). I can log in, add nbextensions as root, and the changes are reflected without touching docker. No restarting required.\nCan add user accounts as admin through the cPanel. If they don\u0026rsquo;t exist, a home directory or volume is created on their behalf. I wouldn\u0026rsquo;t suggest sharing directories automatically for the containerized solutions. That part should be part of a custom spawner (and require hub restart, which can also be done through cpanel since it runs on another port!).\nAdmin is powerful! You can start/stop servers at your whim and launch them to poke around on your own. Very cool.\nFor a smaller class, auto-mounting some shared directory is a great idea.\nJan 5, 2019 To get a bash script for the set-up, you need to package the reverse-proxy with docker-compose. Let\u0026rsquo;s figure out how to use Traefik. Between these two sources, you should be able to figure it out: https://github.com/defeo/jupyterhub-docker/blob/master/docker-compose.yml https://github.com/containous/traefik/blob/master/examples/quickstart/docker-compose.yml\n Fun note, it may be possible to accidentally revoke admin privileges from everyone. Test this. But editing the config file in the container as root should be able to make it work again. ^   ","date":1545462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546378800,"objectID":"5b81a7014dc52e0628e6d1f26d8b09ad","permalink":"https://www.michaelpilosov.com/devlog/openscience/","publishdate":"2018-12-22T00:00:00-07:00","relpermalink":"/devlog/openscience/","section":"devlog","summary":"Filling in the Docs","tags":["dev","docker","openscience"],"title":"Open Science: Working towards reproducibility","type":"docs"},{"authors":null,"categories":null,"content":" Inital Setup I presume the use of vim in a few of these commands. If you are more comfortable using nano for editing text, please be sure to make the appropriate substitutions.\n On a brand new server\u0026hellip; (Change the top line to be your name)\nexport USER_NAME=mathematicalmichael sudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install vim htop make -y useradd $USER_NAME -m -s /bin/bash passwd $USER_NAME usermod -aG sudo $USER_NAME su - $USER_NAME  Now set your password. We then set privileges and switch to this new user (and new shell, which should appear in color!).\nConfigurations Here are some aliases I like to have in my ~/.bash_aliases file:\nvim ~/.bash_aliases\n# show users logged into the hub who have open sessions. alias dpsu='docker ps --format \u0026quot;table {{.ID}}\\t{{.Names}}\\t{{.Status}}\\t{{.Ports}}\u0026quot; -f \u0026quot;name=r*user*\u0026quot;' # both of these show/filter docker processes with prettier formatting. alias dps='docker ps --format \u0026quot;table {{.ID}}\\t{{.Names}}\\t{{.Status}}\\t{{.Ports}}\u0026quot;' alias ..='cd ../' # Move up 1 directory alias ...='cd ../../' # Move up 2 directories alias ....='cd ../../../' # Move up 3 directories alias clc='clear' alias ll='ls -ltr' ## see list of files in reverse chronological order. alias la='ls -A' ## show hidden files (can add a/A to the above too... preference) # completion from history. VERY USEFUL alias cic='set completion-ignore-case On' alias cico='set completion-ignore-case Off' alias rr='source ~/.bashrc' # refresh environment alias erc='vim ~/.bash_aliases' # shortcut to edit my shortcuts. # don't put duplicate lines or lines starting with space in the history. # See bash(1) for more options HISTCONTROL=ignoreboth # append to the history file, don't overwrite it shopt -s histappend HISTSIZE=10000 HISTFILESIZE=5000 # the following bind up/down to history search bind '\u0026quot;\\e[A\u0026quot;: history-search-backward' bind '\u0026quot;\\e[B\u0026quot;: history-search-forward' if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi  Then run source ~/.bashrc and you\u0026rsquo;ll have a nice environment.\nDocker Install If you already had an account with sudo privileges, then you can start from here assuming you are logged in as such. The code that follows installs Docker, and runs a little test by checking the version to make sure everything worked.\nexport DOCKER_COMPOSE_VERSION=1.23.2 sudo apt install apt-transport-https ca-certificates curl software-properties-common  (You will be prompted for your password)\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u0026quot; sudo apt update apt-cache policy docker-ce sudo apt install docker-ce -y docker --version  Success?\nsudo usermod -aG docker root sudo usermod -aG docker ${USER} sudo curl -L https://github.com/docker/compose/releases/download/$DOCKER_COMPOSE_VERSION/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version  Success?\nSSH Keys Now is a good time to reboot the machine since we just updated and installed a bunch of things. Before we do that, let us make sure we can ssh in with either of the usernames now. Hit enter at the prompt. YOUR COMPUTER WILL REBOOT AFTER\nssh-keygen sudo cat /root/.ssh/authorized_keys \u0026gt; ~/.ssh/authorized_keys reboot  Your computer/server might take a minute or two to reboot.\n After restarting ssh in as your-name@your-website.com. We will take care of the proxy and security set up first (in order to complete the instructions that only need to be performed initially).\nSecurity Here is how you do it without a container (ideally we could containerize this whole aspect as well into our docker-compose.yml file, but that is not yet the case). Run all this first.\nEncryption sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update sudo apt-get install certbot sudo certbot certonly  Follow prompts, enter info for your site. This creates a public/private key pair with a certificate authority so that browsers can trust the connection is valid.\nIf you are behind a private network (.pvt or any other non-public domain), then you must acquire this certificate and key from another (presumably the school\u0026rsquo;s) certificate authority. Make sure the paths are set properly in the configuration to reflect where you placed these keys. Furthermore, you may have to comment out the ssl-ciphers line (we had to at CU Denver to make it work, but this isn\u0026rsquo;t required on public domains).\n The keys generated will by default be in /etc/ssl/live/example.com/, which you will see reflected in the example configuration below (set for example.com).\nThen run the following,\nsudo openssl dhparam -out /etc/ssl/certs/dhparam.pem 4096  This creates a long encryption key that we will pass to nginx (our proxy) in the next step for additional security.\nProxy sudo apt-get update sudo apt install nginx  Nginx Config To secure our connection, we will create the following file in /etc/nginx/sites-enabled/:\nYou may need to remove default.conf, or edit it directly to match the following instead.\nIf you choose to edit default.conf, make sure to add the following line to override the default nginx settings, which will prevent you from saving notebooks greater than 1MB in size: client_max_body_size 20m; (see example below).\n Run sudo vim /etc/nginx/sites-enabled/hub.conf and edit the file:\n# top-level http config for websocket headers # If Upgrade is defined, Connection = upgrade # If Upgrade is empty, Connection = close map $http_upgrade $connection_upgrade { default upgrade; '' close; } # HTTP server to redirect all 80 traffic to SSL/HTTPS server { listen 80; server_name example.com; # Tell all requests to port 80 to be 302 redirected to HTTPS return 302 https://$host$request_uri; } # HTTPS server to handle JupyterHub server { listen 443; ssl on; server_name example.com; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; # memory limits need to be upped for notebooks. default of 1mb is insufficient. client_max_body_size 20m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_dhparam /etc/ssl/certs/dhparam.pem; ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:E$ ssl_session_timeout 1d; ssl_session_cache shared:SSL:50m; ssl_stapling on; ssl_stapling_verify on; add_header Strict-Transport-Security max-age=15768000; # Managing literal requests to the JupyterHub front end location /math { proxy_pass http://127.0.0.1:8000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } location /math8660 { proxy_pass http://127.0.0.1:8002; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } # Managing requests to verify letsencrypt host location ~ /.well-known { allow all; }  That last section starting with location is what we\u0026rsquo;ll be tweaking if we want to host this at hub.mathfight.club or mathfight.club/hub/ or whatever we want. (Although in the case of a directory /suffix/ format, we will have to make sure the line from the jupyterhub_config.py that we commented out above matches this configuration file.\nThe assumption (by default) is that the $HUB_NAME is used as the suffix. example.com/math/ is the default location we will have to set. The idea here is that we can set up multiple hubs simply by changing the .env file to choose a new name and port number.\n As long as we point to the correct port, nginx (our proxy) will handle the rest. The / refers to the fact that we want the hub to be hosted at mathfight.club (with nothing appended/prepended).\nIf you want some kind of homepage instead of a Hub login, you can configure that to be the case with the following:\nlocation / { root /var/www/public_html; . . . }  Restart nginx for changes to take effect:\nservice nginx restart  You can now proceed to adding a hub in the Hub Setup documentation.\n","date":1545462000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545462000,"objectID":"83ad746a98b814f389465b4feba1116a","permalink":"https://www.michaelpilosov.com/openscience/walkthrough/","publishdate":"2018-12-22T00:00:00-07:00","relpermalink":"/openscience/walkthrough/","section":"openscience","summary":"Inital Setup I presume the use of vim in a few of these commands. If you are more comfortable using nano for editing text, please be sure to make the appropriate substitutions.\n On a brand new server\u0026hellip; (Change the top line to be your name)\nexport USER_NAME=mathematicalmichael sudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install vim htop make -y useradd $USER_NAME -m -s /bin/bash passwd $USER_NAME usermod -aG sudo $USER_NAME su - $USER_NAME  Now set your password.","tags":null,"title":"Deploying","type":"docs"},{"authors":null,"categories":null,"content":" As previously mentioned, the Jupyter Notebook runs in the client (browser) and connects to a server (either running locally or remotely) to perform necessary calculations in a given language.\nThe kernels provide the means for establishing this communication, and are effectively what jupyter (the program) is set up to manage.\nHere, we demonstrate why this approach is so valuable.\nMotivation Assuming you followed the instructions in Installing Anaconda on your local machine, you\u0026rsquo;ll notice that you establish a connection to the server (running in your Terminal session, likely outputting information with green-highlighted time-stamps) through the \u0026ldquo;address\u0026rdquo; localhost, which references the fact that you are running this server locally.\nHowever, this means that you can follow those instructions on any server, and with some additional commands (which we will review here), have the ability to connect to your Jupyter session from anywhere!1\nThere are a few situations you might find yourself in, and we attempt to address several common ones here. If there is a scenario that is not covered here that you would like us to write about, please contact us.\nServer on Campus Rented Server  Provided the machine running Jupyter is one that is publicly accessible. More on that later. ^   ","date":1545375600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"05b08af229c5670d028b0e1c12e911f0","permalink":"https://www.michaelpilosov.com/openscience/remote/","publishdate":"2018-12-21T00:00:00-07:00","relpermalink":"/openscience/remote/","section":"openscience","summary":"As previously mentioned, the Jupyter Notebook runs in the client (browser) and connects to a server (either running locally or remotely) to perform necessary calculations in a given language.\nThe kernels provide the means for establishing this communication, and are effectively what jupyter (the program) is set up to manage.\nHere, we demonstrate why this approach is so valuable.\nMotivation Assuming you followed the instructions in Installing Anaconda on your local machine, you\u0026rsquo;ll notice that you establish a connection to the server (running in your Terminal session, likely outputting information with green-highlighted time-stamps) through the \u0026ldquo;address\u0026rdquo; localhost, which references the fact that you are running this server locally.","tags":null,"title":"Remote Connection","type":"docs"},{"authors":null,"categories":null,"content":"Let\u0026rsquo;s first get a feel for the possibilities, and see what is possible by checking out some existing websites, organizations, and repositories. The idea of removing installation complexities, platform dependencies, etc. from your end-users is beautifully demonstrated by some of the recent advancements made over at Project Jupyter, all visible on their Github repository\nMy favorite example that demonstrates how slick this new approach to setting up programming environmnents is the repository demonstration with JupyterLab + Binder from the binder-examples repository collection.\nIt even has an interactive map!!!\nHead on over there, click on the little badge that reads \u0026ldquo;launch | binder,\u0026rdquo; and it will automatically open up a notebook for you.1 Have a look around, run code, do whatever you like.\nTo then see how easy adding functionality is, run the following in a new code-cell in the notebook that opens:\n!jupyter labextension install @jupyter-widgets/jupyterlab-manager  The exclamation point tells the notebook to run the command using bash.2. Since widgets are already pre-packaged, all we have to do to enable them is to tell JupyterLab to get the necessary Javascript resources to show them to you in your client (web-browser). A bunch of dialogue should show up, and then once it finishes, replace the code you just ran with\nimport ipywidgets as wd wd.FloatSlider()  and you should see a slider pop up.\nTo avoid this step and ensure that the widget functionality is ready-to-go, one can fork the repository, and edit the file binder/postBuild, which runs after the initial repository is built (using repo2docker on the backend) but before the client displays the interface.\n Be warned: After a few minutes of inactivity, the mybinder.org service that is hosting the docker container which serves this web application will time-out. This means that it will assume you are done with it and clean up the resources it created for you. Say goodbye to whatever you were working on. ^ Alternatively, you could open a new Terminal instance in JupyterLab and run the same code (without the exclamation mark). ^   ","date":1545375600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"dc0cb6064db7e8c0bcc08651db08ba91","permalink":"https://www.michaelpilosov.com/openscience/demo/","publishdate":"2018-12-21T00:00:00-07:00","relpermalink":"/openscience/demo/","section":"openscience","summary":"Let\u0026rsquo;s first get a feel for the possibilities, and see what is possible by checking out some existing websites, organizations, and repositories. The idea of removing installation complexities, platform dependencies, etc. from your end-users is beautifully demonstrated by some of the recent advancements made over at Project Jupyter, all visible on their Github repository\nMy favorite example that demonstrates how slick this new approach to setting up programming environmnents is the repository demonstration with JupyterLab + Binder from the binder-examples repository collection.","tags":null,"title":"Demonstration with Jupyter Lab","type":"docs"},{"authors":null,"categories":null,"content":"There are a number of organizations with deployments you can experiment with.\nWe list some of them here, along with useful links for more information (some of these may be turned into future pages).\n","date":1545375600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"d0497fa5c18df63de00a54c9ff762f8b","permalink":"https://www.michaelpilosov.com/openscience/example/","publishdate":"2018-12-21T00:00:00-07:00","relpermalink":"/openscience/example/","section":"openscience","summary":"There are a number of organizations with deployments you can experiment with.\nWe list some of them here, along with useful links for more information (some of these may be turned into future pages).","tags":null,"title":"Examples of Existing Projects","type":"docs"},{"authors":null,"categories":null,"content":" These instructions originally appeared on the \u0026ldquo;Resources\u0026rdquo; page of my website, where I link to my Github Gist. As of Dec 21, 2018, I am abandoning the aforementioned gist and will keep this webpage updated as my primary source of information for installing Anaconda1.\nAnaconda Basics Introduction It is a package manager.\nWRITE MORE ABOUT WHAT A PACKAGE MANAGER IS/DOES AND WHY IT SHOULD BE USED\nDownload Go download it from https://www.anaconda.com/download/ for your Linux/Windows/OSX\nI like to use\nwget http://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh\nor if I want to keep things lightweight (may lead to more manual installations later on):\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nInstall It will likely be saved in the Downloads folder, so you will install it with something like\nbash ~/Downloads/Anaconda3-5.0.1-Linux-x86_64.sh\nUse all the defaults, agree to everything, and at the end, hit ENTER to prepend it to your PATH\nEnvironments Now let\u0026rsquo;s create an environment for python3:\nconda create -n py3 python=3.6 where py3 is just my personal preferred shorthand for python 3. Feel free to use your own.\nAnd one for python2:\nconda create -n py2 python=2.7\nTo remove:\nconda remove --name py2 --all\nWe can now switch between versions of python with commands like\nsource activate py3 and source activate py2 and then source deactivate when you\u0026rsquo;re ready to be done.\nDiving Deeper Widgets Overview:\npip install ipywidgets\njupyter nbextension install --py --user widgetsnbextension\njupyter nbextension enable --py widgetsnbextension\nMore information about widgets can be found here\nAlternatively, in each of these environments, run\nconda install -c conda-forge ipywidgets\nwhich will install all sorts of dependencies, including Jupyter notebooks2. It supposedly enables the widgets too but you can run\njupyter nbextension enable --py widgetsnbextension to make sure.\npython -m ipykernel install --name py3 --user will then link the kernel to jupyter.\nThen --name py3 flag is optional and just assigns a name of your choosing to that kernel. The version of python used for the kernel is whatever is returned when you ask bash which python (so if you used source activate earlier, it will be the version associated with the kernel whose name is in parentheses in your Terminal session). The --user flag is necessary.\niPyParallel https://github.com/ipython/ipyparallel\npip install ipyparallel to install.\nipcluster nbextension enable (add the --user flag as well if you encounter an error). This one also seems to be necessary to get the clusters to show up:\njupyter serverextension enable --sys-prefix --py ipyparallel\nTo install for all users (as root):\njupyter nbextension install --sys-prefix --py ipyparallel jupyter nbextension enable --sys-prefix --py ipyparallel  Kernels Introduction TODO: WRITE MORE ABOUT MULTI-KERNEL SUPPORT, provide an example.\nOverview To see all the kernels jupyter is aware of: jupyter kernelspec list\nManagement To remove kernels, use jupyter kernelspec remove\nJupyterHub Introduction More information here about installation would be helpful, but we\u0026rsquo;ll be going through this in more detail at another time since JupyterHub will form the basis for the solutions we will use for our classroom environments.\nInstallation Configuration (for personal use)\nGetting Started\nRunning a notebook server\nJupyterLab Introduction I love this environment but it is a little tricky to set up properly and get everything working as expected. Though once you do, I doubt you will want to use anything else.\nInstall Once Jupyter Lab is installed, go ahead and run it the same way you would a notebook:\njupyter lab (--no-browser)\nWidget Extensions By default (as of Dec 2018), it will be accessible through localhost:8888\nGo ahead and launch an instance of Python and see if widgets work.\nimport ipywidgets as wd wd.FloatSlider()  If the output of the following is text instead of a widget, try running the following in the command-line (shutdown the server with Ctrl-C twice, then\njupyter labextension install @jupyter-widgets/jupyterlab-manager\nAnd relaunch JupyterLab (if you didn\u0026rsquo;t close your browswer window, a simple refresh will work to re-establish the connection and pick up where you left off), and try the example above again.\nSomething that came up once\nI saw my server output spitting out TypeError: __init__() got an unexpected keyword argument 'io_loop' over and over again when any kernel was started. Something was going on with the WebSocket I/O communication (whatever that means), but the following downgrade of the Python Web Framework worked to fix it:\npip install tornado==4.5.3\nSharing Motivation TODO write about why this is useful\nExporting conda env export \u0026gt; environment.yml\nWhich you then load up with conda env create -f environment.yml\nCloning You can make an exact copy of an environment by creating a clone of it:\nconda create --name myclone --clone myenv\nTo verify that the copy was made:\nconda info --envs\n Github Gists are actually surprisingly difficult to sort through, and navigating to this webpage will be much easier, anyway. ^ It does appear though that notebooks is pre-packaged now with Anaconda, so it will likely already be installed. ^   ","date":1545375600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"e2d6d9a479df4a6e68efa7ed94b7874b","permalink":"https://www.michaelpilosov.com/openscience/anaconda/","publishdate":"2018-12-21T00:00:00-07:00","relpermalink":"/openscience/anaconda/","section":"openscience","summary":"These instructions originally appeared on the \u0026ldquo;Resources\u0026rdquo; page of my website, where I link to my Github Gist. As of Dec 21, 2018, I am abandoning the aforementioned gist and will keep this webpage updated as my primary source of information for installing Anaconda1.\nAnaconda Basics Introduction It is a package manager.\nWRITE MORE ABOUT WHAT A PACKAGE MANAGER IS/DOES AND WHY IT SHOULD BE USED\nDownload Go download it from https://www.","tags":null,"title":"Installing Anaconda","type":"docs"},{"authors":null,"categories":null,"content":" Basics In this documentation, we will research and likely leverage (most if not all of) the following open-source projects. We will eventually address each in depth within this documentation and will update this section with relevant links as necessary. For now, we provide a brief overview and summary of what we are looking into.\nWidgets If you have ever tried developing a graphical user interface (GUI) for your code, you\u0026rsquo;ll know how cumbersome it can be. Thankfully, the Widgets package from Project Jupyter greatly simplifies linking your code to a large library of existing \u0026ldquo;widgets,\u0026rdquo; which do everything from giving you sliders to fully-interactive maps.\nI highly encourage you to see what is possible to embed in your notebooks (and webpages) by scrolling through their examples.\nJupyter nbviewer Paste a link to a notebook into this form and this website will render the notebook for your reading pleasure.\nDocker Github Anaconda BinderHub BinderHub\ntaken from the repository description:\n BinderHub allows you to BUILD and REGISTER a Docker image using a GitHub repository, then CONNECT with JupyterHub, allowing you to create a public IP address that allows users to interact with the code and environment within a live JupyterHub instance. You can select a specific branch name, commit, or tag to serve.\n Repo2Docker repo2docker\nAs the title suggest, this package turns a github repository into a docker image that can be built.\ntaken from the repository description:\n repo2docker fetches a git repository and builds a container image based on the configuration files found in the repository.\n Binder leverages this to then serve the contents of the image in conjuction with JupyterHub.\nKubernetes Google is a provider. Links/info should go here to a couple other platforms, some basic information.\nJupyterHub JupyterHub\ntaken from the repository description:\n With JupyterHub you can create a multi-user Hub which spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server.\nProject Jupyter created JupyterHub to support many users. The Hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high performance computing group.\nTechnical overview Three main actors make up JupyterHub:\n multi-user Hub (tornado process) configurable http proxy (node-http-proxy) multiple single-user Jupyter notebook servers (Python/Jupyter/tornado)  Basic principles for operation are:\n Hub launches a proxy. Proxy forwards all requests to Hub by default. Hub handles login, and spawns single-user servers on demand. Hub configures proxy to forward url prefixes to the single-user notebook servers.  JupyterHub also provides a REST API for administration of the Hub and its users.\n I personally run Jupyterhub on a remote server so that I always have access to a secure and familiar environment if I ever need to test out some code and find myself without my laptop. By default, it accepts the login information of just the user who is running it, but we will see later how it can be configured for its intended use as a multi-user authentication hub.\n","date":1545375600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"7a60d3f11ac50872f40feeb46328b4ef","permalink":"https://www.michaelpilosov.com/openscience/getting-started/","publishdate":"2018-12-21T00:00:00-07:00","relpermalink":"/openscience/getting-started/","section":"openscience","summary":"Basics In this documentation, we will research and likely leverage (most if not all of) the following open-source projects. We will eventually address each in depth within this documentation and will update this section with relevant links as necessary. For now, we provide a brief overview and summary of what we are looking into.\nWidgets If you have ever tried developing a graphical user interface (GUI) for your code, you\u0026rsquo;ll know how cumbersome it can be.","tags":null,"title":"Relevant Projects","type":"docs"},{"authors":["Michael Pilosov"],"categories":["documentation"],"content":" Summary I spent the day working with the widget library and understanding how interpolating polynomials are handled by scipy. Most of today\u0026rsquo;s developments were concerned with my random art and animation libraries, and I completed some of the most important components of the interface: setting time-dependent functions (independent of frames), and creating a widget that properly reads/writes data to a dictionary.\nI still need to link the interfaces for the existing GUIs that write the properties.\n Allow removal feature? scrape the widgets and write to dictionary. New GUI for stacking effects, connecting each effect to the needed time-series based on available entries in aforementioned dictionary. All effects get applied with their time series, passed to animation function to be written to file. Maybe use the Play button to preview? (Have a GUI focused on the num_frames perspective). It is linked to the index.  Update: As of 01/01/19, dictionary writing is working. Preview button is segmented out. Play button implemented. No removal, bare-bones working time-series manipulation. Need to add in hyperbolic tangent functions (should also write blog about these).\nNotes  Still having trouble with the mybinder.org connections with my giftplanning library that tests out my deployment solutions for interactive blogs. Update: As of 01/01/19, devs and I have been in touch on gitter. Sam pushed a fix to unpkg.com and my site started to work again. TO DO: Go make a publicly accessible redundant copy of JS script, query it. Ask Elliott how to only trigger it if the first one fails.\n After conversing with Elliott last night (12/19/19), I think that writing a simple WebSocket atop Flask with his help may be a good solution. A JSON file gets passed back and forth, manipulations happen on a server. Still would prefer to just learn how to make the widgets do this part. Update: As of 01/01/19, I have found numerous other solutions and there is a feature request submitted for the nbinteract library that would enable me to run the nbserver that mybinder.org is providing on a machine that I own instead, which would greatly reduce latency. Would docker be able to automatically re-start it? Load balance? Etc. (probably, but that\u0026rsquo;s for later. For now, mybinder.org is fine, but steps should be taken to ensure the visualizations work in posterity). Should also attempt to get their plotting to work again (maybe one of their updates have fixed this?)\n  ","date":1545289200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546377780,"objectID":"d2f43559621e1cc5597e34795817651e","permalink":"https://www.michaelpilosov.com/devlog/widgets/","publishdate":"2018-12-20T00:00:00-07:00","relpermalink":"/devlog/widgets/","section":"devlog","summary":"An artistic day","tags":["dev","webdev"],"title":"Day 2","type":"docs"},{"authors":["Michael Pilosov"],"categories":["documentation"],"content":" Introduction This documents the changes made to the academic theme to make this website work when upgrading from Hugo ver=0.47 and Academic Theme ver=2.4.\nThe upgrade for this website was performed on 12-19-2018.\nAll in all, it took the better part of an entire day to get familiar with all the nooks and crannies of the new updates and plan out the migration of the more complicated website\u0026hellip; Mathematical Michael.\nChanges Made to Theme  Removed Weibo sharing icon in /layouts/partials/share in the theme directory. Changed footer in layouts/partials/footer_section.html Re-did config.toml line-by-line, disabled night/day mode. Created slides and docs based on the exampleSite folder, including creating the directory /assets/css/reveal_custom.css for the Reveal.js slideshows.\n added boards.jpg to /static/img/ to test headers and embeds in the example slideshow. created slides.md in /content/ to link to /content/slides/ folder since using an index file does not work to list the available slideshows in that folder.  created /wid/ and /dockerhub/ to experiment with new docs template.\n copied over dummy project files, still need to migrate own projects to new formatting, remove dummy projects.\n Changed work hyperlink and resume to reflect new migration to the docs template.\n resume widget changed to hyperlink to resume doc.\n  To-Do  header/feature images for post, check how fb/twitter scrapes them.  optional feature image without becoming header will be useful for posts that are galleries.  migrate any posts/projects to new formatting. think about how to migrate MathematicalMichael.\n  Next Steps Things I\u0026rsquo;m Considering\n No need to re-make publication type.  Since posts can be linked to projects, and filters can be applied within widgets, we should have all the flexibility we need. Taken with the new folder structure of pages, the overall directory structure remains quite clean.  Use widgets pagetype to construct/mimick the /artwork page.\n Have carousels and project widgets linking to galleries and slideshows.  Can additionally link out to a docs page that describes the art in more depth, also linked to from the projects page of Probably Art to tell the \u0026ldquo;story\u0026rdquo; of this project over time and in its phases.\n Phase 1: Directed Study Phase 2: Pickup with Pocket CHIP, restart coding in Python Phase 3: After some time, picked it up again and added a whole bunch of functionality, attempted to get random art automated in Squarespace Phase 4: Pick up again when I get into GCode. Adds functionality. Phase 5: Lapse again. Then finally tackle creating a framework that will work long-term. Talk about requirements. How is has to be in Python, based on stable packages, recreatable and deterministic. GCode scalable. Talk about some to-do\u0026rsquo;s (bumpers, bash scripts that wrap around to position plotter better). Show images of getting plotter back up again.\n Phase 6: Merge with Animation code/projects. Talk about goals, current progress. Random animations on non-random images\n Have another to tell the story of Moving Pictures   Upgrading to Hugo v0.52 Date: 12/21/18\nToday I began to structure and fill in the basic information for the Open Science Documentation. It involved some research and collection of resources from disparated places and things I\u0026rsquo;ve done before.\nI also sat down and in a couple minutes managed to figure out the \u0026ldquo;breaking changes\u0026rdquo; to my MathematicalMichael website under Hugo v0.52. The only file that needed to be changed (before updating the theme, that is) was the $i, reference in a file I made myself:\nvim themes/academic/layouts/partials/artwork_links.html  Next step was to copy the theme from michaelpilosov/ (which pulled from the github repo only a few days prior, and to which some minor changes were made) into mathematicalmichael/themes/ along with a copy of the config file.\nMotivation Once the reference to the new theme is made, I expect a number of things to break.\n Here I will attempt to document one-by-one the changes I had to make to port my theme over, which I will attempt to do by leveraging the \u0026ldquo;lookup structure\u0026rdquo; of Hugo (something I read online).\n The idea is that rather than make changes to a theme that result in deviations that make merging new changes difficult, one makes changes by copying files into directories that are checked first, before themes/ is searched for layouts or partials files. My hope is that this will aid in future upgrades, since less differences will exist between my version of the theme and the master branch. A really useful command (for comparing files), that I used to see what needed to change in the master configuration file for my theme (compared to the updated one from this website) was:\ndiff -y file1.md file2.md  which will print out two files side-by-side directly into the UNIX Terminal. Excellent.\nAcademic Theme v0.3.32 After making the appropriate changes to the config.toml file (which were all straightforward), I ensured that the site built with the new Hugo and the new configuration file but without the new theme. It did. All was well.\nThen I changed the theme to point to the updated one:\ntheme = \u0026quot;hugo-academic\u0026quot;  which yielded the following error (kind of expected to be honest):\nError: Error building site: failed to render pages: render of \u0026quot;home\u0026quot; failed: execute of template failed: template: index.html:4:3: executing \u0026quot;index.html\u0026quot; at \u0026lt;partial widget_page...\u0026gt;: error calling partial: \u0026quot;/media/mathematicalmichael/HANK/repos/mathematicalmichael/themes/hugo-academic/layouts/partials/widget_page.html:23:9\u0026quot;: execute of template failed: template: partials/widget_page.html:23:9: executing \u0026quot;partials/widget_page.html\u0026quot; at \u0026lt;partial $widget $par...\u0026gt;: error calling partial: Partial \u0026quot;widgets/artworks.html\u0026quot; not found  So why did this happen?\nWell, I had defined my own template page called artworks that was modeled on the publications style of page. I used this to publish a webpage for each piece of art, complete with a rich tagging-system automated through Python. I made random art and had written code that created pages for that art to live on. Any changes made to the front-matter could be easily applied later.\nI believe that simply moving some files to directories that exist above themes will resolve my problems.\nImage Galleries However, I know there will be an opportunity to switch to page-bundles and leverage the image-gallery feature in v3.0 of Hugo-Academic. My thinking is as follows:\n Instead of daily pages, make weekly ones as galleries. Alternatively, keep daily postings as-is, and add galleries additionally. See if you can link images to the main folder, so all art can continue to live in one folder without duplicates. You can make galleries using page-bundles as a separate way to interact with the art. Definitely need to integrate a widget page that hosts all the art. I don\u0026rsquo;t want to clutter the home page with everything.\n I think that clicking from the main menu to art/ should suffice. Curate the experience, minimize contact with /artwork/ page (i.e. make slideshows link to other content rather than the search they perform now, which can be a bit slow.  Since slideshows are just a markdown file, we should have no trouble linking to images in another folder!\n will take some playing around to figure out exactly the right directory structure.   ** Main Goal ** for the time being is to make sure the site builds with the new theme. We can add features and functionality later on.\nToday\u0026rsquo;s agenda is simply to allow migration to Hugo v0.52 on my Macbook by ensuring the site builds with the new version. Having the new theme work would be an added bonus.\n I suspect all that needs to happen is a little migration of partials and shortcodes.\n cp themes/academic/layouts/partials/artwork_* layouts/partials/  Did not change the error.\nmkdir layouts/partials/widgets/ cp themes/academic/layouts/partials/widgets/artworks* layouts/partials/widgets/  Resolved that but sprang a new error:\nError: Error building site: failed to render pages: render of \u0026quot;home\u0026quot; failed: execute of template failed: template: index.html:4:3: executing \u0026quot;index.html\u0026quot; at \u0026lt;partial widget_page...\u0026gt;: error calling partial: \u0026quot;/media/mathematicalmichael/HANK/repos/mathematicalmichael/themes/hugo-academic/layouts/partials/widget_page.html:23:9\u0026quot;: execute of template failed: template: partials/widget_page.html:23:9: executing \u0026quot;partials/widget_page.html\u0026quot; at \u0026lt;partial $widget $par...\u0026gt;: error calling partial: Partial \u0026quot;widgets/search.html\u0026quot; not found  Okay, so let\u0026rsquo;s move widgets/search.html over?\ncp themes/academic/layouts/partials/widgets/search.html layouts/partials/widgets/  hugo server --disableFastRender  And that did it! Wow, relatively painless. Now let\u0026rsquo;s see how it looks.\nThe layout is dark for some reason\u0026hellip; Enabling light/dark mode shows me the white I want to see but with inverted header. :mailto icon is broken. Change to \u0026ldquo;fas\u0026rdquo; instead of \u0026ldquo;fab\u0026rdquo; in config.toml, line 293.\nBack to the theme.. let\u0026rsquo;s grab a file and move it to the root directory and figure out where to put it.\ncp themes/hugo-academic/data/themes/dark.toml . mkdir data mkdir data/themes mv dark.toml data/themes/  I then proceeded to change some things to get the right colors. Still not sure if this is correct. I also changed the theme to self.toml and referenced that in the config.toml file.\nDark Mode Next test: Day/Night mode (re-enable, see what happens).\nDay Mode is fine. Night mode makes my logos look weird (though I do like the look overall). Top bar is also white instead of black for some reason.\nEditing data/themes/self.toml to rename the theme (mistake to not have done that before), and toggle light = true, since I did make it into a light theme.\n# Theme metadata name = \u0026quot;Self\u0026quot; # Is theme light or dark? light = true # Primary primary = \u0026quot;hsl(339, 90%, 68%)\u0026quot; primary_light = \u0026quot;hsl(339, 90%, 78%)\u0026quot; primary_dark = \u0026quot;hsl(339, 90%, 58%)\u0026quot; # Menu menu_primary = \u0026quot;#fff\u0026quot; menu_text = \u0026quot;rgba(0,0,0,0.6)\u0026quot; menu_text_active = \u0026quot;hsl(339, 90%, 68%)\u0026quot; menu_title = \u0026quot;#2b2b2b\u0026quot; # Home sections home_section_odd = \u0026quot;rgb(255, 255, 255)\u0026quot; home_section_even = \u0026quot;rgb(247, 247, 247)\u0026quot;  Note: Also broken is the search widget. Sad.\nDisable search widget\u0026hellip; Should not have moved it in the first place! There is a new search feature that renders this one useless. I will now remove the search.html widget (and search.md from content/home/\nContact form\u0026hellip; Had to paste in the new format. I was using a custom partial code (note to self\u0026hellip; delete that) to make a formspree link. Now I don\u0026rsquo;t have to!\nAlso, remove Search from menu in config.toml. After deleting layouts/shortcodes/contactme.html (my custom formspree link), in favor of using the one the upgraded theme provided, it appears that my website is now settled into the new theme. I did have to remove all references to this deleted shortcode, of course.\nDay/night mode works correctly. (would like to make my logo transparent though).\nPage Bundles Okay, well I knew it was too good to be true that things just worked. While pages loaded correctly, before building the final site, I made sure to investigate the directories a bit more and found that a bunch of files prepended with ._XXX.md had populated my website directories. This must be how they\u0026rsquo;re ensuring backward-compatibility.\nWell, I refactored the pages into page bundles with this shell script:\n#!/bin/sh # Helps migrate from v2.4.0 to v3.0.0 # # Refactor a page named `X.md` to `content/\u0026lt;section\u0026gt;/X/index.md` to use the # new page bundles and featured image system # # - E.g. a post `content/post/X.md` is converted to `content/post/X/index.md` refactor_pages_to_page_bundles() { if [ ! -d ./content/ ]; then echo \u0026quot;Please run script from root of hugo site\u0026quot; fi local files=\u0026quot;$(find ./content/ -iname '*.md' -not -iname '*index.md' -not -ipath './content/home/*')\u0026quot; for file in ${files}; do local pagedir=\u0026quot;${file%.md}\u0026quot; echo \u0026quot;${file} -\u0026gt; ${pagedir}/index.md\u0026quot; if [ ! -d \u0026quot;${pagedir}\u0026quot; ]; then mkdir \u0026quot;${pagedir}\u0026quot; fi mv \u0026quot;${file}\u0026quot; \u0026quot;${pagedir}/index.md\u0026quot; done } # Bash Strict Mode set -eu # set -x refactor_pages_to_page_bundles \u0026quot;$@\u0026quot;  Source\nAfter poking around, it looks like the pages obey the proper structure now (for the most part)\u0026hellip; There are a few stray .md files around.\nI will push the changes live but later on go through both websites to ensure that bundles are everywhere and no stray pages are left.\nSearch/Filter Notably, I will need to\n update my python script that writes project pages and art pages. ensure featured images (optional) are properly handled find out how galleries can work with the artwork/images directory, which so far doesn\u0026rsquo;t seem to be conflicting with the page-bundle format. Make sure the artwork search works. Seems to be broken, though publications search still works.  Oh, I think all that needs to happen is to move /themes/academic/layouts/artwork into the layouts folder I have in my root directory.\nmv themes/academic/layouts/artwork/ layouts/  But that led to an error regarding partial header_image.html. Thus, it seems like I will need to copy the new publication template over and edit it again. At least this time I will document it.\ncp themes/hugo-academic/layouts/publication/single.html layouts/artwork/  And then I replaced all instances of publication with artwork\nBut that still didn\u0026rsquo;t lead to the filter-ability that I wanted.\nThe default behavior was to paginate and list all of the art, which doesn\u0026rsquo;t have image previews but honestly is just fine, especially since it loads quickly.\nThe widget on the homepage that links to the search of artwork though is now broken as a consequence. I can fall back on searching by tags, or disable the widget temporarily until I set up galleries for the artworks.\n If I write a python script that scrapes the artwork/images folder and creates galleries linked to that, well that would be just perfect. This widget can be linked to those galleries, or slideshows. Disable it for now.  Okay, later at night I had to come across this as well, but I wanted the listing of artworks in their projects, so I copied the new layouts/partials/project/single.html into the layouts/partials/projects/ directory I created and pasted the following relevant chunk of code:\n\u0026lt;br\u0026gt; \u0026lt;h4\u0026gt; Archive \u0026lt;/h4\u0026gt; {{ $items := where (where .Site.RegularPages \u0026quot;Type\u0026quot; \u0026quot;artwork\u0026quot;) \u0026quot;.Params.projects\u0026quot; \u0026quot;intersect\u0026quot; (slice $project) }} {{ $items := $items | union (where (where .Site.RegularPages \u0026quot;Type\u0026quot; \u0026quot;artwork\u0026quot;) \u0026quot;.Params.url_project\u0026quot; $project_path) }} {{ $arts_len := len $items }} {{ if ge $arts_len 1 }} \u0026lt;h4\u0026gt;{{ (i18n \u0026quot;artworks\u0026quot;) }}\u0026lt;/h4\u0026gt; {{ range $items }} {{ if eq $page.Site.Params.projects.artwork_format 1 }} {{ partial \u0026quot;artwork_li_detailed\u0026quot; . }} {{ else }} {{ partial \u0026quot;artwork_li_simple\u0026quot; . }} {{ end }} {{ end }} {{ end }}  And I got my desired lists!\nSee here for example.\nTable of Contents I also realized I needed to edit the table of contents with some custom CSS.\nCreate a custom file in /static/css and link it to line custom_css = [] in config.toml:\n#TableOfContents{ padding-left: 10px }  ","date":1545268567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546377900,"objectID":"1b68f384c44b269ec61adbeb03896477","permalink":"https://www.michaelpilosov.com/devlog/academicupgrades/","publishdate":"2018-12-19T18:16:07-07:00","relpermalink":"/devlog/academicupgrades/","section":"devlog","summary":"Cataloging changes made to the Academic theme.","tags":["dev","webdev"],"title":"Website Upgrading Logs","type":"docs"},{"authors":null,"categories":null,"content":" Here we describe the necessary steps for deployment.\nPlease pardon the state of this document as it is a living and breathing set of instructions, and this part of the project has yet to be addressed.\nKubernetes What is it? Why use it? Possibilities \u0026ldquo;Bare Metal\u0026rdquo; What is it? Why use it? Perhaps you have old computers lying around (if at a university, these may be more than enough for a considerable server) and are looking to repurpose them. Alternatively, your university or company may already have available server resources that you can access for such purposes as classrooms or workshops.\nBinder What is it? Setting up Kubernetes for Binder Traefik is really cool and powerful.\nGetting Docker, Docker-Compose, configuring basics of Nginx]nginx-install etc.\nWhat this does is add repositories to apt-get that instruct Ubuntu on where to get the latest Docker-CE versions, then installs it (rather than from the default apt-get repositories). It then adds the current user (non-root admin).\nNote: This first creates an admin user with sudo privileges, prompts you for the password (and will again a couple of times)\nAs root: (TODO: grab bash scripts for set up to make environment more pleasant.\nGrab stuff from https://github.com/mathematicalmichael/jupyterhub-deploy-docker.git\nsudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y apt install vim htop -y useradd mathematicalmichael -m -s /bin/bash passwd mathematicalmichael  Make sure to change the line above to be a different password. We are assuming that you will be using this account as user mathematicalmichael.\nusermod -aG sudo mathematicalmichael  This adds privileges we will need. Now we switch users.\nsu - mathematicalmichael export DOCKER_COMPOSE_VERSION=1.23.2 sudo apt update sudo apt install apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\u0026quot; sudo apt update apt-cache policy docker-ce sudo apt install docker-ce docker --version sudo usermod -aG docker root sudo usermod -aG docker ${USER} su - ${USER} sudo curl -L https://github.com/docker/compose/releases/download/$DOCKER_COMPOSE_VERSION/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker-compose --version  Have your public_html out in this top directory. We\u0026rsquo;ll be looking for it to serve the baseline-version of your website.\nWe need some environment variables based on what we\u0026rsquo;ve done above..\n.env\nDOMAIN_NAME=consistentbayes.com EMAIL=consistentbayes@gmail.com COMPOSE_PROJECT_NAME=masterhub JUPYTER_HUB_IMAGE_NAME=jupyterhub JUPYTER_SINGLE_NAME=jupyter-singleuser_img  vi docker-compose.yml (Note: may remove --logLevel=DEBUG later?)\nversion: '3' services: reverse-proxy: image: traefik:latest command: --docker --docker.domain=${DOMAIN_NAME}.local --logLevel=DEBUG networks: - traefik-network ports: - 80:80 - 443:443 - 8080:8080 volumes: - /var/run/docker.sock:/var/run/docker.sock - ./traefik.toml:/traefik.toml - ./secrets/acme.json:/acme.json nginx: image: nginx:latest labels: - \u0026quot;traefik.frontend.rule=Host:${DOMAIN_NAME},www.${DOMAIN_NAME}\u0026quot; networks: - traefik-network volumes: - public_html:/usr/share/nginx/html networks: traefik-network: external: true  vi traefik.toml\nlogLevel = \u0026quot;DEBUG\u0026quot; defaultEntryPoints = [\u0026quot;http\u0026quot;, \u0026quot;https\u0026quot;] [web] address = \u0026quot;:8080\u0026quot; [docker] domain = \u0026quot;${DOMAIN_NAME}.local\u0026quot; watch = true [entryPoints] [entryPoints.http] address = \u0026quot;:80\u0026quot; [entryPoints.http.redirect] entryPoint = \u0026quot;https\u0026quot; [entryPoints.https] address = \u0026quot;:443\u0026quot; [entryPoints.https.tls] [acme] email=\u0026quot;${EMAIL}\u0026quot; storage=\u0026quot;acme.json\u0026quot; entryPoint=\u0026quot;https\u0026quot; acmeLogging=true onDemand=false OnHostRule=true [acme.httpChallenge] entryPoint = \u0026quot;http\u0026quot;   The above Traefik configuration file sets the log level to debug and allows both HTTP and HTTPS requests to the frontend. We then force HTTP (80) traffic to redirect to HTTPS (443) in entrypoints section. The Traefik web interface is configured on port 8080, and the Docker section instructs Traefik to use Docker as a configuration source.\nThe acme section is used by Traefik to fetch a Let’s Encrypt certificate for the domain that appears in the docker-compose.yml. The great thing about Traefik is that these certificates are dynamic, meaning that if you add a new domain or subdomain to docker-compose.yml, Traefik will automatically fetch the key/certificate and store them in acme.json.\nThe onDemandoption in acme section will let Traefik request certificates whenever a web request is received for a domain or subdomain which does not already have a certificate. The onHostRule only requests new certificates for domain names that are listed in the docker-compose.yml file.\nCreate an empty JSON file to hold Let’s Encrypt data that and make this file readable/writable to only the present user.\n Should just need to make my version of the jupyterhub-deploy-docker repository include labels for the jupyterhub so that traefik can forward them.\nlabels: # Traefik configuration. - \u0026quot;traefik.enable=true\u0026quot; - \u0026quot;traefik.frontend.rule=Host:hub.consistentbayes.com\u0026quot;  or \u0026quot;traefik.frontend.rule=Host:hub.consistentbayes.com\u0026quot; Furthermore, this will let me create hubs at any website.com/hubname, etc. Traefik will handle a lot of things for me.\nsu admin touch secrets/acme.json touch secrets/postgres.env make secrets/postgres.env make secrets/acme.json make build docker-compose up -d  ","date":1545116400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"3195a0014cd6352d3f47cebdd1287334","permalink":"https://www.michaelpilosov.com/openscience/deploy/","publishdate":"2018-12-18T00:00:00-07:00","relpermalink":"/openscience/deploy/","section":"openscience","summary":"Here we describe the necessary steps for deployment.\nPlease pardon the state of this document as it is a living and breathing set of instructions, and this part of the project has yet to be addressed.\nKubernetes What is it? Why use it? Possibilities \u0026ldquo;Bare Metal\u0026rdquo; What is it? Why use it? Perhaps you have old computers lying around (if at a university, these may be more than enough for a considerable server) and are looking to repurpose them.","tags":null,"title":"Deployment","type":"docs"},{"authors":null,"categories":null,"content":" Here we describe the several use-cases we hope to address within this documentation.\nLocal Laboratory Here is how one sets up Jupyter on one\u0026rsquo;s own machine (\u0026ldquo;local installation\u0026rdquo;). Since Project Jupyter relies on web frameworks and operates through your browser, the instructions will also apply for installing on a remote server.\nThe instructions for doing so can be found on the Install Anaconda page.\nClassroom Resources You are a teacher with a small-to-medium sized classroom and want to set up a centralized server for your students to have access to notebooks you wrote. You want:\n An account for each student that they can use to log in from any (modern) web-browser. The ability to add/remove/edit files in each student\u0026rsquo;s file-system, perhaps for  updates to a classroom github/gitlab/bitbucket repository (without teaching the students git) troubleshooting for projects/homework  Isolated environments so that students cannot interfere witth eachothers files  Functionality for groups of students to be able to access shared files, collaboration (without git)  The ability to control the appearance of the web-browser  (This should all be possible but is low priority) Add university branding Restrict features (such as Terminal)  Handles simultaneous users (up to the whole class at one time) Be able to control how much resources each student has (CPU, memory, etc)  Should be controllable if using Docker (?)   Workshops You want to run a short workshop (perhaps at a conference, or on a weekend at your university), and want similar functionality to the classroom environment, but you care less about the permanence of files. Once the workshop is over, you plan to delete everything.\nWe will cover how this is done. In theory (as of this writing), the same Docker image can be used with slight modifications, then deleted whenever one chooses to do so.\nInteractive Publishing You wrote a scientific paper, developed most of the code in Python (perhaps using Jupyter Notebooks/Lab), and want to write interactive visualizations for your webpage or some online publication.1\n We hope to address doing this with R or Julia (or any other kernel), but since there are language-specific tools (such as Shiny for R, Dash for Python), we focus on the most directly-applicable use-case for the author, which is to convert ipywidgets embedded in a notebook into embeddable HTML pages. ^   ","date":1545030000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"6e4f38fa8704c205354d8d24d834efa0","permalink":"https://www.michaelpilosov.com/openscience/usage/","publishdate":"2018-12-17T00:00:00-07:00","relpermalink":"/openscience/usage/","section":"openscience","summary":"Here we describe the several use-cases we hope to address within this documentation.\nLocal Laboratory Here is how one sets up Jupyter on one\u0026rsquo;s own machine (\u0026ldquo;local installation\u0026rdquo;). Since Project Jupyter relies on web frameworks and operates through your browser, the instructions will also apply for installing on a remote server.\nThe instructions for doing so can be found on the Install Anaconda page.\nClassroom Resources You are a teacher with a small-to-medium sized classroom and want to set up a centralized server for your students to have access to notebooks you wrote.","tags":null,"title":"Use-Cases","type":"docs"},{"authors":null,"categories":null,"content":" Here we briefly describe some of the available resources and possible directions this documentation might take.\nMost notably, there is Binder, which allows you to recreate an entire environment as-needed in an isolated instance that is destroyed once it is no longer needed. The ideal scenario is to have Binder installed on a server, and we discuss the process of setting up such a server in this documentation, but for now we address setting up an environment on your own computer.\nWe focus here on Python, and rely on Anaconda deployed on a Linux distribution. Here, we use Ubuntu 18.04, but the instructions should be comparable on previous versions as well as on Mac and Windows, though some of the interactions in the command-line will be different for Windows users.\nHistory (This brief history represents the view of the author of this webpage and may not be completely accurate).\nProject Jupyter is the maturation of the iPython project, a command-line program that allowed for an interactive Python shell, and what I relied on in my early days of learning Python. Their flagship development/breakthrough was the Notebook, which forms the foundation for all the other products.\nThe notebook is an HTML page that is served for you by a jupyter notebook server. It looks and feels like a desktop application. It was undoubtedly inspired by Mathematica\u0026rsquo;s \u0026ldquo;Notebooks,\u0026rdquo; but released as an open-source project, leading to its rapidly-expanding adoption in the scientific community.\nThe key to understanding why the notebook is so special is that it allows the web-browser to interact with code running on any computer (be it your own or a remote server). By divorcing the computation from the rendering, the developers allow for a unified framework so that scientists can focus on writing code and not on keeping up with dev-ops architecture.\nThe idea is simply that once one adapts to the Jupyter ecosystem, they are freed from platform considerations. Code is shareable across Windows/Mac/Linux, and viewable in the same environment that it was developed in by using a web-browser. It seeks to completely replace IDEs (Integrated Development Environments), which were the best solutions for setting up someone who was new to programming.\nBasically, there was a serious desire to make getting up-and-running with software development easier. Now, thanks to Jupyter, a teacher can set up isolated computing environments for every student and be sure that their experiences are uniform. This is an especially huge time-saver for workshops.\n","date":1544943600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545375600,"objectID":"77aa7862f8cdd88cc524f9d66997cca2","permalink":"https://www.michaelpilosov.com/openscience/intro/","publishdate":"2018-12-16T00:00:00-07:00","relpermalink":"/openscience/intro/","section":"openscience","summary":"Here we briefly describe some of the available resources and possible directions this documentation might take.\nMost notably, there is Binder, which allows you to recreate an entire environment as-needed in an isolated instance that is destroyed once it is no longer needed. The ideal scenario is to have Binder installed on a server, and we discuss the process of setting up such a server in this documentation, but for now we address setting up an environment on your own computer.","tags":null,"title":"Introduction","type":"docs"},{"authors":null,"categories":null,"content":"Presentations can be sorted/viewed on the Presentations Page.\n","date":1536472800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545202800,"objectID":"ad80cb50dcdc397c16af6e948d214ee9","permalink":"https://www.michaelpilosov.com/resume/presentations/","publishdate":"2018-09-09T00:00:00-06:00","relpermalink":"/resume/presentations/","section":"resume","summary":"Presentations can be sorted/viewed on the Presentations Page.","tags":null,"title":"Presentations","type":"docs"},{"authors":null,"categories":null,"content":"Publications can be sorted/viewed on the Publications Page.\nManuscripts in review are not shown.\n","date":1536472800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545202800,"objectID":"8a658af2557f9001345502e3b55a39ea","permalink":"https://www.michaelpilosov.com/resume/publications/","publishdate":"2018-09-09T00:00:00-06:00","relpermalink":"/resume/publications/","section":"resume","summary":"Publications can be sorted/viewed on the Publications Page.\nManuscripts in review are not shown.","tags":null,"title":"Publications","type":"docs"},{"authors":[],"categories":[],"content":"This was a really fun project I undertook for a Math Club game night in 2016 with my friend Elliott Regan.\n current  min  max   Start New Game   var canvas = document.getElementById('plotCanvas'); var ctx = canvas.getContext('2d'); var num, maxnum, minnum; var interval = 5; var B = [0] var brakes = 200; var ht = canvas.height; var n = canvas.width; var alpha = 5; initPlot(); drawWalk(); function reset() { B = [0]; var id = window.setTimeout(function() {}, 0); while (id--) { window.clearTimeout(id); } document.getElementById(\"mytext2\").value = 0; document.getElementById(\"mytext3\").value = 0; ctx.clearRect(0, 0, canvas.width, canvas.height); initPlot() drawWalk(); } function initPlot() { ctx.font = \"16px sans-serif\"; ctx.filStyle = \"light\"; ctx.font = \"36px serif\"; ctx.beginPath(); ctx.moveTo(0, canvas.height / 2) ctx.lineTo(canvas.width, canvas.height / 2) ctx.stroke(); var tick_length = 7; var tick_its = 8; for (var i = 1; i Flip a coin, heads you earn a point, tails you lose one. Now repeat this rapidly–no, in fact, let the computer do it–and keep your running score hidden to yourself. This webpage is your hand. Your opponents each have one of their own and bet on their hands. Game concept by Dr. Burt Simon. Copyright 2016 E. Regan \u0026 M. Pilosov. Enjoy.\nIf you have any questions\u0026hellip;\n","date":1532152800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532152800,"objectID":"bb3683b098cf495b8b11697517b8fcf9","permalink":"https://www.michaelpilosov.com/project/poker/","publishdate":"2018-07-21T00:00:00-06:00","relpermalink":"/project/poker/","section":"project","summary":"Like... the stock market, but faster!","tags":[],"title":"A New Game of Poker","type":"project"},{"authors":[],"categories":["environment","academia"],"content":"More to Come Soon.\n","date":1532152800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532152800,"objectID":"a7990883bc3f3b3e12b55f65ffd55214","permalink":"https://www.michaelpilosov.com/project/contaminant-transport/","publishdate":"2018-07-21T00:00:00-06:00","relpermalink":"/project/contaminant-transport/","section":"project","summary":"Contaminant remediation for environmental cleanup.","tags":["computation","geology","national lab"],"title":"Tracking Toxins","type":"project"},{"authors":null,"categories":null,"content":"","date":1530424800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530424800,"objectID":"cf344ed5c04bd1ae227e977e77d74922","permalink":"https://www.michaelpilosov.com/talk/siam_an18/","publishdate":"2018-07-01T00:00:00-06:00","relpermalink":"/talk/siam_an18/","section":"talk","summary":"To quantify uncertainties of inputs to models of dynamical systems, a fixed spatial configuration of sensors is often designed and deployed to collect time-series data for solving stochastic inverse problems. A general goal is to configure the sensors so that they provide useful information for the stochastic inverse problem over the full duration of the experiment and provide us with minimally redundant data. We use a recently developed Consistent Bayesian framework for formulating and solving stochastic inverse problems to investigate the effects of measurement frequency, duration, and quality on posterior distributions and predictions.","tags":[],"title":"A Consistent Bayesian Inference Problem with Time Series Data","type":"talk"},{"authors":null,"categories":null,"content":"This website uses no cookies and has no tracking enabled whatsoever on the server on which this site is hosted. However, Google Analytics is enabled (as it is on much of the web), which does take anonymized data about visitors to the site.\n","date":1530165600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530165600,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://www.michaelpilosov.com/privacy/","publishdate":"2018-06-28T00:00:00-06:00","relpermalink":"/privacy/","section":"","summary":"This website uses no cookies and has no tracking enabled whatsoever on the server on which this site is hosted. However, Google Analytics is enabled (as it is on much of the web), which does take anonymized data about visitors to the site.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"","date":1522562400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522562400,"objectID":"34e543ec6e9332387243e69288b57d0a","permalink":"https://www.michaelpilosov.com/talk/siam_uq18/","publishdate":"2018-04-01T00:00:00-06:00","relpermalink":"/talk/siam_uq18/","section":"talk","summary":"To quantify uncertainties of inputs to models of dynamical systems, a fixed spatial configuration of sensors is often designed and deployed to collect time-series data for solving stochastic inverse problems. A general goal is to configure the sensors so that they provide useful information for the stochastic inverse problem over the full duration of the experiment and provide us with minimally redundant data. We use a recently developed Consistent Bayesian framework for formulating and solving stochastic inverse problems to investigate several design criteria and objectives for deploying sensors. We draw comparisons to other approaches and results based on more classical statistical Bayesian formulations.","tags":[],"title":"Optimal Experimental Design of Time Series Data in a Consistent Bayesian Framework","type":"talk"},{"authors":null,"categories":null,"content":"","date":1520233200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520233200,"objectID":"216d6d6fa210dad6ce9ba0543b3ae102","permalink":"https://www.michaelpilosov.com/talk/framsc18/","publishdate":"2018-03-05T00:00:00-07:00","relpermalink":"/talk/framsc18/","section":"talk","summary":"We demonstrate the pervasiveness of inverse problems in the scientific disciplines. \nTo illustrate the breadth of possible applications, we will rely solely on the use of visual illustrations and animations. \nBy seeing the connections between mathematics and other fields, the audience will learn how to recognize inverse problems “in the real world.\"\n","tags":[],"title":"That’s an Inverse Problem!","type":"talk"},{"authors":null,"categories":null,"content":" (only showing academic jobs) University of Colorado: Denver (Downtown Campus)  Student Teaching Assistant, Calculus I (Aug-Dec 2016) Student Research Assistant (Jan-Aug 2016) \u0026ldquo;DiaMonD: An Integrated Multifaceted Approach to Mathematics at the Interfaces of Data, Models, and Decisions,\u0026rdquo; DE-SC0009279 Student Teaching Assistant, Calculus I (Aug-Dec 2015) Tutor, Mathematics Education Research Center (Aug-Dec 2015) Student Research Assistant (Aug 2014-May 2015) \u0026ldquo;CAREER: Foundations for Understanding and Reaching the Limits of Standard Numerical Linear Algebra,\u0026rdquo; NSF CCF 1054864 Student Research Assistant (Jun-Aug 2014) \u0026ldquo;SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS),\u0026rdquo; NSF ACI 1339797  ","date":1519887600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545202800,"objectID":"d089d41f694cb0dbd35d21f4300cd93d","permalink":"https://www.michaelpilosov.com/resume/employment/","publishdate":"2018-03-01T00:00:00-07:00","relpermalink":"/resume/employment/","section":"resume","summary":" (only showing academic jobs) University of Colorado: Denver (Downtown Campus)  Student Teaching Assistant, Calculus I (Aug-Dec 2016) Student Research Assistant (Jan-Aug 2016) \u0026ldquo;DiaMonD: An Integrated Multifaceted Approach to Mathematics at the Interfaces of Data, Models, and Decisions,\u0026rdquo; DE-SC0009279 Student Teaching Assistant, Calculus I (Aug-Dec 2015) Tutor, Mathematics Education Research Center (Aug-Dec 2015) Student Research Assistant (Aug 2014-May 2015) \u0026ldquo;CAREER: Foundations for Understanding and Reaching the Limits of Standard Numerical Linear Algebra,\u0026rdquo; NSF CCF 1054864 Student Research Assistant (Jun-Aug 2014) \u0026ldquo;SI2-SSI: Collaborative Research: Sustained Innovation for Linear Algebra Software (SILAS),\u0026rdquo; NSF ACI 1339797  ","tags":null,"title":"Employment","type":"docs"},{"authors":null,"categories":null,"content":" (only showing academic jobs) State University of New York: College at Geneseo  Teaching Assistant for Linear Programming \u0026amp; Operations Research (Jan-May 2014) Geneseo Research Foundation Undergraduate STEM Summer Research Fellow (Jun-Aug 2013)  Volunteer Data Analyst for As Green As It Gets (Nov-Dec 2012)\nPrivate Tutor in Mathematics (Summer-Fall 2013)\n","date":1519887600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545202800,"objectID":"8fb4cad9cabce9fb319ebba66b75b183","permalink":"https://www.michaelpilosov.com/resume/newyork/","publishdate":"2018-03-01T00:00:00-07:00","relpermalink":"/resume/newyork/","section":"resume","summary":"(only showing academic jobs) State University of New York: College at Geneseo  Teaching Assistant for Linear Programming \u0026amp; Operations Research (Jan-May 2014) Geneseo Research Foundation Undergraduate STEM Summer Research Fellow (Jun-Aug 2013)  Volunteer Data Analyst for As Green As It Gets (Nov-Dec 2012)\nPrivate Tutor in Mathematics (Summer-Fall 2013)","tags":null,"title":"Employment","type":"docs"},{"authors":null,"categories":null,"content":" Research Grants Lynn Bateman Teaching Fellowship 2018, CU Denver\nGeneseo Foundation STEM Undergraduate Summer Research Fellowship Grant 2013\nAcademic Societies Phi Beta Kappa Society 2014, SUNY Geneseo, Alpha Omega Chapter\nPi Mu Epsilon Mathematics Honor Society 2014, SUNY Geneseo, New York Alpha Delta Chapter\nGolden Key International Honor Society 2014, SUNY Geneseo\nAwards SUNY Geneseo Award for Excellence in Mathematics 2014-2015 (photos)\n74th annual William Lowell Putnam Math Competition First Place, Pi Mu Epsilon Alpha Delta Chapter\nScore: 10 points, ranked 1324\u0026frasl;4113\n2014 University of Rochester Math Olympiad First Place, Pi Mu Epsilon Alpha Delta Chapter\nGeneseo Open Chess Tournament (unrated section) 1st place (Spring \u0026lsquo;13)\n2nd place (Fall \u0026lsquo;13)\nSarah Lawrence College Poetry Festival Public School Initiative First Place, 2009 Contest\nMathematics League Certificate of Merit for Superior Achievement in the High School Contest 2009-10\nHonors President\u0026rsquo;s List Spring 2014\nDean\u0026rsquo;s List Fall 2011, 13; Spring 2012, 13\nFull Scholarship to partake in the Summer SUNY Oswego Global Laboratory Project in Brazil 2014, (declined)\nSUNY Research Foundation Summer Research Assistantship Grant 2012, (declined)\n","date":1519887600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545202800,"objectID":"4330096954cfe4c99e04c90a4f97a57e","permalink":"https://www.michaelpilosov.com/resume/honors/","publishdate":"2018-03-01T00:00:00-07:00","relpermalink":"/resume/honors/","section":"resume","summary":"Research Grants Lynn Bateman Teaching Fellowship 2018, CU Denver\nGeneseo Foundation STEM Undergraduate Summer Research Fellowship Grant 2013\nAcademic Societies Phi Beta Kappa Society 2014, SUNY Geneseo, Alpha Omega Chapter\nPi Mu Epsilon Mathematics Honor Society 2014, SUNY Geneseo, New York Alpha Delta Chapter\nGolden Key International Honor Society 2014, SUNY Geneseo\nAwards SUNY Geneseo Award for Excellence in Mathematics 2014-2015 (photos)\n74th annual William Lowell Putnam Math Competition First Place, Pi Mu Epsilon Alpha Delta Chapter","tags":null,"title":"Honors \u0026 Awards","type":"docs"},{"authors":null,"categories":null,"content":" Python  lead developer of cbayes contributor and researcher on BET extensive experience generating technical notebooks with varying levels of interactivity familiarity with JupyterHub, JupyterLab deep interest in reproducible open science and removing all friction towards such an end  Matlab  first language, used for countless projects. I have since moved away from it in favor of open-source alternatives  Git  familiarity with basic repository management have used Github, Bitbucket, and Gitlab I use it to track changes to nearly all the work I do  LaTeX  preferred document-generation language (pdfs) basic familiarity with publishing formatting for large documents extensive experience writing technical reports  Terminal (Bash/Shell)  Almost everything I do with respect to programming happens in a bash shell Much of my work is done remotely through ssh  Vim  My preferred text-editor and distraction-free organizational tool  Adobe Suite  experienced in Photoshop for general graphic design (posters, figures) familiar with Illustrator  Microsoft Suite  comfortable in Excel, though I prefer not to use it for anything related to analyzing data. I mostly use it for data entry. comfortable with Microsoft Word, but prefer to use LaTeX for its type-setting flexibility.  CLangs  I have coded in C++ in the past, but these days mostly use it when programming Arduinos Look, give me a piece of code in any C language, and I\u0026rsquo;ll likely be able to tell you what it does, but I don\u0026rsquo;t program in it (anymore). For scientific computing, Python is just more preferable overall Given some time, I can bind C libraries to Python wrappers if need be I can manipulate an existing codebase, but writing one from scratch is a poor use of my time and skillsets at this point (I would rather collaborate with someone more proficient in the language)  R  I have used it a lot over the years but these days prefer Python for anything that I would have once done in R Basically, if asked, I could translate ConsistentBayes into a non-optimized R-library in a couple weeks, I would just prefer not to  Wolfram  I use WolframAlpha (free) a bit, but I resist learning it because I cannot afford their licenses I wish nothing more than for them to open their code base. I fear that if they do not, their innovative notebooks will soon be associated primarily with Jupyter  Web-lanugages  I can manipulate HTML but cannot really write anything for the web from scratch. Pretty clueless with CSS but can guess-and-check my way around when necessary\u0026hellip;  Java  Can read it, have used it, just not in a long time. I have no desire to pick it back up unless someone makes a compelling argument to me for why I should. =)  ... And that about summarizes my experience with programming languages. ","date":1519887600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519887600,"objectID":"23bbd4118e72066563155809e2e63ce2","permalink":"https://www.michaelpilosov.com/resume/techskills/","publishdate":"2018-03-01T00:00:00-07:00","relpermalink":"/resume/techskills/","section":"resume","summary":"Python  lead developer of cbayes contributor and researcher on BET extensive experience generating technical notebooks with varying levels of interactivity familiarity with JupyterHub, JupyterLab deep interest in reproducible open science and removing all friction towards such an end  Matlab  first language, used for countless projects. I have since moved away from it in favor of open-source alternatives  Git  familiarity with basic repository management have used Github, Bitbucket, and Gitlab I use it to track changes to nearly all the work I do  LaTeX  preferred document-generation language (pdfs) basic familiarity with publishing formatting for large documents extensive experience writing technical reports  Terminal (Bash/Shell)  Almost everything I do with respect to programming happens in a bash shell Much of my work is done remotely through ssh  Vim  My preferred text-editor and distraction-free organizational tool  Adobe Suite  experienced in Photoshop for general graphic design (posters, figures) familiar with Illustrator  Microsoft Suite  comfortable in Excel, though I prefer not to use it for anything related to analyzing data.","tags":null,"title":"Technical Skills","type":"docs"},{"authors":null,"categories":null,"content":" Introduction These slideshows are created as single markdown files using Reveal.js implemented into the Academic Theme for Hugo, which was used as the template for this website.\nThe slideshows will open in a new tab (with any browser, including mobile ones). You can use the arrow keys to move through slides. If you are presenting using a laptop/desktop whose screen is extended to a monitor/projection, pressing the S key will enable a talk-timer in a separate browser window.\nThis presentation interface will be familiar to anyone who has used one of the leading presentation software tools, complete with a preview of the next slide and speaker notes.\nSlideshows Example Slides (packaged with theme)\nTest Slideshow\n","date":1513666800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513666800,"objectID":"90014a872bdadc29d38e0ddafb91b9e9","permalink":"https://www.michaelpilosov.com/slides/","publishdate":"2017-12-19T00:00:00-07:00","relpermalink":"/slides/","section":"","summary":"Introduction These slideshows are created as single markdown files using Reveal.js implemented into the Academic Theme for Hugo, which was used as the template for this website.\nThe slideshows will open in a new tab (with any browser, including mobile ones). You can use the arrow keys to move through slides. If you are presenting using a laptop/desktop whose screen is extended to a monitor/projection, pressing the S key will enable a talk-timer in a separate browser window.","tags":null,"title":"Slide Decks","type":"page"},{"authors":null,"categories":null,"content":"","date":1511222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511222400,"objectID":"573d5a8bd10d5f348c2402ee00eefcff","permalink":"https://www.michaelpilosov.com/wid/resume/","publishdate":"2017-11-21T00:00:00Z","relpermalink":"/wid/resume/","section":"wid","summary":"","tags":null,"title":"","type":"wid"},{"authors":null,"categories":null,"content":"","date":1488610800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488610800,"objectID":"76f3d511229642de9af6cd9e81efebfc","permalink":"https://www.michaelpilosov.com/talk/framsc17/","publishdate":"2017-03-04T00:00:00-07:00","relpermalink":"/talk/framsc17/","section":"talk","summary":"Responsible for securing funding, location, and managing executive board as President of the SIAM Student Chapter. \n","tags":[],"title":"[ Organizer ]","type":"talk"},{"authors":null,"categories":null,"content":"","date":1488178800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488178800,"objectID":"9e831558c3d01b552c0c0f778aadcde9","permalink":"https://www.michaelpilosov.com/talk/siam_cse17/","publishdate":"2017-02-27T00:00:00-07:00","relpermalink":"/talk/siam_cse17/","section":"talk","summary":"We present recent advancements on a measure-theoretic perspective for Simulation-Based Optimal Experimental Design (SB-OED). This perspective focuses on optimizing quantifiable geometric properties of Quantity of Interest (QoI) maps from model inputs to observable model output data. The two geometric properties considered in the formulation of the SB-OED are related to the accuracy and precision in solving stochastic inverse problems based on modeling uncertain QoI data using probability measures. We define these geometric properties precisely, provide numerical results illustrating various concepts, and discuss the current and future work of SB-OED.","tags":[],"title":"Advancements in Simulation-Based Optimal Experimental Design","type":"talk"},{"authors":["Burt Simon","Michael Pilosov"],"categories":null,"content":" Some Notes from the (co)author I worked on this topic primarily in summer 2014, with occasional contributions into early 2016. There is ample room for extending the work, but ultimately I did not venture into this field for my doctoral research.\nI now recognize that we were, however, attempting to answer an inverse problem in this paper and managed to get a glimpse of the solution by solving many forward problems, holding certain parameters fixed.\nThe motivation behind the linguistic choice of \u0026ldquo;catalyst\u0026rdquo; refers to the broadening of favorable conditions when group-level effects are factored into a population model.\nWere I to redo this, I would define our Quantity of Interest as \u0026ldquo;Time to x% Cooperation,\u0026rdquo; and invert a density that represents a reasonable time period for the evolution of cooperation.\nUndoubtedly, this would yield a set-valued solution that holistically describes the various conditions under which cooperation could have taken hold in societies under our proposed model.\nIt would be very interesting to see the relative likelihoods on parameters relating to group-level and population-level events, but ultimately it would make the same point as was shown in the paper: there are more conditions under which a \u0026ldquo;mutation of cooperation\u0026rdquo; could become established as the prevalent trait in a population.\n","date":1471240800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471240800,"objectID":"d3efb729ee368db28b9778185250b445","permalink":"https://www.michaelpilosov.com/publication/cooperation/","publishdate":"2016-08-15T00:00:00-06:00","relpermalink":"/publication/cooperation/","section":"publication","summary":"Group-level events, like fission and extinction, catalyze the evolution of cooperation in group-structured populations by creating new paths from uncooperative population states to more cooperative states. Group-level events allow cooperation to thrive under unfavorable conditions such as low intra-group assortment and moderate rates of migration, and can greatly speed up the evolution of cooperation when conditions are more favorable. The time-dependent effects of fission and extinction events are studied and illustrated here using a PDE model of a group-structured population based loosely on populations of hunter-gatherer tribes. By solving the PDE numerically we can compare models with and without group- level events, and explicitly calculate quantities associated with dynamics, like how long it takes a small population of cooperators to become a majority, as well as equilibrium population densities.","tags":["biology","evolution"],"title":"Group-level events are catalysts in the evolution of cooperation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461736800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461736800,"objectID":"91dd82e17bf6548e783db919f2f1fae8","permalink":"https://www.michaelpilosov.com/project/consistentbayes/","publishdate":"2016-04-27T00:00:00-06:00","relpermalink":"/project/consistentbayes/","section":"project","summary":"A repository I wrote for my PhD research.","tags":["code"],"title":"Consistent Bayes","type":"project"},{"authors":null,"categories":null,"content":"                                                                                                      Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461736800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461736800,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://www.michaelpilosov.com/project/internal-project/","publishdate":"2016-04-27T00:00:00-06:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":[],"title":"Internal Project","type":"project"},{"authors":[],"categories":["Artwork"],"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461132000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515826800,"objectID":"ae4af9de2ef7eac6cb78ab2dde2b44b4","permalink":"https://www.michaelpilosov.com/post/2018/getting-started/","publishdate":"2016-04-20T00:00:00-06:00","relpermalink":"/post/2018/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic","Demo"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":null,"categories":null,"content":"Test words.\n","date":1461132000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461132000,"objectID":"419429b9d3b4594a0492844756a799a5","permalink":"https://www.michaelpilosov.com/wid/projects/","publishdate":"2016-04-20T00:00:00-06:00","relpermalink":"/wid/projects/","section":"wid","summary":"Test words.","tags":null,"title":"Projects","type":"wid"},{"authors":null,"categories":null,"content":"","date":1457161200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1457161200,"objectID":"a5b174ebb4c148ba492b93be65bb2c7a","permalink":"https://www.michaelpilosov.com/talk/framsc16/","publishdate":"2016-03-05T00:00:00-07:00","relpermalink":"/talk/framsc16/","section":"talk","summary":"Assisted with organization in my capacity as Vice President of the SIAM Student Chapter.\n","tags":[],"title":"[ Volunteer ]","type":"talk"},{"authors":null,"categories":null,"content":"","date":1436940000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436940000,"objectID":"933b266b75dc2dc035f13790c154805b","permalink":"https://www.michaelpilosov.com/talk/rmswuq15/","publishdate":"2015-07-15T00:00:00-06:00","relpermalink":"/talk/rmswuq15/","section":"talk","summary":"\n","tags":[],"title":"[ Volunteer ]","type":"talk"},{"authors":null,"categories":null,"content":"","date":1426312800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426312800,"objectID":"b5ccd4478ece5e523a4acef3149179c2","permalink":"https://www.michaelpilosov.com/talk/siam_cse15/","publishdate":"2015-03-14T00:00:00-06:00","relpermalink":"/talk/siam_cse15/","section":"talk","summary":"We present a means of developing digital image transformations that allow a still image to be turned into a short and visually pleasing animation. Rather than manually altering successive frames to create the illusion of motion, the method presented here requires only the input of a few parameters for each transformation. We developed a mathematical framework wherein we defined animations as sequences of still images, and 'transformations' as composable functions on such sequences.\n\nTo implement this work, we have built a Matlab library of composable functions that streamline the process of turning still images into novel animations. Examples include manipulation of contrast, intensity, and colors of pixels, as well as warps of contours, positions, and size of select regions. The transformations allow for easy animation of regions of interest, giving some semblance of life to still images by turning them into animated GIFs.\n","tags":[],"title":"Moving Pictures: Animating Still Images","type":"talk"},{"authors":null,"categories":null,"content":"","date":1425106800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425106800,"objectID":"577e94988002a966bb57edc423bf3032","permalink":"https://www.michaelpilosov.com/talk/framsc15/","publishdate":"2015-02-28T00:00:00-07:00","relpermalink":"/talk/framsc15/","section":"talk","summary":"We present a means of developing digital image transformations that allow a still image to be turned into a short and visually pleasing animation. Rather than manually altering successive frames to create the illusion of motion, the method presented here requires only the input of a few parameters for each transformation. We developed a mathematical framework wherein we defined animations as sequences of still images, and transformations as composable functions on such sequences.\n\nTo implement this work, we have built a MATLAB library of composable functions that stream- line the process of turning still images into novel animations. Examples include manipulation of contrast, intensity, and colors of pixels, as well as warps of contours, positions, and size of select regions. The transformations allow for easy animation of regions of interest, giving some semlance of life to still images by turning them into animated GIFs.\n","tags":[],"title":"Moving Pictures: Animating Still Images","type":"talk"},{"authors":null,"categories":null,"content":"","date":1397368800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397368800,"objectID":"6878fff7ba172b247c240a7953ab0c66","permalink":"https://www.michaelpilosov.com/talk/great14-animation/","publishdate":"2014-04-13T00:00:00-06:00","relpermalink":"/talk/great14-animation/","section":"talk","summary":"We present a means of developing digital image transformations that allow a still image to be turned into a short and visually pleasing animation. Rather than manually altering successive frames to create the illusion of motion, the method presented here requires only the input of a few parameters for each transformation. To implement this work, we have built a library of composable functions that stream e the process of turning still images into novel animations. Examples include manipulation of contrast, intensity, and colors of pixels, as well as warps of contours, positions, and size of select regions. The transformations allow for easy animation of regions of interest, giving some semblance of life to still images by turning them into animated GIFs, and allowing for the creation of artwork in a popular on e medium. We will also discuss a possible application of the program to the amelioration of color perception deficiencies.\n","tags":[],"title":"Moving Pictures: Animating Still Images","type":"talk"},{"authors":null,"categories":null,"content":"","date":1397368800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397368800,"objectID":"fa1232830a9605cf9c917d199cf08c68","permalink":"https://www.michaelpilosov.com/talk/great14-pharma/","publishdate":"2014-04-13T00:00:00-06:00","relpermalink":"/talk/great14-pharma/","section":"talk","summary":"The state of the biopharmaceutical industry is such that the development of new drugs and therapies is hindered by the economic circumstances related to the high levels of risk associated with early stage testing. The payoff for advances in biomedicine is obstructed by high failure rates that detract potential investors from investing in individual products. In their paper \"Commercializing Biomedical Research Through Securitization Techniques,\" Fernandez, Stein, \u0026 Lo propose a \"financial structure in which a large number of biomedical programs at various stages of development are funded by a single entity to substantially reduce the portfolio's risk...by employing financial engineering techniques such as securitization\" to raise greater amounts of capital for longer-term investment strategies. The authors have made available the source code and data sets with which they ran their simulations in order to invite academic scrutiny. We aim to understand the mechanics of the research and perform various scenarios-namely sensitivity analysis-with the simulation and visualize the stochastic processes involved. Our current progress with this undertaking will be presented.\n","tags":[],"title":"Pharma Mutuals","type":"talk"},{"authors":null,"categories":null,"content":"","date":1397368800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397368800,"objectID":"83b75e0729d57fa1bd568b6881de5779","permalink":"https://www.michaelpilosov.com/talk/great14-art/","publishdate":"2014-04-13T00:00:00-06:00","relpermalink":"/talk/great14-art/","section":"talk","summary":"We will demonstrate an application that allows one to create artistic images using probability distributions with an intuitive GUI. This program will allow users to visually explore the interactions and behavior of randomness using joint probability distributions, which may be of particular interest to\nintroductory probability classrooms. The images created may also be used to create 'random' music as well.\n","tags":[],"title":"Probably Art","type":"talk"},{"authors":null,"categories":null,"content":"This talk was presented in the MAA Contributed Papers Session: At the Intersection of Mathematics and the Arts, as well as during a poster session.\n","date":1389855600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1389855600,"objectID":"39d2552b147dc6c436301028eeeec38b","permalink":"https://www.michaelpilosov.com/talk/jmm14/","publishdate":"2014-01-16T00:00:00-07:00","relpermalink":"/talk/jmm14/","section":"talk","summary":"We present a means of developing digital image transformations that allow a still image to be turned into a short and visually pleasing animation. Rather than manually augmenting successive frames to create the illusion of motion, the method presented here requires only the input of a few parameters for each transformation. We developed a mathematical framework wherein we defined animations as sequences of still images, and “transformations” as composable functions on such sequences.\n\nTo implement this work, we have built a MATLAB library of composable functions that streamline the process of turning still images into novel animations. Examples include manipulation of contrast, intensity, and colors of pixels, as well as warps of contours, positions, and size of select regions. The transformations allow for easy animation of regions of interest, giving some semblance of life to still images by turning them into animated GIFs.\n","tags":[],"title":"Animating Still Images","type":"talk"},{"authors":null,"categories":null,"content":"I was selected as one of eight students from the liberal arts college to attend this regional conference on behalf of SUNY Geneseo. I was the representative for the Department of Mathematics.\n","date":1389855600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1389855600,"objectID":"20c09d9614d5f2a7d9a49293c16a3854","permalink":"https://www.michaelpilosov.com/talk/coplac13/","publishdate":"2014-01-16T00:00:00-07:00","relpermalink":"/talk/coplac13/","section":"talk","summary":"The  animated  GIF  image  format  is  pervasive  throughout  the  Internet  community.  However,  to the best of my knowledge, no one has created animated GIFs solely from  a single still image.  I believe  this  is  because  current  technology  requires  tedious  manual  alteration  of  successive frames  to  create  such  animations.  I  investigated  whether  it  is  possible  to  automate  this  manual process  with  digital  image  transformations  by  working  at  the  interdisciplinary  intersection  of mathematics,  computer  science,  and  art.I  developed  a  mathematical  framework  wherein  I defined  “animations”  as  sequences  of  still  images,  and  “transformations”  as  composable functions  that  work  on  such  sequences.  To  implement  these  concepts,  I  built  a  library  of MATLAB functions that simplify and streamline the entire creative process by requiring only the input  of  a  few  parameters. Examples  include  manipulation  of  contrast,  intensity,  and  colors  of pixels,  as  well  as  warps  of  contours,  positions,  and  size  of  select  regions.  The  transformations allow for easy animation of regions of interest, giving some semblance of life to still images by turning  them  into  animated  GIFS.I  will  showcase  some  of  the  potential  of  this  work  when artistic layering of effects is performed.\n","tags":[],"title":"Moving Pictures?! Animating Still Images","type":"talk"},{"authors":null,"categories":null,"content":"This talk won Best Fun Statistics Presentation.\nThe theme for this year’s meeting is \u0026ldquo;Creating and Applying Tools for Massive Data Analytics\u0026rdquo;.\nUnfortunately, this conference was not well-documented online. It was a local conference for Western NY.\n","date":1388559600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388559600,"objectID":"a99959f54c10f54a2617d62bd2c8e221","permalink":"https://www.michaelpilosov.com/talk/upstat14-art/","publishdate":"2014-01-01T00:00:00-07:00","relpermalink":"/talk/upstat14-art/","section":"talk","summary":"This talk was the result of a directed study in which I first began to explore procedurally generated art as a means of \"visualizing\" high-dimensional probability distributions. \n","tags":[],"title":"Making Art with Probability Distributions","type":"talk"},{"authors":null,"categories":null,"content":"The theme for this year’s meeting is \u0026ldquo;Creating and Applying Tools for Massive Data Analytics\u0026rdquo;.\nUnfortunately, this conference was not well-documented online. It was a local conference for Western NY.\n","date":1388559600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388559600,"objectID":"cb3e0b65cbad08af45da5ff09c46a0d3","permalink":"https://www.michaelpilosov.com/talk/upstat14-pharma/","publishdate":"2014-01-01T00:00:00-07:00","relpermalink":"/talk/upstat14-pharma/","section":"talk","summary":"This talk was the result of a directed study in which myself and two colleagues performed a numerical sensitivity analysis on the assumptions in a paper on securitized portfolios for biomedical research.","tags":[],"title":"State of Pharma Mutuals","type":"talk"},{"authors":null,"categories":null,"content":"[Here is a paper][http://lasa-4.univ.pitt.edu/LARR/prot/fulltext/Vol49no1/49-1_155-177_fischer-victor.pdf] that gives some surrounding context.\n","date":1366092000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366092000,"objectID":"abfd51fef1b3c236b221040faf888559","permalink":"https://www.michaelpilosov.com/talk/great13-coffee/","publishdate":"2013-04-16T00:00:00-06:00","relpermalink":"/talk/great13-coffee/","section":"talk","summary":"The overarching goal of this study was to investigate the long-term sustainability of the San Miguel Escobar coffee cooperative associated with the non- governmental organization As Green As It Gets. A computer simulation determined that while coffee yields are projected to increase at a positive rate over the next five years, long-term sustainability would be a challenge as a result of the timing of coffee tree growth cycles. The focus of the study was to analyze the efficacy of various strategies aimed at leveling major crop yield fluctuations projected by the simulation. This investigation was conducted to aid sustainable economic development for the Guatemalan coffee farmers in the cooperative.\n","tags":[],"title":"Long-Term Sustainability of Cooperative-Wide Coffee Yields in San Miguel Escobar, Guatemala","type":"talk"},{"authors":null,"categories":null,"content":"","date":1366092000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1366092000,"objectID":"5cb9c2e33d14da7098cf5ed368ddd711","permalink":"https://www.michaelpilosov.com/talk/great13-poker/","publishdate":"2013-04-16T00:00:00-06:00","relpermalink":"/talk/great13-poker/","section":"talk","summary":"The use of card counting in Blackjack has received considerable media attention in the past decade. Basic strategy is simple to understand but the reasoning behind the probabilistic advantage is full of nuance and complications. We will provide a conceptual understanding of the mathematics underlying the advantage of counting cards in Blackjack. Multiple strategies will be discussed with attention to their relative benefits and efficacy of shifting the probability of winning towards the player and away from the casino. We will explain analytical approaches such as the Monte Carlo Method, a simulation method dependent on randomness; and the Kelly Criterion, a popular formula for scaling bet sizes.\n","tags":[],"title":"So You Want to Count Cards?","type":"talk"},{"authors":null,"categories":null,"content":" Welcome to Slides Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Test of Math in Speaker Notes $x + y = z$\nCode Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Fin Homepage\nSlideshows\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c4f5c7a2264f9673a2de022adf50d111","permalink":"https://www.michaelpilosov.com/slides/test/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/test/","section":"slides","summary":"Welcome to Slides Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Test of Math in Speaker Notes $x + y = z$\nCode Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:","tags":["Demo"],"title":"New Slideshow","type":"slides"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Home\nAsk\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://www.michaelpilosov.com/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":["Test","Demo"],"title":"Slides","type":"slides"}]